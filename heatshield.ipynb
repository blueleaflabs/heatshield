{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32fffd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Activate environment manually\n",
    "# conda activate blueleaflabs\n",
    "\n",
    "# Local project root\n",
    "PROJECT_DIR = os.path.expanduser(\"~/heatshield\")\n",
    "os.makedirs(os.path.join(PROJECT_DIR, \"results\", \"manual\"), exist_ok=True)\n",
    "os.chdir(PROJECT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b88a6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, math, io, zipfile, time, re, shutil, glob, pathlib\n",
    "from datetime import datetime as dt, timedelta, timezone, date\n",
    "from dateutil import parser as dateparser\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from urllib.parse import urljoin, quote\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import pytz\n",
    "\n",
    "import xarray as xr\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "import shapely\n",
    "from shapely import ops \n",
    "from shapely.geometry import Point, Polygon, box, mapping\n",
    "from shapely.ops import unary_union, transform as shp_transform\n",
    "from pyproj import Transformer\n",
    "import geopandas as gpd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import folium\n",
    "import ee  # Earth Engine\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"Environment ready in VS Code.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c0fe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configurations for local VS Code setup (California full extent) ---\n",
    "\n",
    "PURPLEAIR_SENSOR_INDEX = ''  # optional manual override\n",
    "CDS_UID = ''\n",
    "CDS_API_KEY = ''\n",
    "\n",
    "CONFIG = {\n",
    "    # Project identity\n",
    "    \"parent\": \"blueleaflabs\",\n",
    "    \"project\": \"heatshield\",\n",
    "\n",
    "    # California bounding box (approximate)\n",
    "    \"bbox\": {\n",
    "        \"nwlat\": 42.0095,\n",
    "        \"nwlng\": -124.4820,\n",
    "        \"selat\": 32.5343,\n",
    "        \"selng\": -114.1315\n",
    "    },\n",
    "\n",
    "    # Time window (UTC)\n",
    "    \"start_date\": \"2024-06-01\",\n",
    "    \"end_date\": \"2024-10-31\",\n",
    "\n",
    "    # API tokens\n",
    "    \"CDO_TOKEN\": \"BrboArtFBPlkPVXhDLlmrUZiTLVwThSr\",\n",
    "    \"AIRNOW_API_KEY\": \"BDF7B01F-7934-45E9-ABF7-5DC35F4E716C\",\n",
    "    \"PURPLEAIR_API_KEY\": \"A2F84F7C-B439-11F0-BDE5-4201AC1DC121\", #changed because the prior one ran out!\n",
    "    \"ACS_KEY\": \"b7e5e9c956393182b93ae8c0786895acdbbbf8e9\",\n",
    "\n",
    "    # Base URLs\n",
    "    \"USCRN_BASE_HOURLY\": \"https://www.ncei.noaa.gov/pub/data/uscrn/products/hourly02/\",\n",
    "    \"HRRR_BASE_GENERIC\": \"https://nomads.ncep.noaa.gov/cgi-bin/filter_hrrr_2d.pl\",\n",
    "    \"HRRR_BASE_SMOKE\":   \"https://nomads.ncep.noaa.gov/cgi-bin/filter_hrrr_smoke_2d.pl\",\n",
    "    \"HMS_BASE_SMOKE\": \"https://satepsanone.nesdis.noaa.gov/pub/FIRE/web/HMS/Smoke_Polygons/Shapefile\",\n",
    "    \"HMS_BASE_FIRE\":  \"https://satepsanone.nesdis.noaa.gov/pub/FIRE/web/HMS/Fire_Points/Shapefile\",\n",
    "    \"ACS_BASE_ACS5\": \"https://api.census.gov/data/2023/acs/acs5\",\n",
    "    \"ACS_BASE_SUBJECT\": \"https://api.census.gov/data/2023/acs/acs5/subject\",\n",
    "    \"WHP_ZIP\": \"https://www.fs.usda.gov/rds/archive/products/RDS-2015-0047-4/RDS-2015-0047-4_Data.zip\",\n",
    "    \"WHP_TIF_NAME\": \"Data/whp2023_GeoTIF/whp2023_cnt_conus.tif\",\n",
    "\n",
    "    # Headers\n",
    "    \"USER_AGENT_HEADERS\": {\"User-Agent\": \"HeatShield/1.0\"},\n",
    "    \"NOMADS_HEADERS\": {\"User-Agent\": \"HeatShield/1.0\", \"Referer\": \"https://nomads.ncep.noaa.gov/\"},\n",
    "\n",
    "    # HRRR\n",
    "    \"HRRR_CYCLES\": [\"00\"],\n",
    "    \"HRRR_FCST_HOURS\": [0],\n",
    "\n",
    "    # Output paths (will be absolutized below)\n",
    "    \"out_dir\": \"results\",\n",
    "    \"NLCD_MANUAL_ZIP\": \"results/manual/Annual_NLCD_LndCov_2024_CU_C1V1.zip\",\n",
    "\n",
    "    # Spatial config\n",
    "    \"grid_resolution_m\": 3000, # grid cell size in meters; 500 was too small for all of CA!\n",
    "    \"crs_epsg\": 4326,\n",
    "\n",
    "    # USCRN configuration for California\n",
    "    \"USCRN_STATION_NAME\": [\n",
    "        \"CA Bodega 6 WSW\",\n",
    "        \"CA Fallbrook 5 NE\",\n",
    "        \"CA Merced 23 WSW\",\n",
    "        \"CA Redding 12 WNW\",\n",
    "        \"CA Santa Barbara 11 W\",\n",
    "        \"CA Stovepipe Wells 1 SW\",\n",
    "        \"CA Yosemite Village 12 W\"\n",
    "    ],\n",
    "    \"USCRN_YEARS\": list(range(2024, 2025)),\n",
    "\n",
    "    # AQS parameters\n",
    "    \"AQS_STATE_CODE\": \"06\",  # California\n",
    "    \"AQS_EMAIL\": \"rohan3142@gmail.com\",\n",
    "    \"AQS_KEY\": \"berrymouse51\"\n",
    "}\n",
    "\n",
    "# --- Local file structure ---\n",
    "PROJECT_DIR = \"/Users/Shared/blueleaflabs/heatshield\"\n",
    "os.makedirs(os.path.join(PROJECT_DIR, \"results\", \"manual\"), exist_ok=True)\n",
    "\n",
    "# Absolutize output paths\n",
    "CONFIG[\"out_dir\"] = os.path.join(PROJECT_DIR, \"results\")\n",
    "CONFIG[\"NLCD_MANUAL_ZIP\"] = os.path.join(CONFIG[\"out_dir\"], \"manual\", \"Annual_NLCD_LndCov_2024_CU_C1V1.zip\")\n",
    "\n",
    "# Set working directory\n",
    "os.chdir(PROJECT_DIR)\n",
    "\n",
    "print(\"Config loaded and local directories ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb446e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Ensure California boundary and build 3 km grid clipped to land ---\n",
    "\n",
    "import os, io, zipfile, requests, numpy as np, pandas as pd, geopandas as gpd\n",
    "from shapely.geometry import box\n",
    "from pyproj import Transformer\n",
    "from shapely import ops\n",
    "\n",
    "# Config\n",
    "WGS84_EPSG = 4326\n",
    "CA_ALBERS_EPSG = 3310\n",
    "res_m = int(CONFIG.get(\"grid_resolution_m\", 3000))\n",
    "out_epsg = int(CONFIG.get(\"crs_epsg\", 4326))\n",
    "out_dir = CONFIG[\"out_dir\"]; os.makedirs(out_dir, exist_ok=True)\n",
    "inset_buffer_m = int(CONFIG.get(\"coast_inset_m\", 0))  # e.g. 5000\n",
    "boundary_path = CONFIG.get(\"ca_boundary_path\", None)\n",
    "\n",
    "# 1) Ensure boundary: download Census cartographic boundary if missing\n",
    "if not boundary_path or not os.path.exists(boundary_path):\n",
    "    states_zip = os.path.join(out_dir, \"cb_2023_us_state_20m.zip\")\n",
    "    if not os.path.exists(states_zip):\n",
    "        url = \"https://www2.census.gov/geo/tiger/GENZ2023/shp/cb_2023_us_state_20m.zip\"\n",
    "        r = requests.get(url, timeout=120); r.raise_for_status()\n",
    "        with open(states_zip, \"wb\") as f: f.write(r.content)\n",
    "    # Read from zip directly and select California\n",
    "    states = gpd.read_file(f\"zip://{states_zip}\")\n",
    "    if states.empty:\n",
    "        raise ValueError(\"Census states file loaded empty.\")\n",
    "    ca = states[states[\"STATEFP\"].astype(str).str.zfill(2).eq(\"06\")][[\"geometry\"]]\n",
    "    if ca.empty:\n",
    "        raise ValueError(\"California polygon not found in Census states file.\")\n",
    "    boundary_path = os.path.join(out_dir, \"california_boundary.gpkg\")\n",
    "    ca.to_file(boundary_path, driver=\"GPKG\")\n",
    "    CONFIG[\"ca_boundary_path\"] = boundary_path  # persist for later cells\n",
    "\n",
    "# 2) Load boundary, dissolve, project, optional inward buffer\n",
    "b = gpd.read_file(boundary_path)\n",
    "if b.crs is None: raise ValueError(\"Boundary file has no CRS.\")\n",
    "b = b[[\"geometry\"]].copy()\n",
    "b = b.to_crs(CA_ALBERS_EPSG)\n",
    "b = gpd.GeoDataFrame(geometry=[b.unary_union], crs=f\"EPSG:{CA_ALBERS_EPSG}\")\n",
    "if inset_buffer_m > 0:\n",
    "    b.geometry = b.buffer(-inset_buffer_m)\n",
    "    b = gpd.GeoDataFrame(geometry=[b.unary_union], crs=f\"EPSG:{CA_ALBERS_EPSG}\")\n",
    "\n",
    "# 3) Build snapped rectilinear grid over boundary bounds in EPSG:3310\n",
    "minx, miny, maxx, maxy = b.total_bounds\n",
    "snap_down = lambda v, s: np.floor(v/s)*s\n",
    "snap_up   = lambda v, s: np.ceil(v/s)*s\n",
    "minx, miny = snap_down(minx, res_m), snap_down(miny, res_m)\n",
    "maxx, maxy = snap_up(maxx, res_m), snap_up(maxy, res_m)\n",
    "\n",
    "xs = np.arange(minx, maxx, res_m)\n",
    "ys = np.arange(miny, maxy, res_m)\n",
    "n_rect = len(xs)*len(ys)\n",
    "if n_rect > 3_500_000:\n",
    "    raise MemoryError(f\"Grid too large ({n_rect:,}). Increase res_m or tile the state.\")\n",
    "\n",
    "cells, col_i, row_j = [], [], []\n",
    "for j, y in enumerate(ys):\n",
    "    for i, x in enumerate(xs):\n",
    "        cells.append(box(x, y, x+res_m, y+res_m)); col_i.append(i); row_j.append(j)\n",
    "\n",
    "gdf_proj = gpd.GeoDataFrame({\"col_i\": np.int32(col_i), \"row_j\": np.int32(row_j)},\n",
    "                            geometry=cells, crs=f\"EPSG:{CA_ALBERS_EPSG}\")\n",
    "gdf_proj[\"cell_area_m2\"] = float(res_m)*float(res_m)\n",
    "gdf_proj[\"grid_id\"] = f\"CA3310_{res_m}_\" + gdf_proj[\"col_i\"].astype(str) + \"_\" + gdf_proj[\"row_j\"].astype(str)\n",
    "\n",
    "# 4) Strict land clip and land fraction\n",
    "gdf_proj = gpd.sjoin(gdf_proj, b, how=\"inner\", predicate=\"intersects\").drop(columns=[\"index_right\"])\n",
    "inter = gpd.overlay(gdf_proj[[\"grid_id\",\"geometry\"]], b, how=\"intersection\", keep_geom_type=True)\n",
    "inter[\"land_area_m2\"] = inter.geometry.area\n",
    "land = inter[[\"grid_id\",\"land_area_m2\"]].groupby(\"grid_id\", as_index=False).sum()\n",
    "gdf_proj = gdf_proj.merge(land, on=\"grid_id\", how=\"left\")\n",
    "gdf_proj[\"land_area_m2\"] = gdf_proj[\"land_area_m2\"].fillna(0.0)\n",
    "gdf_proj[\"land_frac\"] = (gdf_proj[\"land_area_m2\"] / gdf_proj[\"cell_area_m2\"]).clip(0,1)\n",
    "gdf_proj = gdf_proj[gdf_proj[\"land_frac\"] > 0].reset_index(drop=True)\n",
    "\n",
    "# 5) Reproject to requested output CRS and save\n",
    "grid_gdf = gdf_proj.to_crs(out_epsg)\n",
    "\n",
    "parquet_path = os.path.join(out_dir, f\"grid_{res_m}m_CA.parquet\")\n",
    "grid_gdf.to_parquet(parquet_path, index=False)\n",
    "\n",
    "geojson_path = os.path.join(out_dir, f\"grid_{res_m}m_CA_head10.geojson\")\n",
    "grid_gdf.head(10).to_file(geojson_path, driver=\"GeoJSON\")\n",
    "\n",
    "# Diagnostics\n",
    "cell_area_km2 = (res_m/1000.0)**2\n",
    "eff_land_km2 = float((grid_gdf.get(\"land_frac\",1.0) * cell_area_km2).sum())\n",
    "print(f\"Saved: {parquet_path}\")\n",
    "print(f\"Cells: {len(grid_gdf):,}\")\n",
    "print(f\"Effective land area ≈ {round(eff_land_km2):,} km²\")\n",
    "print(f\"Implied cell size ≈ {round((eff_land_km2/len(grid_gdf))**0.5,2)} km\")\n",
    "\n",
    "grid_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1926a85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Persist config + save grid (3310 ops copy, 4326 preview) + write metadata ---\n",
    "\n",
    "# Inputs assumed from prior cell:\n",
    "# - grid_gdf            : current grid GeoDataFrame (any CRS)\n",
    "# - CONFIG              : dict with out_dir, grid_resolution_m, crs_epsg, ca_boundary_path\n",
    "# - CA_ALBERS_EPSG=3310 : defined earlier\n",
    "\n",
    "out_dir = CONFIG[\"out_dir\"]; os.makedirs(out_dir, exist_ok=True)\n",
    "res_m = int(CONFIG.get(\"grid_resolution_m\", 3000))\n",
    "out_epsg = int(CONFIG.get(\"crs_epsg\", 4326))\n",
    "boundary_path = CONFIG.get(\"ca_boundary_path\")\n",
    "\n",
    "# 1) Persist boundary path back to CONFIG \n",
    "if not boundary_path or not os.path.exists(boundary_path):\n",
    "    raise FileNotFoundError(\"CONFIG['ca_boundary_path'] missing or invalid. Rebuild boundary.\")\n",
    "CONFIG[\"ca_boundary_path\"] = boundary_path\n",
    "\n",
    "config_runtime_path = os.path.join(out_dir, \"config_runtime.json\")\n",
    "with open(config_runtime_path, \"w\") as f:\n",
    "    json.dump(CONFIG, f, indent=2)\n",
    "print(\"Saved:\", config_runtime_path)\n",
    "\n",
    "# 2) Ensure we have an EPSG:3310 version for spatial ops\n",
    "if grid_gdf.crs is None:\n",
    "    raise ValueError(\"grid_gdf has no CRS. Rebuild grid.\")\n",
    "grid_3310 = grid_gdf.to_crs(3310) if grid_gdf.crs.to_epsg() != 3310 else grid_gdf\n",
    "\n",
    "# 3) Save operational GeoParquet in 3310 + lightweight WGS84 preview\n",
    "parquet_3310 = os.path.join(out_dir, f\"grid_{res_m}m_CA_epsg3310.parquet\")\n",
    "grid_3310.to_parquet(parquet_3310, index=False)\n",
    "print(\"Saved:\", parquet_3310, \"| cells:\", len(grid_3310))\n",
    "\n",
    "# Optional small preview in 4326 for quick map checks\n",
    "preview_4326 = grid_3310.to_crs(4326).head(500)  # cap to avoid huge files\n",
    "geojson_preview = os.path.join(out_dir, f\"grid_{res_m}m_CA_head500_epsg4326.geojson\")\n",
    "preview_4326.to_file(geojson_preview, driver=\"GeoJSON\")\n",
    "print(\"Saved:\", geojson_preview)\n",
    "\n",
    "# 4) Compute and save metadata\n",
    "cell_area_km2 = (res_m/1000.0)**2\n",
    "effective_land_km2 = float((grid_3310.get(\"land_frac\", 1.0) * cell_area_km2).sum())\n",
    "implied_cell_km = float((effective_land_km2 / len(grid_3310))**0.5)\n",
    "minx, miny, maxx, maxy = grid_3310.total_bounds\n",
    "bbox_km = ((maxx-minx)/1000.0, (maxy-miny)/1000.0)\n",
    "\n",
    "meta = {\n",
    "    \"timestamp_utc\": dt.utcnow().isoformat(timespec=\"seconds\") + \"Z\",\n",
    "    \"grid_resolution_m\": res_m,\n",
    "    \"crs_ops_epsg\": 3310,\n",
    "    \"crs_export_default_epsg\": out_epsg,\n",
    "    \"cells\": int(len(grid_3310)),\n",
    "    \"effective_land_area_km2\": round(effective_land_km2, 2),\n",
    "    \"implied_cell_km\": round(implied_cell_km, 4),\n",
    "    \"bbox_km_width_height\": [round(bbox_km[0], 2), round(bbox_km[1], 2)],\n",
    "    \"has_land_frac\": bool(\"land_frac\" in grid_3310.columns),\n",
    "    \"boundary_path\": boundary_path,\n",
    "    \"parquet_3310_path\": parquet_3310,\n",
    "    \"geojson_preview_4326_path\": geojson_preview,\n",
    "}\n",
    "\n",
    "meta_path = os.path.join(out_dir, f\"grid_{res_m}m_CA_meta.json\")\n",
    "with open(meta_path, \"w\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "print(\"Saved:\", meta_path)\n",
    "meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bc42f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CDO data fetch and processing functions\n",
    "# repeat some variables for clarity\n",
    "OUT_DIR = CONFIG[\"out_dir\"]\n",
    "RAW_DIR = os.path.join(OUT_DIR, \"cdo_raw_monthly\")\n",
    "CLEAN_DIR = os.path.join(OUT_DIR, \"cdo_clean\")\n",
    "os.makedirs(RAW_DIR, exist_ok=True); os.makedirs(CLEAN_DIR, exist_ok=True)\n",
    "\n",
    "def month_windows(start_date, end_date):\n",
    "    s = dt.fromisoformat(start_date).date().replace(day=1)\n",
    "    e = dt.fromisoformat(end_date).date()\n",
    "    cur = s\n",
    "    while cur <= e:\n",
    "        nxt = (cur + relativedelta(months=1)) - relativedelta(days=1)\n",
    "        yield cur.isoformat(), min(nxt, e).isoformat()\n",
    "        cur = (cur + relativedelta(months=1)).replace(day=1)\n",
    "\n",
    "def parse_attributes(attr):\n",
    "    parts = (attr or \"\").split(\",\"); parts += [\"\"] * (4 - len(parts))\n",
    "    mflag, qflag, sflag, obs_hhmm = parts[:4]\n",
    "    return mflag or None, qflag or None, sflag or None, obs_hhmm or None\n",
    "\n",
    "def fetch_cdo_page(session, url, headers, params, max_retries=6, base_delay=0.8):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            r = session.get(url, headers=headers, params=params, timeout=180)\n",
    "            if r.status_code in (429, 500, 502, 503, 504):\n",
    "                raise requests.HTTPError(f\"{r.status_code} Retryable\", response=r)\n",
    "            r.raise_for_status()\n",
    "            return r.json()\n",
    "        except requests.HTTPError as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                raise\n",
    "            # exponential backoff with jitter\n",
    "            delay = base_delay * (2 ** attempt) * (1 + 0.25 * (2 * (time.time() % 1) - 1))\n",
    "            time.sleep(delay)\n",
    "\n",
    "def cdo_stream_monthly(datasetid, locationid, startdate, enddate, datatypes, token,\n",
    "                       units=\"standard\", page_limit=1000, force=False):\n",
    "    url = \"https://www.ncei.noaa.gov/cdo-web/api/v2/data\"\n",
    "    headers = {\"token\": token}\n",
    "    session = requests.Session()\n",
    "    written = []\n",
    "\n",
    "    for dtid in datatypes:\n",
    "        for ms, me in month_windows(startdate, enddate):\n",
    "            out_csv = os.path.join(RAW_DIR, f\"ghcnd_{dtid}_{ms[:7]}.csv\")\n",
    "            if os.path.exists(out_csv) and not force:\n",
    "                # resume: skip existing month-datatype file\n",
    "                written.append(out_csv); continue\n",
    "\n",
    "            frames = []\n",
    "            offset = 1\n",
    "            while True:\n",
    "                params = {\n",
    "                    \"datasetid\": datasetid, \"locationid\": locationid,\n",
    "                    \"startdate\": ms, \"enddate\": me,\n",
    "                    \"datatypeid\": dtid, \"units\": units,\n",
    "                    \"limit\": page_limit, \"offset\": offset\n",
    "                }\n",
    "                js = fetch_cdo_page(session, url, headers, params)\n",
    "                rows = js.get(\"results\", [])\n",
    "                if not rows:\n",
    "                    break\n",
    "                frames.append(pd.json_normalize(rows))\n",
    "                if len(rows) < page_limit:\n",
    "                    break\n",
    "                offset += page_limit\n",
    "                time.sleep(0.15)  # gentle pacing\n",
    "\n",
    "            if frames:\n",
    "                df = pd.concat(frames, ignore_index=True)\n",
    "                # normalize\n",
    "                df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\").dt.date\n",
    "                parsed = df[\"attributes\"].apply(parse_attributes)\n",
    "                df[[\"mflag\",\"qflag\",\"sflag\",\"obs_hhmm\"]] = pd.DataFrame(parsed.tolist(), index=df.index)\n",
    "                # scale tenths\n",
    "                scale = {\"PRCP\": 0.1, \"TMAX\": 0.1, \"TMIN\": 0.1}\n",
    "                df[\"datatype\"] = df[\"datatype\"].astype(str)\n",
    "                df[\"value\"] = pd.to_numeric(df[\"value\"], errors=\"coerce\")\n",
    "                df[\"value_scaled\"] = df.apply(lambda r: r[\"value\"] * scale.get(r[\"datatype\"], 1.0), axis=1)\n",
    "                # write monthly raw\n",
    "                df[[\"date\",\"datatype\",\"station\",\"attributes\",\"mflag\",\"qflag\",\"sflag\",\"obs_hhmm\",\"value\",\"value_scaled\"]].to_csv(out_csv, index=False)\n",
    "                written.append(out_csv)\n",
    "            else:\n",
    "                # create an empty file with header to mark completion\n",
    "                with open(out_csv, \"w\", newline=\"\") as f:\n",
    "                    w = csv.writer(f); w.writerow([\"date\",\"datatype\",\"station\",\"attributes\",\"mflag\",\"qflag\",\"sflag\",\"obs_hhmm\",\"value\",\"value_scaled\"])\n",
    "                written.append(out_csv)\n",
    "    return written\n",
    "\n",
    "def build_clean_wide():\n",
    "    # read all monthly raw files and assemble cleaned wide once\n",
    "    files = sorted([os.path.join(RAW_DIR, f) for f in os.listdir(RAW_DIR) if f.endswith(\".csv\")])\n",
    "    if not files:\n",
    "        return None\n",
    "    df = pd.concat((pd.read_csv(f, dtype={\"datatype\":str,\"station\":str}) for f in files), ignore_index=True)\n",
    "    # convert types back\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"]).dt.date\n",
    "    # keep good qflag\n",
    "    df = df[(df[\"qflag\"].isna()) | (df[\"qflag\"]==\"\")]\n",
    "    wide = (\n",
    "        df.pivot_table(index=[\"station\",\"date\"], columns=\"datatype\", values=\"value_scaled\", aggfunc=\"mean\")\n",
    "          .reset_index()\n",
    "          .rename(columns={\"date\":\"obs_date\",\"PRCP\":\"precipitation_mm\",\"TMAX\":\"temperature_max_c\",\"TMIN\":\"temperature_min_c\"})\n",
    "          .sort_values([\"obs_date\",\"station\"])\n",
    "    )\n",
    "    # attach obs time from PRCP\n",
    "    prcp_times = df[df[\"datatype\"]==\"PRCP\"][[\"station\",\"date\",\"obs_hhmm\"]].drop_duplicates().rename(columns={\"date\":\"obs_date\"})\n",
    "    wide = wide.merge(prcp_times, on=[\"station\",\"obs_date\"], how=\"left\")\n",
    "    raw_all = os.path.join(OUT_DIR, \"ghcnd_daily_raw_all.csv\")\n",
    "    wide_all = os.path.join(OUT_DIR, \"ghcnd_daily_wide.csv\")\n",
    "    df.to_csv(raw_all, index=False)\n",
    "    wide.to_csv(wide_all, index=False)\n",
    "    return raw_all, wide_all, len(df), len(wide), wide[\"station\"].nunique(), wide[\"obs_date\"].nunique()\n",
    "\n",
    "# ---- Run statewide with resume capability ----\n",
    "if CONFIG[\"CDO_TOKEN\"] and CONFIG[\"CDO_TOKEN\"] != \"YOUR_NCEI_CDO_TOKEN\":\n",
    "    written = cdo_stream_monthly(\n",
    "        datasetid=\"GHCND\",\n",
    "        locationid=\"FIPS:06\",                      # California statewide\n",
    "        startdate=CONFIG[\"start_date\"],\n",
    "        enddate=CONFIG[\"end_date\"],\n",
    "        datatypes=[\"TMAX\",\"TMIN\",\"PRCP\"],\n",
    "        token=CONFIG[\"CDO_TOKEN\"],\n",
    "        units=\"standard\",\n",
    "        page_limit=1000,\n",
    "        force=False                                 # set True to re-download\n",
    "    )\n",
    "    print(f\"Monthly files written: {len(written)} → {RAW_DIR}\")\n",
    "\n",
    "    res = build_clean_wide()\n",
    "    if res:\n",
    "        raw_all, wide_all, n_raw, n_wide, n_stn, n_dates = res\n",
    "        print(f\"Saved raw:  {raw_all}\")\n",
    "        print(f\"Saved wide: {wide_all}\")\n",
    "        print(f\"Counts → raw: {n_raw} | wide: {n_wide} | stations: {n_stn} | dates: {n_dates}\")\n",
    "else:\n",
    "    print(\"Skipping CDO (missing CDO token).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88126253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AirNow PM2.5 hourly\n",
    "# repeat some variables for clarity\n",
    "\n",
    "OUT_DIR = CONFIG[\"out_dir\"]\n",
    "RAW_DIR = os.path.join(OUT_DIR, \"airnow_raw_daily\")\n",
    "CLEAN_DIR = os.path.join(OUT_DIR, \"airnow_clean\")\n",
    "os.makedirs(RAW_DIR, exist_ok=True); os.makedirs(CLEAN_DIR, exist_ok=True)\n",
    "\n",
    "def day_windows(start_iso, end_iso):\n",
    "    s = dt.fromisoformat(start_iso).date()\n",
    "    e = dt.fromisoformat(end_iso).date()\n",
    "    d = s\n",
    "    while d <= e:\n",
    "        yield d.isoformat()\n",
    "        d += timedelta(days=1)\n",
    "\n",
    "def fetch_airnow_day(api_key, bbox, day_iso, max_retries=6, base_delay=0.8):\n",
    "    base = \"https://www.airnowapi.org/aq/data/\"\n",
    "    params = {\n",
    "        \"startDate\": f\"{day_iso}T00\",\n",
    "        \"endDate\":   f\"{day_iso}T23\",\n",
    "        \"parameters\": \"PM25\",\n",
    "        \"BBOX\": f\"{bbox['nwlng']},{bbox['selat']},{bbox['selng']},{bbox['nwlat']}\",\n",
    "        \"dataType\": \"A\",\n",
    "        \"format\": \"text/csv\",\n",
    "        \"API_KEY\": api_key\n",
    "    }\n",
    "    for attempt in range(max_retries):\n",
    "        r = requests.get(base, params=params, timeout=180)\n",
    "        # Retry on 429 and 5xx\n",
    "        if r.status_code in (429, 500, 502, 503, 504):\n",
    "            if attempt == max_retries - 1:\n",
    "                r.raise_for_status()\n",
    "            delay = base_delay * (2 ** attempt)\n",
    "            time.sleep(delay)\n",
    "            continue\n",
    "        r.raise_for_status()\n",
    "        return r.text\n",
    "    return \"\"  # unreachable if raise above\n",
    "\n",
    "def schema_coerce(df):\n",
    "    # Expect 6 cols: lat, lon, datetime_utc, parameter, value, valid_flag\n",
    "    if df.shape[1] != 6:\n",
    "        # fall back: try reading with header and pick needed cols later\n",
    "        return None\n",
    "    df = df.rename(columns={0:\"latitude\",1:\"longitude\",2:\"datetime_utc\",3:\"parameter\",4:\"value_ugm3\",5:\"valid\"})\n",
    "    # types\n",
    "    df[\"latitude\"] = pd.to_numeric(df[\"latitude\"], errors=\"coerce\")\n",
    "    df[\"longitude\"] = pd.to_numeric(df[\"longitude\"], errors=\"coerce\")\n",
    "    # allow both “YYYY-MM-DDTHH:MM” and “YYYY-MM-DD HH:MM:SS”\n",
    "    df[\"datetime_utc\"] = pd.to_datetime(df[\"datetime_utc\"], errors=\"coerce\", utc=True)\n",
    "    df[\"value_ugm3\"] = pd.to_numeric(df[\"value_ugm3\"], errors=\"coerce\")\n",
    "    df[\"valid\"] = pd.to_numeric(df[\"valid\"], errors=\"coerce\")\n",
    "    # normalize parameter label\n",
    "    df[\"parameter\"] = df[\"parameter\"].astype(str).str.upper().str.replace(\" \", \"\")\n",
    "    return df\n",
    "\n",
    "def in_bbox(df, b):\n",
    "    return df[\n",
    "        (df[\"longitude\"] >= b[\"nwlng\"]) & (df[\"longitude\"] <= b[\"selng\"]) &\n",
    "        (df[\"latitude\"]  >= b[\"selat\"]) & (df[\"latitude\"]  <= b[\"nwlat\"])\n",
    "    ]\n",
    "\n",
    "def clean_airnow(df, bbox):\n",
    "    df = df.dropna(subset=[\"datetime_utc\",\"latitude\",\"longitude\",\"value_ugm3\"])\n",
    "    df = in_bbox(df, bbox)\n",
    "    df = df[df[\"parameter\"].isin([\"PM2.5\",\"PM25\"])]\n",
    "    # validity flag: keep valid==1 if present, else keep all\n",
    "    if \"valid\" in df.columns:\n",
    "        df = df[(df[\"valid\"].isna()) | (df[\"valid\"] == 1)]\n",
    "    # value filters\n",
    "    df = df[(df[\"value_ugm3\"] >= 0) & (df[\"value_ugm3\"] <= 1000)]\n",
    "    # hour rounding and date field\n",
    "    df[\"datetime_utc\"] = df[\"datetime_utc\"].dt.floor(\"H\")\n",
    "    df[\"date_utc\"] = df[\"datetime_utc\"].dt.date\n",
    "    # de-dup\n",
    "    df = df.drop_duplicates(subset=[\"latitude\",\"longitude\",\"datetime_utc\",\"parameter\"])\n",
    "    return df\n",
    "\n",
    "def airnow_stream(CONFIG, force=False):\n",
    "    if not CONFIG.get(\"AIRNOW_API_KEY\"):\n",
    "        print(\"Skipping AirNow (missing API key).\")\n",
    "        return None\n",
    "\n",
    "    bbox = CONFIG[\"bbox\"]\n",
    "    api_key = CONFIG[\"AIRNOW_API_KEY\"]\n",
    "    written = []\n",
    "\n",
    "    for day in day_windows(CONFIG[\"start_date\"], CONFIG[\"end_date\"]):\n",
    "        out_csv = os.path.join(RAW_DIR, f\"airnow_PM25_{day}.csv\")\n",
    "        if os.path.exists(out_csv) and not force:\n",
    "            written.append(out_csv)\n",
    "            continue\n",
    "        try:\n",
    "            csv_txt = fetch_airnow_day(api_key, bbox, day)\n",
    "        except requests.HTTPError as e:\n",
    "            print(f\"HTTP error {e.response.status_code} on {day}. Skipping after retries.\")\n",
    "            continue\n",
    "\n",
    "        if csv_txt.strip():\n",
    "            # Save raw daily file immediately\n",
    "            with open(out_csv, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(csv_txt)\n",
    "            written.append(out_csv)\n",
    "            time.sleep(0.15)  # gentle pacing\n",
    "        else:\n",
    "            # create empty file to mark completion\n",
    "            with open(out_csv, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(\"\")\n",
    "            written.append(out_csv)\n",
    "\n",
    "    # Build consolidated raw and cleaned\n",
    "    parts = []\n",
    "    for fn in sorted(os.listdir(RAW_DIR)):\n",
    "        if not fn.endswith(\".csv\"): \n",
    "            continue\n",
    "        p = os.path.join(RAW_DIR, fn)\n",
    "        if os.path.getsize(p) == 0:\n",
    "            continue\n",
    "        # AirNow daily CSV has no header; read as no-header\n",
    "        df = pd.read_csv(p, header=None)\n",
    "        df = schema_coerce(df)\n",
    "        if df is None:\n",
    "            continue\n",
    "        parts.append(df)\n",
    "\n",
    "    if not parts:\n",
    "        print(\"No AirNow data assembled.\")\n",
    "        return None\n",
    "\n",
    "    raw_all = pd.concat(parts, ignore_index=True)\n",
    "    # Filter to project window again and clean\n",
    "    sd = pd.to_datetime(CONFIG[\"start_date\"]).date()\n",
    "    ed = pd.to_datetime(CONFIG[\"end_date\"]).date()\n",
    "    raw_all = raw_all[(raw_all[\"datetime_utc\"].dt.date >= sd) & (raw_all[\"datetime_utc\"].dt.date <= ed)]\n",
    "\n",
    "    raw_all_path = os.path.join(OUT_DIR, \"airnow_pm25_raw.parquet\")\n",
    "    raw_all.to_parquet(raw_all_path, index=False)\n",
    "\n",
    "    clean = clean_airnow(raw_all.copy(), bbox)\n",
    "\n",
    "    # Aggregate to station-like IDs by lat/lon to dovetail with other sources\n",
    "    # Define a stable sensor_id as rounded lat/lon\n",
    "    clean[\"sensor_id\"] = clean[\"latitude\"].round(5).astype(str) + \",\" + clean[\"longitude\"].round(5).astype(str)\n",
    "\n",
    "    clean_path = os.path.join(OUT_DIR, \"airnow_pm25_clean.parquet\")\n",
    "    clean.to_parquet(clean_path, index=False)\n",
    "\n",
    "    # Daily wide aggregation if needed:\n",
    "    daily = (clean\n",
    "             .groupby([\"sensor_id\", \"date_utc\"], as_index=False)[\"value_ugm3\"]\n",
    "             .mean()\n",
    "             .rename(columns={\"date_utc\":\"obs_date\",\"value_ugm3\":\"pm25_ugm3\"}))\n",
    "    daily_path = os.path.join(OUT_DIR, \"airnow_pm25_daily.parquet\")\n",
    "    daily.to_parquet(daily_path, index=False)\n",
    "\n",
    "    print(f\"Written raw:   {raw_all_path}\")\n",
    "    print(f\"Written clean: {clean_path}\")\n",
    "    print(f\"Written daily: {daily_path}\")\n",
    "    print(\"Rows raw:\", len(raw_all), \"Rows clean:\", len(clean), \"Sensors:\", clean[\"sensor_id\"].nunique())\n",
    "\n",
    "# Run with resume behavior\n",
    "airnow_stream(CONFIG, force=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72f093e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PurpleAir PM2.5 snapshot (CA bbox) with max_age and resume\n",
    "# - fields minimized\n",
    "# - max_age=1440 minutes (24h) to target ~1,100 pages at 1,000 rows/page\n",
    "# - per-page parquet checkpoints for resume\n",
    "# - retries with backoff; graceful 402 stop\n",
    "# - cleaned + calibrated outputs aligned with other datasets\n",
    "\n",
    "# USELESS for the moment -- this is only current data\n",
    "# History requires PurpleAir Premium subscription and different API calls\n",
    "# Keep for reference/future use\n",
    "\n",
    "\n",
    "OUT_DIR = CONFIG[\"out_dir\"]\n",
    "RAW_DIR = os.path.join(OUT_DIR, \"purpleair_raw_pages\")\n",
    "os.makedirs(RAW_DIR, exist_ok=True)\n",
    "\n",
    "PA_BASE   = \"https://api.purpleair.com/v1/sensors\"\n",
    "PA_FIELDS = \"sensor_index,pm2.5_atm,humidity,temperature,latitude,longitude,last_seen\"\n",
    "PA_LIMIT  = 1000\n",
    "PA_LOC    = 0          # 0 = outdoor\n",
    "MAX_AGE_M = 1440       # last 24 hours\n",
    "MAX_PAGES = 1100       # target cap\n",
    "BACKOFF_BASE = 0.8\n",
    "\n",
    "def fetch_page(session, api_key, bbox, page, max_retries=6, timeout=180):\n",
    "    headers = {\"X-API-Key\": api_key}\n",
    "    params = {\n",
    "        \"fields\": PA_FIELDS,\n",
    "        \"location_type\": PA_LOC,\n",
    "        \"nwlng\": bbox[\"nwlng\"], \"nwlat\": bbox[\"nwlat\"],\n",
    "        \"selng\": bbox[\"selng\"], \"selat\": bbox[\"selat\"],\n",
    "        \"limit\": PA_LIMIT, \"page\": page,\n",
    "        \"max_age\": MAX_AGE_M\n",
    "    }\n",
    "    for attempt in range(max_retries):\n",
    "        r = session.get(PA_BASE, headers=headers, params=params, timeout=timeout)\n",
    "        if r.status_code == 402:\n",
    "            # Payment required; keep what we have and stop paging\n",
    "            return \"PAYWALL\", None\n",
    "        if r.status_code in (429, 500, 502, 503, 504):\n",
    "            # retry with exponential backoff\n",
    "            if attempt == max_retries - 1:\n",
    "                r.raise_for_status()\n",
    "            time.sleep(BACKOFF_BASE * (2 ** attempt))\n",
    "            continue\n",
    "        r.raise_for_status()\n",
    "        js = r.json()\n",
    "        return \"OK\", js\n",
    "    return \"ERROR\", None\n",
    "\n",
    "def coerce_schema(df):\n",
    "    for c in [\"latitude\",\"longitude\",\"pm2.5_atm\",\"humidity\",\"temperature\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    if \"sensor_index\" in df.columns:\n",
    "        df[\"sensor_index\"] = pd.to_numeric(df[\"sensor_index\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    if \"last_seen\" in df.columns:\n",
    "        df[\"last_seen_utc\"] = pd.to_datetime(df[\"last_seen\"], unit=\"s\", utc=True, errors=\"coerce\")\n",
    "        df[\"hour_utc\"] = df[\"last_seen_utc\"].dt.floor(\"H\")\n",
    "        df[\"date_utc\"] = df[\"last_seen_utc\"].dt.date\n",
    "    return df\n",
    "\n",
    "def in_bbox(df, b):\n",
    "    return df[\n",
    "        (df[\"longitude\"] >= b[\"nwlng\"]) & (df[\"longitude\"] <= b[\"selng\"]) &\n",
    "        (df[\"latitude\"]  >= b[\"selat\"]) & (df[\"latitude\"]  <= b[\"nwlat\"])\n",
    "    ]\n",
    "\n",
    "def clean_calibrate(df, bbox, start_iso=None, end_iso=None):\n",
    "    df = df.dropna(subset=[\"latitude\",\"longitude\",\"pm2.5_atm\",\"last_seen_utc\"])\n",
    "    df = in_bbox(df, bbox)\n",
    "    if start_iso and end_iso:\n",
    "        sd = pd.to_datetime(start_iso).date()\n",
    "        ed = pd.to_datetime(end_iso).date()\n",
    "        df = df[(df[\"date_utc\"] >= sd) & (df[\"date_utc\"] <= ed)]\n",
    "    # sanity filters\n",
    "    df = df[(df[\"pm2.5_atm\"].between(0, 1000))]\n",
    "    if \"humidity\" in df:\n",
    "        df = df[(df[\"humidity\"].between(0, 100)) | df[\"humidity\"].isna()]\n",
    "    if \"temperature\" in df:\n",
    "        df = df[(df[\"temperature\"].between(-40, 60)) | df[\"temperature\"].isna()]\n",
    "    # calibration\n",
    "    rh = df[\"humidity\"].fillna(0) if \"humidity\" in df else 0\n",
    "    df[\"pm25_corrected_ugm3\"] = (0.52*df[\"pm2.5_atm\"] - 0.086*rh + 5.75).clip(lower=0)\n",
    "    # stable id\n",
    "    df[\"sensor_id\"] = df[\"sensor_index\"].astype(str) if \"sensor_index\" in df else df[\"latitude\"].round(5).astype(str)+\",\"+df[\"longitude\"].round(5).astype(str)\n",
    "    # dedup per sensor-hour\n",
    "    if \"hour_utc\" in df:\n",
    "        df = df.sort_values([\"sensor_id\",\"hour_utc\"]).drop_duplicates([\"sensor_id\",\"hour_utc\"], keep=\"last\")\n",
    "    return df\n",
    "\n",
    "def purpleair_run(CONFIG, force=False):\n",
    "    if not CONFIG.get(\"PURPLEAIR_API_KEY\"):\n",
    "        print(\"Skipping PurpleAir (missing API key).\")\n",
    "        return\n",
    "\n",
    "    bbox = CONFIG[\"bbox\"]\n",
    "    api_key = CONFIG[\"PURPLEAIR_API_KEY\"]\n",
    "    session = requests.Session()\n",
    "\n",
    "    # page loop with resume\n",
    "    page = 1\n",
    "    page_files = []\n",
    "    while page <= MAX_PAGES:\n",
    "        page_path = os.path.join(RAW_DIR, f\"purpleair_page_{page:04d}.parquet\")\n",
    "        if os.path.exists(page_path) and not force:\n",
    "            page_files.append(page_path)\n",
    "            page += 1\n",
    "            continue\n",
    "\n",
    "        status, js = fetch_page(session, api_key, bbox, page)\n",
    "        if status == \"PAYWALL\":\n",
    "            print(f\"Stopped at paywall on page {page}. Check MAX_AGE or plan.\")\n",
    "            break\n",
    "        if status != \"OK\" or js is None:\n",
    "            print(f\"Stopped on error at page {page}.\")\n",
    "            break\n",
    "\n",
    "        rows = js.get(\"data\", [])\n",
    "        if not rows:\n",
    "            break\n",
    "\n",
    "        cols = js.get(\"fields\", PA_FIELDS.split(\",\"))\n",
    "        df = pd.DataFrame(rows, columns=cols)\n",
    "        df = coerce_schema(df)\n",
    "\n",
    "        # write page checkpoint\n",
    "        df.to_parquet(page_path, index=False)\n",
    "        page_files.append(page_path)\n",
    "\n",
    "        if len(rows) < PA_LIMIT:\n",
    "            break\n",
    "        page += 1\n",
    "        time.sleep(0.15)\n",
    "\n",
    "    if not page_files:\n",
    "        print(\"No PurpleAir pages written.\")\n",
    "        return\n",
    "\n",
    "    # consolidate\n",
    "    parts = [pd.read_parquet(p) for p in page_files if os.path.getsize(p) > 0]\n",
    "    if not parts:\n",
    "        print(\"No PurpleAir data assembled.\")\n",
    "        return\n",
    "    raw_all = pd.concat(parts, ignore_index=True)\n",
    "    raw_all = coerce_schema(raw_all)\n",
    "\n",
    "    raw_path = os.path.join(OUT_DIR, \"purpleair_raw.parquet\")\n",
    "    raw_all.to_parquet(raw_path, index=False)\n",
    "\n",
    "    # clean + calibrate\n",
    "    clean = clean_calibrate(raw_all.copy(), bbox, CONFIG[\"start_date\"], CONFIG[\"end_date\"])\n",
    "    clean_path = os.path.join(OUT_DIR, \"purpleair_clean.parquet\")\n",
    "    clean.to_parquet(clean_path, index=False)\n",
    "\n",
    "    # hourly aggregate\n",
    "    hourly = (clean.groupby([\"sensor_id\",\"hour_utc\"], as_index=False)\n",
    "              .agg(latitude=(\"latitude\",\"mean\"),\n",
    "                   longitude=(\"longitude\",\"mean\"),\n",
    "                   pm25_atm_ugm3=(\"pm2.5_atm\",\"mean\"),\n",
    "                   pm25_corrected_ugm3=(\"pm25_corrected_ugm3\",\"mean\")))\n",
    "    hourly_path = os.path.join(OUT_DIR, \"purpleair_hourly.parquet\")\n",
    "    hourly.to_parquet(hourly_path, index=False)\n",
    "\n",
    "    print(f\"Pages saved: {len(page_files)} (target cap {MAX_PAGES})\")\n",
    "    print(f\"Saved raw:   {raw_path} rows={len(raw_all)}\")\n",
    "    print(f\"Saved clean: {clean_path} rows={len(clean)} sensors={clean['sensor_id'].nunique()}\")\n",
    "    print(f\"Saved hourly:{hourly_path} rows={len(hourly)}\")\n",
    "\n",
    "purpleair_run(CONFIG, force=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9c1533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AQS PM2.5 (88101) hourly for CA, daily slicing with resume\n",
    "# - requires AQS_EMAIL and AQS_KEY in CONFIG\n",
    "# variables repeated for clarity\n",
    "\n",
    "OUT_DIR   = CONFIG[\"out_dir\"]\n",
    "AQS_EMAIL = CONFIG.get(\"AQS_EMAIL\", \"\")\n",
    "AQS_KEY   = CONFIG.get(\"AQS_KEY\", \"\")\n",
    "BBOX      = CONFIG[\"bbox\"]\n",
    "START_ISO = CONFIG[\"start_date\"]\n",
    "END_ISO   = CONFIG[\"end_date\"]\n",
    "\n",
    "if not AQS_EMAIL or not AQS_KEY:\n",
    "    print(\"Skipping AQS (missing AQS_EMAIL/AQS_KEY in CONFIG).\")\n",
    "else:\n",
    "    RAW_DIR = os.path.join(OUT_DIR, \"aqs_raw_daily\")\n",
    "    os.makedirs(RAW_DIR, exist_ok=True)\n",
    "\n",
    "    BASE = \"https://aqs.epa.gov/data/api/sampleData/byBox\"\n",
    "    PARAM = \"88101\"\n",
    "    PT = pytz.timezone(\"America/Los_Angeles\")\n",
    "    UTC = pytz.UTC\n",
    "    MAX_RETRIES = 6\n",
    "    BACKOFF_BASE = 0.8\n",
    "    TIMEOUT = 120  # tighter per-day timeout\n",
    "\n",
    "    def day_slices(start_iso, end_iso):\n",
    "        s = dt.fromisoformat(start_iso).date()\n",
    "        e = dt.fromisoformat(end_iso).date()\n",
    "        d = s\n",
    "        while d <= e:\n",
    "            ds = d.isoformat()\n",
    "            yield ds, ds\n",
    "            d += timedelta(days=1)\n",
    "\n",
    "    def fetch_day(bdate, edate):\n",
    "        params = dict(\n",
    "            email=AQS_EMAIL, key=AQS_KEY, param=PARAM,\n",
    "            bdate=bdate.replace(\"-\", \"\"), edate=edate.replace(\"-\", \"\"),\n",
    "            minlat=BBOX[\"selat\"], maxlat=BBOX[\"nwlat\"],\n",
    "            minlon=BBOX[\"nwlng\"], maxlon=BBOX[\"selng\"],\n",
    "        )\n",
    "        headers = {\"Accept\":\"application/json\"}\n",
    "        for a in range(MAX_RETRIES):\n",
    "            try:\n",
    "                r = requests.get(BASE, params=params, headers=headers, timeout=TIMEOUT)\n",
    "                if r.status_code in (429,500,502,503,504):\n",
    "                    if a == MAX_RETRIES-1: r.raise_for_status()\n",
    "                    time.sleep(BACKOFF_BASE*(2**a)); continue\n",
    "                if r.status_code == 400:\n",
    "                    try:\n",
    "                        print(\"AQS 400:\", r.json().get(\"Header\"))\n",
    "                    except Exception:\n",
    "                        print(\"AQS 400 raw:\", r.text[:400])\n",
    "                r.raise_for_status()\n",
    "                js = r.json()\n",
    "                return js.get(\"Data\", []) or js.get(\"data\", [])\n",
    "            except requests.exceptions.ReadTimeout:\n",
    "                if a == MAX_RETRIES-1: raise\n",
    "                time.sleep(BACKOFF_BASE*(2**a))\n",
    "        return []\n",
    "\n",
    "    def parse_times(df):\n",
    "        if {\"date_gmt\",\"time_gmt\"}.issubset(df.columns):\n",
    "            dt_gmt = pd.to_datetime(df[\"date_gmt\"].astype(str)+\" \"+df[\"time_gmt\"].astype(str),\n",
    "                                    errors=\"coerce\", utc=True)\n",
    "            df[\"datetime_utc\"] = dt_gmt\n",
    "        else:\n",
    "            dt_loc = pd.to_datetime(df[\"date_local\"].astype(str)+\" \"+df[\"time_local\"].astype(str),\n",
    "                                    errors=\"coerce\")\n",
    "            dt_loc = dt_loc.apply(lambda x: PT.localize(x) if pd.notna(x) else x)\n",
    "            df[\"datetime_utc\"] = pd.to_datetime(dt_loc).dt.tz_convert(UTC)\n",
    "        df[\"datetime_pt\"] = df[\"datetime_utc\"].dt.tz_convert(PT)\n",
    "        df[\"hour_utc\"] = df[\"datetime_utc\"].dt.floor(\"H\")\n",
    "        return df\n",
    "\n",
    "    def clean_aqs(df):\n",
    "        if df.empty: return df\n",
    "        val_col = \"sample_measurement\" if \"sample_measurement\" in df.columns else (\"value\" if \"value\" in df.columns else None)\n",
    "        lat_col = \"latitude\" if \"latitude\" in df.columns else (\"site_latitude\" if \"site_latitude\" in df.columns else None)\n",
    "        lon_col = \"longitude\" if \"longitude\" in df.columns else (\"site_longitude\" if \"site_longitude\" in df.columns else None)\n",
    "\n",
    "        for c in [val_col, lat_col, lon_col]:\n",
    "            if c in df.columns: df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "        # bbox and ranges\n",
    "        if lat_col and lon_col:\n",
    "            df = df[(df[lon_col].between(BBOX[\"nwlng\"], BBOX[\"selng\"])) &\n",
    "                    (df[lat_col].between(BBOX[\"selat\"], BBOX[\"nwlat\"]))]\n",
    "        df = df[(df[val_col].between(0, 1000))]\n",
    "\n",
    "        df = parse_times(df)\n",
    "\n",
    "        # station id\n",
    "        if {\"state_code\",\"county_code\",\"site_number\",\"poc\"}.issubset(df.columns):\n",
    "            df[\"station_id\"] = (df[\"state_code\"].astype(str)+\"-\"+df[\"county_code\"].astype(str)+\"-\"+\n",
    "                                df[\"site_number\"].astype(str)+\"-\"+df[\"poc\"].astype(str))\n",
    "        elif \"aqs_site_id\" in df.columns:\n",
    "            df[\"station_id\"] = df[\"aqs_site_id\"].astype(str)\n",
    "        else:\n",
    "            df[\"station_id\"] = (df.get(lat_col, pd.Series(dtype=float)).round(5).astype(str)+\",\"+\n",
    "                                df.get(lon_col, pd.Series(dtype=float)).round(5).astype(str))\n",
    "\n",
    "        df = df.rename(columns={val_col:\"pm25_ugm3\", lat_col:\"latitude\", lon_col:\"longitude\"})\n",
    "        df = df.sort_values([\"station_id\",\"hour_utc\"]).drop_duplicates([\"station_id\",\"hour_utc\"], keep=\"last\")\n",
    "\n",
    "        keep = [\"station_id\",\"latitude\",\"longitude\",\"datetime_utc\",\"datetime_pt\",\"hour_utc\",\"pm25_ugm3\"]\n",
    "        meta = [c for c in [\"method\",\"method_code\",\"units\",\"units_of_measure\",\"sample_duration\",\"qc_status\"] if c in df.columns]\n",
    "        return df[keep+meta].reset_index(drop=True)\n",
    "\n",
    "    # ---- Run: daily resume ----\n",
    "    daily_files = []\n",
    "    for b, e in day_slices(START_ISO, END_ISO):\n",
    "        raw_csv = os.path.join(RAW_DIR, f\"aqs_88101_{b}.csv\")\n",
    "        if not os.path.exists(raw_csv):\n",
    "            data = fetch_day(b, e)\n",
    "            if data:\n",
    "                pd.json_normalize(data).to_csv(raw_csv, index=False)\n",
    "            else:\n",
    "                open(raw_csv, \"w\").close()\n",
    "            time.sleep(0.1)\n",
    "        daily_files.append(raw_csv)\n",
    "    print(f\"Days fetched or skipped: {len(daily_files)} → {RAW_DIR}\")\n",
    "\n",
    "    # consolidate\n",
    "    parts = [pd.read_csv(p, low_memory=False) for p in daily_files if os.path.getsize(p) > 0]\n",
    "    if not parts:\n",
    "        print(\"No AQS data to assemble.\"); raise SystemExit\n",
    "    raw_all = pd.concat(parts, ignore_index=True)\n",
    "\n",
    "    # clean\n",
    "    clean = clean_aqs(raw_all.copy())\n",
    "\n",
    "    # enforce project window in UTC\n",
    "    sd = pd.to_datetime(START_ISO).tz_localize(\"UTC\")\n",
    "    ed = pd.to_datetime(END_ISO).tz_localize(\"UTC\") + pd.Timedelta(hours=23, minutes=59)\n",
    "    clean = clean[(clean[\"datetime_utc\"] >= sd) & (clean[\"datetime_utc\"] <= ed)]\n",
    "\n",
    "    # save outputs\n",
    "    raw_path   = os.path.join(OUT_DIR, \"aqs_pm25_raw.parquet\")\n",
    "    clean_path = os.path.join(OUT_DIR, \"aqs_pm25_clean.parquet\")\n",
    "    daily_path = os.path.join(OUT_DIR, \"aqs_pm25_daily.parquet\")\n",
    "\n",
    "    raw_all.to_parquet(raw_path, index=False)\n",
    "    clean.to_parquet(clean_path, index=False)\n",
    "    daily = (clean.assign(obs_date=clean[\"datetime_pt\"].dt.date)\n",
    "                   .groupby([\"station_id\",\"obs_date\"], as_index=False)[\"pm25_ugm3\"].mean())\n",
    "    daily.to_parquet(daily_path, index=False)\n",
    "\n",
    "    print(f\"Saved raw:   {raw_path}  rows={len(raw_all)}\")\n",
    "    print(f\"Saved clean: {clean_path} rows={len(clean)} stations={clean['station_id'].nunique()}\")\n",
    "    print(f\"Saved daily: {daily_path} rows={len(daily)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5559247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USCRN 2024 hourly → raw + clean with resume, bbox/time filters, and QA\n",
    "# we'll move the imports back up later \n",
    "\n",
    "import os, re, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from datetime import datetime as dt, timezone\n",
    "import pytz\n",
    "\n",
    "BASE_HOURLY = CONFIG[\"USCRN_BASE_HOURLY\"]\n",
    "OUT_DIR     = CONFIG[\"out_dir\"]\n",
    "UA          = CONFIG[\"USER_AGENT_HEADERS\"]\n",
    "BBOX        = CONFIG[\"bbox\"]\n",
    "PT          = pytz.timezone(\"America/Los_Angeles\")\n",
    "\n",
    "RAW_DIR   = os.path.join(OUT_DIR, \"uscrn_raw\")\n",
    "CLEAN_DIR = os.path.join(OUT_DIR, \"uscrn_clean\")\n",
    "os.makedirs(RAW_DIR, exist_ok=True); os.makedirs(CLEAN_DIR, exist_ok=True)\n",
    "\n",
    "# enforce 2024 only\n",
    "# better to do this purely from config but we're locking it here for now\n",
    "years = [y for y in CONFIG.get(\"USCRN_YEARS\", []) if y == 2024] or [2024]\n",
    "if years != [2024]:\n",
    "    print(\"USCRN_YEARS adjusted to [2024] for this run.\")\n",
    "stations_cfg = CONFIG[\"USCRN_STATION_NAME\"]\n",
    "if not isinstance(stations_cfg, list):\n",
    "    stations_cfg = [stations_cfg]\n",
    "stations_cfg = [s for s in stations_cfg if s and str(s).strip()]\n",
    "\n",
    "def list_links(url):\n",
    "    r = requests.get(url, headers=UA, timeout=120)\n",
    "    r.raise_for_status()\n",
    "    soup = BeautifulSoup(r.text, \"lxml\")\n",
    "    return [a[\"href\"] for a in soup.find_all(\"a\", href=True)]\n",
    "\n",
    "def normalize(s):\n",
    "    return re.sub(r\"[^a-z0-9]\", \"\", str(s).lower())\n",
    "\n",
    "def within_bbox(lat, lon, b=BBOX):\n",
    "    return (b[\"selat\"] <= lat <= b[\"nwlat\"]) and (b[\"nwlng\"] <= lon <= b[\"selng\"])\n",
    "\n",
    "def download_uscrn_files(force=False):\n",
    "    downloaded = 0\n",
    "    for station_raw in stations_cfg:\n",
    "        # match variants for filenames\n",
    "        variants_norm = {\n",
    "            normalize(station_raw),\n",
    "            normalize(station_raw.replace(\" \", \"_\")),\n",
    "            normalize(station_raw.replace(\" \", \"-\")),\n",
    "            normalize(station_raw.replace(\" \", \"\")),\n",
    "        }\n",
    "        for y in years:\n",
    "            year_url = urljoin(BASE_HOURLY, f\"{y}/\")\n",
    "            try:\n",
    "                hrefs = list_links(year_url)\n",
    "            except Exception as e:\n",
    "                print(f\"{y}: index error → {e}\")\n",
    "                continue\n",
    "            txts = [h for h in hrefs if h.lower().endswith(\".txt\") and not re.search(r\"readme|header\", h, re.I)]\n",
    "            if not txts:\n",
    "                print(f\"{y}: no .txt files\")\n",
    "                continue\n",
    "            matched = [h for h in txts if any(v in normalize(h) for v in variants_norm)]\n",
    "            if not matched:\n",
    "                print(f\"{y}: no files matched {station_raw}\")\n",
    "                continue\n",
    "            for h in matched:\n",
    "                url = urljoin(year_url, h)\n",
    "                out = os.path.join(RAW_DIR, os.path.basename(url))\n",
    "                if os.path.exists(out) and not force:\n",
    "                    # resume: keep existing\n",
    "                    continue\n",
    "                try:\n",
    "                    with requests.get(url, headers=UA, stream=True, timeout=300) as r:\n",
    "                        r.raise_for_status()\n",
    "                        with open(out, \"wb\") as f:\n",
    "                            for chunk in r.iter_content(1<<16):\n",
    "                                if chunk: f.write(chunk)\n",
    "                    downloaded += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"skip {url} → {e}\")\n",
    "                    continue\n",
    "    print(f\"Raw USCRN files ready. New downloads: {downloaded}\")\n",
    "\n",
    "def parse_uscrn_hourly_file(filepath):\n",
    "    # CRNH02 hourly v2.5 columns (subset + flags). Many files are whitespace-delimited.\n",
    "    cols = [\"WBANNO\",\"UTC_DATE\",\"UTC_TIME\",\"LST_DATE\",\"LST_TIME\",\"CRX_VN\",\"LONGITUDE\",\"LATITUDE\",\n",
    "            \"T_CALC\",\"T_HR_AVG\",\"T_MAX\",\"T_MIN\",\"P_CALC\",\n",
    "            \"SOLARAD\",\"SOLARAD_FLAG\",\"SOLARAD_MAX\",\"SOLARAD_MAX_FLAG\",\"SOLARAD_MIN\",\"SOLARAD_MIN_FLAG\",\n",
    "            \"SUR_TEMP_TYPE\",\"SUR_TEMP\",\"SUR_TEMP_FLAG\",\"SUR_TEMP_MAX\",\"SUR_TEMP_MAX_FLAG\",\"SUR_TEMP_MIN\",\"SUR_TEMP_MIN_FLAG\",\n",
    "            \"RH_HR_AVG\",\"RH_HR_AVG_FLAG\",\n",
    "            \"SOIL_MOISTURE_5\",\"SOIL_MOISTURE_10\",\"SOIL_MOISTURE_20\",\"SOIL_MOISTURE_50\",\"SOIL_MOISTURE_100\",\n",
    "            \"SOIL_TEMP_5\",\"SOIL_TEMP_10\",\"SOIL_TEMP_20\",\"SOIL_TEMP_50\",\"SOIL_TEMP_100\"]\n",
    "    recs = []\n",
    "    with open(filepath, \"r\") as f:\n",
    "        for line in f:\n",
    "            s = line.strip()\n",
    "            if not s or s.startswith(\"#\"):  # skip blanks and comments\n",
    "                continue\n",
    "            parts = s.split()\n",
    "            if len(parts) < 28:  # minimal length guard\n",
    "                continue\n",
    "            parts = parts[:len(cols)] + [\"\"]*(len(cols)-len(parts)) if len(parts) < len(cols) else parts[:len(cols)]\n",
    "            recs.append(parts)\n",
    "    if not recs:\n",
    "        return pd.DataFrame(columns=cols)\n",
    "    df = pd.DataFrame(recs, columns=cols)\n",
    "\n",
    "    # numeric coercion; USCRN sentinel often -9999 or -99\n",
    "    num_cols = [\"LONGITUDE\",\"LATITUDE\",\"T_CALC\",\"T_HR_AVG\",\"T_MAX\",\"T_MIN\",\"P_CALC\",\n",
    "                \"SOLARAD\",\"SOLARAD_MAX\",\"SOLARAD_MIN\",\"SUR_TEMP\",\"SUR_TEMP_MAX\",\"SUR_TEMP_MIN\",\n",
    "                \"RH_HR_AVG\",\"SOIL_MOISTURE_5\",\"SOIL_MOISTURE_10\",\"SOIL_MOISTURE_20\",\"SOIL_MOISTURE_50\",\"SOIL_MOISTURE_100\",\n",
    "                \"SOIL_TEMP_5\",\"SOIL_TEMP_10\",\"SOIL_TEMP_20\",\"SOIL_TEMP_50\",\"SOIL_TEMP_100\"]\n",
    "    for c in num_cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    # time parsing\n",
    "    df[\"UTC_DATETIME\"] = pd.to_datetime(df[\"UTC_DATE\"].astype(str) + df[\"UTC_TIME\"].astype(str), format=\"%Y%m%d%H%M\", errors=\"coerce\").dt.tz_localize(\"UTC\")\n",
    "    # LST (file’s local standard time) is not TZ-aware; for CA we convert UTC → PT explicitly for alignment\n",
    "    df[\"LST_DATETIME_PT\"] = df[\"UTC_DATETIME\"].dt.tz_convert(PT)\n",
    "\n",
    "    # filter obvious missings (USCRN uses -9999 etc.)\n",
    "    for c in [\"T_HR_AVG\",\"T_MAX\",\"T_MIN\",\"P_CALC\",\"SOLARAD\",\"SUR_TEMP\",\"RH_HR_AVG\"]:\n",
    "        if c in df.columns:\n",
    "            df.loc[df[c] <= -999, c] = np.nan\n",
    "\n",
    "    # range sanity checks (kept conservative)\n",
    "    rng = {\n",
    "        \"T_HR_AVG\": (-60, 60), \"T_MAX\": (-60, 60), \"T_MIN\": (-80, 60),\n",
    "        \"P_CALC\": (0, 500),    # mm/h\n",
    "        \"RH_HR_AVG\": (0, 100),\n",
    "        \"SOLARAD\": (0, 1400),  # W/m^2\n",
    "    }\n",
    "    for c,(lo,hi) in rng.items():\n",
    "        if c in df.columns:\n",
    "            df.loc[~df[c].between(lo, hi), c] = np.nan\n",
    "\n",
    "    # keep only rows with coordinates in CA bbox\n",
    "    df = df.dropna(subset=[\"LATITUDE\",\"LONGITUDE\"])\n",
    "    df = df[df.apply(lambda r: within_bbox(r[\"LATITUDE\"], r[\"LONGITUDE\"]), axis=1)]\n",
    "\n",
    "    return df\n",
    "\n",
    "def build_clean_for_window():\n",
    "    # read all downloaded files\n",
    "    files = [os.path.join(RAW_DIR, f) for f in os.listdir(RAW_DIR) if f.endswith(\".txt\")]\n",
    "    if not files:\n",
    "        print(\"No USCRN raw .txt files found.\")\n",
    "        return None\n",
    "\n",
    "    parts = []\n",
    "    for fp in files:\n",
    "        try:\n",
    "            df = parse_uscrn_hourly_file(fp)\n",
    "            if not df.empty:\n",
    "                parts.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"parse error {os.path.basename(fp)} → {e}\")\n",
    "\n",
    "    if not parts:\n",
    "        print(\"No parsable USCRN data.\")\n",
    "        return None\n",
    "\n",
    "    df = pd.concat(parts, ignore_index=True)\n",
    "\n",
    "    # enforce project window in UTC\n",
    "    sd = pd.to_datetime(CONFIG[\"start_date\"]).tz_localize(\"UTC\")\n",
    "    ed = pd.to_datetime(CONFIG[\"end_date\"]).tz_localize(\"UTC\") + pd.Timedelta(hours=23, minutes=59)\n",
    "    df = df[(df[\"UTC_DATETIME\"] >= sd) & (df[\"UTC_DATETIME\"] <= ed)]\n",
    "\n",
    "    # rename to standard columns\n",
    "    df = df.rename(columns={\n",
    "        \"WBANNO\":\"station_id\",\n",
    "        \"LONGITUDE\":\"longitude\",\n",
    "        \"LATITUDE\":\"latitude\",\n",
    "        \"T_CALC\":\"temperature_calculated_c\",\n",
    "        \"T_HR_AVG\":\"air_temperature_hourly_avg_c\",\n",
    "        \"T_MAX\":\"air_temperature_hourly_max_c\",\n",
    "        \"T_MIN\":\"air_temperature_hourly_min_c\",\n",
    "        \"P_CALC\":\"precipitation_mm_hourly\",\n",
    "        \"SOLARAD\":\"solar_radiation_wm2\",\n",
    "        \"SUR_TEMP\":\"surface_temperature_c\",\n",
    "        \"RH_HR_AVG\":\"relative_humidity_percent\",\n",
    "        \"UTC_DATETIME\":\"datetime_utc\",\n",
    "        \"LST_DATETIME_PT\":\"datetime_pt\",\n",
    "    })\n",
    "\n",
    "    # station_id stable type\n",
    "    df[\"station_id\"] = df[\"station_id\"].astype(str)\n",
    "\n",
    "    # de-dup per station-hour (keep last)\n",
    "    df[\"hour_utc\"] = df[\"datetime_utc\"].dt.floor(\"H\")\n",
    "    df = df.sort_values([\"station_id\",\"hour_utc\"]).drop_duplicates(subset=[\"station_id\",\"hour_utc\"], keep=\"last\")\n",
    "\n",
    "    # save\n",
    "    raw_out   = os.path.join(CLEAN_DIR, \"uscrn_2024_hourly_raw.parquet\")\n",
    "    clean_out = os.path.join(OUT_DIR, \"uscrn_2024_hourly_clean.parquet\")\n",
    "    df.to_parquet(raw_out, index=False)\n",
    "    # select clean analysis columns\n",
    "    keep = [\"station_id\",\"latitude\",\"longitude\",\"datetime_utc\",\"datetime_pt\",\n",
    "            \"air_temperature_hourly_avg_c\",\"air_temperature_hourly_max_c\",\"air_temperature_hourly_min_c\",\n",
    "            \"precipitation_mm_hourly\",\"relative_humidity_percent\",\"solar_radiation_wm2\",\"surface_temperature_c\"]\n",
    "    df[keep].to_parquet(clean_out, index=False)\n",
    "\n",
    "    # summary\n",
    "    print(f\"Saved raw:   {raw_out}\")\n",
    "    print(f\"Saved clean: {clean_out}\")\n",
    "    print(\"Rows:\", len(df), \"Stations:\", df[\"station_id\"].nunique(),\n",
    "          \"Dates:\", df[\"datetime_utc\"].dt.date.nunique())\n",
    "\n",
    "# ---- Run: resume-friendly ----\n",
    "download_uscrn_files(force=False)\n",
    "build_clean_for_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf2b7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HRRR → CA subset via wgrib2 (robust) → NetCDF; delete original GRIB\n",
    "# IGNORE THIS FOR NOW\n",
    "# Keeping it here for reference/future use\n",
    "\n",
    "\n",
    "import os, shutil, subprocess, shlex, requests, time\n",
    "from datetime import datetime as dt, timedelta\n",
    "\n",
    "OUT_DIR   = CONFIG[\"out_dir\"]\n",
    "RAW_DIR   = os.path.join(OUT_DIR, \"hrrr_raw\");  os.makedirs(RAW_DIR, exist_ok=True)\n",
    "CLEAN_DIR = os.path.join(OUT_DIR, \"hrrr_ca\");   os.makedirs(CLEAN_DIR, exist_ok=True)\n",
    "\n",
    "# California bbox\n",
    "LON_LEFT, LON_RIGHT = CONFIG[\"bbox\"][\"nwlng\"], CONFIG[\"bbox\"][\"selng\"]\n",
    "LAT_TOP, LAT_BOTTOM = CONFIG[\"bbox\"][\"nwlat\"], CONFIG[\"bbox\"][\"selat\"]\n",
    "\n",
    "START = dt.fromisoformat(CONFIG[\"start_date\"])\n",
    "END   = dt.fromisoformat(CONFIG[\"end_date\"])\n",
    "\n",
    "AWS_TPL = \"https://noaa-hrrr-bdp-pds.s3.amazonaws.com/hrrr.{ymd}/conus/hrrr.t{hh}z.wrfsfcf00.grib2\"\n",
    "\n",
    "# Resolve wgrib2 path (set env var WGRIB2 to override)\n",
    "WGRIB2 = os.environ.get(\"WGRIB2\") or shutil.which(\"wgrib2\")\n",
    "if not WGRIB2:\n",
    "    raise SystemExit(\"wgrib2 not found. Install (conda-forge) or set env var WGRIB2 to its full path.\")\n",
    "\n",
    "def download(url, dst, timeout=900):\n",
    "    with requests.get(url, stream=True, timeout=timeout) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(dst, \"wb\") as f:\n",
    "            for chunk in r.iter_content(1<<20):\n",
    "                if chunk: f.write(chunk)\n",
    "\n",
    "def run_cmd(cmd_list):\n",
    "    \"\"\"Run and raise with readable stderr on failure.\"\"\"\n",
    "    p = subprocess.run(cmd_list, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "    if p.returncode != 0:\n",
    "        raise RuntimeError(f\"CMD failed:\\n{' '.join(cmd_list)}\\n--- STDERR ---\\n{p.stderr[:2000]}\")\n",
    "    return p.stdout\n",
    "\n",
    "def subset_to_nc(src_grib, dst_nc):\n",
    "    import os, subprocess\n",
    "\n",
    "    def run(cmd):\n",
    "        p = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "        if p.returncode != 0:\n",
    "            raise RuntimeError(f\"CMD failed:\\n{' '.join(cmd)}\\n--- STDERR ---\\n{p.stderr[:2000]}\")\n",
    "        return p.stdout\n",
    "\n",
    "    tmp_tmp  = src_grib + \".TMP.ca.grib2\"\n",
    "    tmp_dpt  = src_grib + \".DPT.ca.grib2\"\n",
    "    both_grb = src_grib + \".CA.grib2\"\n",
    "    for fp in (tmp_tmp, tmp_dpt, both_grb):\n",
    "        if os.path.exists(fp):\n",
    "            try: os.remove(fp)\n",
    "            except: pass\n",
    "\n",
    "    # TMP 2 m → tmp_tmp\n",
    "    run([\n",
    "        WGRIB2, src_grib,\n",
    "        \"-match\", \":TMP:2 m above ground:\",\n",
    "        \"-small_grib\", f\"{LON_LEFT}:{LON_RIGHT}\", f\"{LAT_BOTTOM}:{LAT_TOP}\",\n",
    "        tmp_tmp\n",
    "    ])\n",
    "\n",
    "    # DPT 2 m → tmp_dpt (may not exist every hour)\n",
    "    have_dpt = True\n",
    "    try:\n",
    "        run([\n",
    "            WGRIB2, src_grib,\n",
    "            \"-match\", \":DPT:2 m above ground:\",\n",
    "            \"-small_grib\", f\"{LON_LEFT}:{LON_RIGHT}\", f\"{LAT_BOTTOM}:{LAT_TOP}\",\n",
    "            tmp_dpt\n",
    "        ])\n",
    "    except Exception:\n",
    "        have_dpt = False\n",
    "\n",
    "    # CONCAT as raw bytes (avoids wgrib2 -cat)\n",
    "    with open(both_grb, \"wb\") as out_f:\n",
    "        with open(tmp_tmp, \"rb\") as a: out_f.write(a.read())\n",
    "        if have_dpt and os.path.exists(tmp_dpt):\n",
    "            with open(tmp_dpt, \"rb\") as b: out_f.write(b.read())\n",
    "\n",
    "    # GRIB -> NetCDF\n",
    "    run([WGRIB2, both_grb, \"-netcdf\", dst_nc])\n",
    "\n",
    "    # cleanup\n",
    "    for fp in (tmp_tmp, tmp_dpt, both_grb):\n",
    "        if os.path.exists(fp):\n",
    "            try: os.remove(fp)\n",
    "            except: pass\n",
    "\n",
    "\n",
    "def run():\n",
    "    cur = START\n",
    "    while cur <= END:\n",
    "        for hh in range(24):\n",
    "            url = AWS_TPL.format(ymd=cur.strftime(\"%Y%m%d\"), hh=f\"{hh:02d}\")\n",
    "            raw_name = f\"hrrr_{cur:%Y%m%d}_{hh:02d}z_f00.grib2\"\n",
    "            raw_path = os.path.join(RAW_DIR, raw_name)\n",
    "            out_name = f\"hrrr_ca_{cur:%Y%m%d}_{hh:02d}z.nc\"\n",
    "            out_path = os.path.join(CLEAN_DIR, out_name)\n",
    "\n",
    "            if os.path.exists(out_path):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                print(\"→\", url)\n",
    "                download(url, raw_path)\n",
    "                subset_to_nc(raw_path, out_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Skip {url}: {e}\")\n",
    "            finally:\n",
    "                if os.path.exists(raw_path):\n",
    "                    try: os.remove(raw_path)\n",
    "                    except: pass\n",
    "            time.sleep(0.05)\n",
    "        cur += timedelta(days=1)\n",
    "    print(f\"Done. NetCDF files in {CLEAN_DIR}\")\n",
    "\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86240c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HMS Smoke Polygons + Fire Points (CA) → daily Parquet + consolidated Parquet\n",
    "# Uses CONFIG for bbox, dates, base URLs, headers. Local paths only.\n",
    "# repeating configs for clarity\n",
    "\n",
    "OUT_DIR = CONFIG[\"out_dir\"]; os.makedirs(OUT_DIR, exist_ok=True)\n",
    "BBOX = CONFIG[\"bbox\"]\n",
    "BBOX_POLY = box(BBOX[\"nwlng\"], BBOX[\"selat\"], BBOX[\"selng\"], BBOX[\"nwlat\"])\n",
    "\n",
    "BASE_SMOKE = CONFIG[\"HMS_BASE_SMOKE\"]\n",
    "BASE_FIRE  = CONFIG[\"HMS_BASE_FIRE\"]\n",
    "HDRS = CONFIG[\"USER_AGENT_HEADERS\"]\n",
    "\n",
    "START_ISO = CONFIG[\"start_date\"]  # \"2024-06-01\"\n",
    "END_ISO   = CONFIG[\"end_date\"]    # \"2024-10-31\"\n",
    "\n",
    "RAW_DIR   = os.path.join(OUT_DIR, \"hms_raw\")\n",
    "CLEAN_DIR = os.path.join(OUT_DIR, \"hms_clean\")\n",
    "os.makedirs(RAW_DIR, exist_ok=True)\n",
    "os.makedirs(CLEAN_DIR, exist_ok=True)\n",
    "\n",
    "def ensure_wgs84(gdf):\n",
    "    if gdf.crs is None: return gdf.set_crs(4326)\n",
    "    return gdf.to_crs(4326) if gdf.crs.to_epsg() != 4326 else gdf\n",
    "\n",
    "def parse_dt_series(gdf, col):\n",
    "    if not col or col not in gdf.columns: return pd.Series([pd.NaT]*len(gdf))\n",
    "    try:\n",
    "        return pd.to_datetime(gdf[col], utc=True, errors=\"coerce\", format=\"mixed\")\n",
    "    except TypeError:\n",
    "        return pd.to_datetime(gdf[col], utc=True, errors=\"coerce\")\n",
    "\n",
    "def read_first_shp(extracted_dir):\n",
    "    for fn in os.listdir(extracted_dir):\n",
    "        if fn.lower().endswith(\".shp\"):\n",
    "            return os.path.join(extracted_dir, fn)\n",
    "    return None\n",
    "\n",
    "def download_zip(url, out_zip):\n",
    "    try:\n",
    "        r = requests.get(url, headers=HDRS, timeout=120, allow_redirects=True)\n",
    "        if r.status_code != 200:\n",
    "            return False, f\"HTTP {r.status_code}\"\n",
    "        os.makedirs(os.path.dirname(out_zip), exist_ok=True)\n",
    "        with open(out_zip, \"wb\") as f: f.write(r.content)\n",
    "        return True, \"OK\"\n",
    "    except Exception as e:\n",
    "        return False, str(e)\n",
    "\n",
    "def clean_smoke(gdf):\n",
    "    gdf = ensure_wgs84(gdf)\n",
    "    cols = {c.lower(): c for c in gdf.columns}\n",
    "    start_col = cols.get(\"start\") or cols.get(\"starttime\") or cols.get(\"start_time\")\n",
    "    end_col   = cols.get(\"end\")   or cols.get(\"endtime\")   or cols.get(\"end_time\")\n",
    "    dens_col  = cols.get(\"density\") or cols.get(\"smoke\") or cols.get(\"category\")\n",
    "\n",
    "    start_utc = parse_dt_series(gdf, start_col)\n",
    "    end_utc   = parse_dt_series(gdf, end_col)\n",
    "    density   = gdf[dens_col].astype(str).str.title() if dens_col else pd.Series([\"Unknown\"]*len(gdf))\n",
    "\n",
    "    # compute area + centroid in Albers (CONUS)\n",
    "    gdf_aea   = gdf.to_crs(5070)\n",
    "    area_km2  = (gdf_aea.area / 1e6).round(3)\n",
    "    cent_ll   = gpd.GeoSeries(gdf_aea.centroid, crs=5070).to_crs(4326)\n",
    "\n",
    "    # keep rows intersecting CA bbox\n",
    "    mask = gdf.intersects(BBOX_POLY)\n",
    "    gdf = gdf.loc[mask]\n",
    "    area_km2 = area_km2.loc[mask]\n",
    "    cent_ll  = cent_ll.loc[mask]\n",
    "    start_utc = start_utc.loc[mask]\n",
    "    end_utc   = end_utc.loc[mask]\n",
    "    density   = density.loc[mask]\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"start_utc\": start_utc,\n",
    "        \"end_utc\": end_utc,\n",
    "        \"density\": density,\n",
    "        \"area_km2\": area_km2.values,\n",
    "        \"centroid_lat\": cent_ll.y.round(5).values,\n",
    "        \"centroid_lon\": cent_ll.x.round(5).values,\n",
    "    })\n",
    "    # carry a few metadata columns if present\n",
    "    for k in [\"source\",\"satellite\",\"sensor\",\"analysis_time\",\"id\",\"polygon_id\",\"poly_id\"]:\n",
    "        if k in cols: df[k] = gdf[cols[k]].astype(str).values\n",
    "    return df\n",
    "\n",
    "def clean_fire(gdf):\n",
    "    gdf = ensure_wgs84(gdf)\n",
    "    gdf = gdf[gdf.geometry.notnull()].copy()\n",
    "    # explode MultiPoint if any\n",
    "    if \"Multi\" in \",\".join(gdf.geometry.geom_type.unique()):\n",
    "        gdf = gdf.explode(index_parts=False, ignore_index=True)\n",
    "\n",
    "    cols = {c.lower(): c for c in gdf.columns}\n",
    "    start_col = cols.get(\"start\") or cols.get(\"starttime\") or cols.get(\"start_time\")\n",
    "    end_col   = cols.get(\"end\")   or cols.get(\"endtime\")   or cols.get(\"end_time\")\n",
    "    frp_col   = cols.get(\"frp\") or cols.get(\"fire_radiative_power\")\n",
    "    type_col  = cols.get(\"type\") or cols.get(\"source\")\n",
    "\n",
    "    # CA bbox filter first\n",
    "    gdf = gdf[gdf.intersects(BBOX_POLY)]\n",
    "    if gdf.empty:\n",
    "        return pd.DataFrame(columns=[\"start_utc\",\"end_utc\",\"lat\",\"lon\",\"frp\",\"type\"])\n",
    "\n",
    "    start_utc = parse_dt_series(gdf, start_col)\n",
    "    end_utc   = parse_dt_series(gdf, end_col)\n",
    "    lat = gdf.geometry.y.round(5); lon = gdf.geometry.x.round(5)\n",
    "    frp = pd.to_numeric(gdf[frp_col], errors=\"coerce\") if frp_col in gdf.columns else pd.Series([pd.NA]*len(gdf))\n",
    "    typ = gdf[type_col].astype(str) if type_col in gdf.columns else pd.Series([\"Unknown\"]*len(gdf))\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"start_utc\": start_utc,\n",
    "        \"end_utc\": end_utc,\n",
    "        \"lat\": lat.values,\n",
    "        \"lon\": lon.values,\n",
    "        \"frp\": frp.values,\n",
    "        \"type\": typ.values,\n",
    "    })\n",
    "    for k in [\"id\",\"point_id\",\"poly_id\",\"event_id\"]:\n",
    "        if k in cols: df[k] = gdf[cols[k]].astype(str).values\n",
    "    return df\n",
    "\n",
    "def daily_url(kind, d):\n",
    "    y, m, ymd = d.strftime(\"%Y\"), d.strftime(\"%m\"), d.strftime(\"%Y%m%d\")\n",
    "    base = BASE_SMOKE if kind==\"smoke\" else BASE_FIRE\n",
    "    zip_name = f\"hms_{kind}{ymd}.zip\"\n",
    "    url = f\"{base}/{y}/{m}/{zip_name}\"\n",
    "    return url, zip_name, y, m, ymd\n",
    "\n",
    "def process_hms_range(kind):\n",
    "    all_parts = []\n",
    "    for d in pd.date_range(START_ISO, END_ISO, freq=\"D\"):\n",
    "        url, zip_name, y, m, ymd = daily_url(kind, d)\n",
    "        raw_dir_d = os.path.join(RAW_DIR, kind, y, m); os.makedirs(raw_dir_d, exist_ok=True)\n",
    "        clean_dir_d = os.path.join(CLEAN_DIR, kind, y, m); os.makedirs(clean_dir_d, exist_ok=True)\n",
    "\n",
    "        out_zip = os.path.join(raw_dir_d, zip_name)\n",
    "        out_parq = os.path.join(clean_dir_d, f\"hms_{kind}_{ymd}.parquet\")\n",
    "\n",
    "        if os.path.exists(out_parq):\n",
    "            # already processed\n",
    "            continue\n",
    "\n",
    "        ok, msg = download_zip(url, out_zip)\n",
    "        if not ok:\n",
    "            # 404 or other issue; skip day quietly\n",
    "            # print(f\"[{kind}] {ymd} skip: {msg}\")\n",
    "            continue\n",
    "\n",
    "        # extract to temp, read .shp, clean, save parquet, cleanup\n",
    "        tmp_dir = f\"/tmp/hms_{kind}_{ymd}\"; os.makedirs(tmp_dir, exist_ok=True)\n",
    "        try:\n",
    "            with zipfile.ZipFile(out_zip, \"r\") as zf: zf.extractall(tmp_dir)\n",
    "            shp_path = read_first_shp(tmp_dir)\n",
    "            if not shp_path:\n",
    "                # print(f\"[{kind}] {ymd} has no shp\")\n",
    "                shutil.rmtree(tmp_dir, ignore_errors=True)\n",
    "                continue\n",
    "\n",
    "            gdf = gpd.read_file(shp_path)\n",
    "            clean = clean_smoke(gdf) if kind==\"smoke\" else clean_fire(gdf)\n",
    "\n",
    "            if not clean.empty:\n",
    "                clean[\"obs_date\"] = d.date()\n",
    "                clean.to_parquet(out_parq, index=False)\n",
    "                all_parts.append(clean)\n",
    "        except zipfile.BadZipFile:\n",
    "            pass\n",
    "        except Exception as e:\n",
    "            # print(f\"[{kind}] {ymd} clean error: {e}\")\n",
    "            pass\n",
    "        finally:\n",
    "            shutil.rmtree(tmp_dir, ignore_errors=True)\n",
    "\n",
    "    # consolidate to window-level parquet\n",
    "    if all_parts:\n",
    "        big = pd.concat(all_parts, ignore_index=True)\n",
    "        big_path = os.path.join(OUT_DIR, f\"hms_{kind}_{START_ISO}_to_{END_ISO}.parquet\")\n",
    "        big.to_parquet(big_path, index=False)\n",
    "        print(f\"[{kind}] Consolidated -> {big_path} rows={len(big)}\")\n",
    "    else:\n",
    "        print(f\"[{kind}] No data in {START_ISO}..{END_ISO} for CA bbox.\")\n",
    "\n",
    "# Run both\n",
    "process_hms_range(\"smoke\")\n",
    "process_hms_range(\"fire\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21f53ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHP 2023 → download → extract → CA clip → sample to grid → Parquet lookups\n",
    "# move the imports back up later\n",
    "\n",
    "import os, zipfile, glob, requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "from shapely.geometry import box, mapping\n",
    "from shapely.ops import transform as shp_transform\n",
    "from pyproj import Transformer\n",
    "\n",
    "# Paths\n",
    "OUT_DIR = CONFIG[\"out_dir\"]; os.makedirs(OUT_DIR, exist_ok=True)\n",
    "MANUAL_DIR = os.path.join(OUT_DIR, \"manual\"); os.makedirs(MANUAL_DIR, exist_ok=True)\n",
    "LOOKUPS_DIR = os.path.join(OUT_DIR, \"lookups\"); os.makedirs(LOOKUPS_DIR, exist_ok=True)\n",
    "\n",
    "# Inputs from CONFIG\n",
    "WHP_ZIP_URL   = CONFIG[\"WHP_ZIP\"]        # e.g., FS archive zip URL\n",
    "WHP_TIF_NAME  = CONFIG[\"WHP_TIF_NAME\"]   # e.g., Data/whp2023_GeoTIF/whp2023_cnt_conus.tif\n",
    "BBOX = CONFIG[\"bbox\"]\n",
    "bbox_wgs84 = box(BBOX[\"nwlng\"], BBOX[\"selat\"], BBOX[\"selng\"], BBOX[\"nwlat\"])\n",
    "\n",
    "# Local filenames\n",
    "zip_local = os.path.join(MANUAL_DIR, os.path.basename(WHP_ZIP_URL))\n",
    "raw_tif   = os.path.join(MANUAL_DIR, os.path.basename(WHP_TIF_NAME))\n",
    "clip_tif  = os.path.join(OUT_DIR, \"whp_ca_clip.tif\")\n",
    "lookup_parq = os.path.join(LOOKUPS_DIR, \"whp_grid.parquet\")\n",
    "\n",
    "# 1) Download zip if missing\n",
    "if not os.path.exists(zip_local):\n",
    "    print(\"Downloading WHP zip →\", zip_local)\n",
    "    with requests.get(WHP_ZIP_URL, stream=True, timeout=600) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(zip_local, \"wb\") as f:\n",
    "            for chunk in r.iter_content(1<<20):\n",
    "                if chunk: f.write(chunk)\n",
    "else:\n",
    "    print(\"Using existing zip:\", zip_local)\n",
    "\n",
    "# 2) Extract target GeoTIFF if missing\n",
    "if not os.path.exists(raw_tif):\n",
    "    with zipfile.ZipFile(zip_local, \"r\") as zf:\n",
    "        if WHP_TIF_NAME not in zf.namelist():\n",
    "            raise FileNotFoundError(f\"{WHP_TIF_NAME} not found inside {zip_local}\")\n",
    "        with zf.open(WHP_TIF_NAME) as zsrc, open(raw_tif, \"wb\") as dst:\n",
    "            dst.write(zsrc.read())\n",
    "    print(\"Extracted:\", raw_tif)\n",
    "else:\n",
    "    print(\"Using existing GeoTIFF:\", raw_tif)\n",
    "\n",
    "# 3) Clip to CA bbox\n",
    "if not os.path.exists(clip_tif):\n",
    "    with rasterio.open(raw_tif) as src:\n",
    "        if src.crs is None:\n",
    "            raise ValueError(\"WHP raster has no CRS.\")\n",
    "        if src.crs.to_epsg() != 4326:\n",
    "            to_src = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True).transform\n",
    "            geom = [mapping(shp_transform(to_src, bbox_wgs84))]\n",
    "        else:\n",
    "            geom = [mapping(bbox_wgs84)]\n",
    "        out, transform = mask(src, geom, crop=True)\n",
    "        meta = src.meta.copy()\n",
    "        meta.update({\"height\": out.shape[1], \"width\": out.shape[2], \"transform\": transform, \"compress\": \"LZW\"})\n",
    "        with rasterio.open(clip_tif, \"w\", **meta) as dst:\n",
    "            dst.write(out)\n",
    "    print(\"Clipped WHP saved →\", clip_tif)\n",
    "else:\n",
    "    print(\"Using existing clip:\", clip_tif)\n",
    "\n",
    "# 4) Sample WHP to grid centroids → Parquet lookup\n",
    "# Pick latest grid_*m.parquet\n",
    "grid_candidates = sorted(glob.glob(os.path.join(OUT_DIR, \"grid_*m.parquet\")))\n",
    "if not grid_candidates:\n",
    "    print(\"No grid_*.parquet found; skipping grid sampling.\")\n",
    "else:\n",
    "    grid_path = grid_candidates[-1]\n",
    "    grid = gpd.read_parquet(grid_path)\n",
    "    if grid.crs is None:\n",
    "        grid = grid.set_crs(f\"EPSG:{CONFIG['crs_epsg']}\")\n",
    "    if grid.crs.to_epsg() != 4326:\n",
    "        grid = grid.to_crs(4326)\n",
    "\n",
    "    cent = grid.geometry.centroid\n",
    "    pts = gpd.GeoDataFrame(grid[[\"geounit_id\"]].copy(), geometry=cent, crs=grid.crs)\n",
    "\n",
    "    with rasterio.open(clip_tif) as src:\n",
    "        # reproject points if needed\n",
    "        pts_src = pts.to_crs(src.crs) if src.crs.to_epsg() != 4326 else pts\n",
    "        coords = [(geom.x, geom.y) for geom in pts_src.geometry]\n",
    "        vals = list(src.sample(coords))\n",
    "        whp = np.array([v[0] if (v is not None) else np.nan for v in vals])\n",
    "\n",
    "    out_df = pd.DataFrame({\"geounit_id\": pts[\"geounit_id\"], \"whp_score\": whp})\n",
    "    out_df.to_parquet(lookup_parq, index=False)\n",
    "    print(\"Saved WHP→grid lookup →\", lookup_parq)\n",
    "\n",
    "print(\"WHP done. (Static 2023 layer; time-invariant across your 2024 window.)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00896154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CDC SVI 2022 → California subset → tidy → Parquet lookups (county + tract)\n",
    "# move te he imports back up later\n",
    "\n",
    "import os, glob\n",
    "import pandas as pd\n",
    "\n",
    "OUT_DIR = CONFIG[\"out_dir\"]\n",
    "MANUAL = os.path.join(OUT_DIR, \"manual\")\n",
    "LOOKUPS = os.path.join(OUT_DIR, \"lookups\"); os.makedirs(LOOKUPS, exist_ok=True)\n",
    "\n",
    "# Place these files (national coverage) in results/manual/:\n",
    "#   SVI_2022_US_COUNTY.csv   and/or   SVI_2022_US_TRACT.csv\n",
    "county_csv = None\n",
    "tract_csv  = None\n",
    "for fn in os.listdir(MANUAL):\n",
    "    f = fn.lower()\n",
    "    if f.endswith(\".csv\") and \"svi_2022\" in f and \"county\" in f: county_csv = os.path.join(MANUAL, fn)\n",
    "    if f.endswith(\".csv\") and \"svi_2022\" in f and \"tract\"  in f: tract_csv  = os.path.join(MANUAL, fn)\n",
    "\n",
    "def _normal_cols(df):\n",
    "    u = {c.upper(): c for c in df.columns}\n",
    "    # robust column detection\n",
    "    col = lambda *opts: next((u[o] for o in opts if o in u), None)\n",
    "    return {\n",
    "        \"st_abbr\": col(\"STATE\",\"ST\",\"ST_ABBR\"),\n",
    "        \"state_fips\": col(\"STATEFIPS\",\"STATE_FIPS\",\"FIPS_STATE\",\"STCNTY\",\"STFIPS\"),\n",
    "        \"county\": col(\"COUNTY\",\"COUNTY_NAME\"),\n",
    "        \"county_fips\": col(\"FIPS\",\"GEOID\",\"COUNTYFIPS\",\"COUNTY_FIPS\"),\n",
    "        \"geoid\": col(\"GEOID\",\"FIPS\",\"TRACT\",\"TRACTFIPS\",\"TRACTFIPSID\"),\n",
    "        # SVI key fields (names are standard in CDC SVI 2022)\n",
    "        \"rpl_theme1\": col(\"RPL_THEME1\"),\n",
    "        \"rpl_theme2\": col(\"RPL_THEME2\"),\n",
    "        \"rpl_theme3\": col(\"RPL_THEME3\"),\n",
    "        \"rpl_theme4\": col(\"RPL_THEME4\",\"RPL_THE4\"),  # some earlier docs show THE4\n",
    "        \"rpl_themes\": col(\"RPL_THEMES\"),\n",
    "        \"e_totpop\": col(\"E_TOTPOP\"),\n",
    "        \"ep_pov\": col(\"EP_POV\"),\n",
    "        \"ep_unemp\": col(\"EP_UNEMP\"),\n",
    "        \"ep_age65\": col(\"EP_AGE65\"),\n",
    "        \"ep_age17\": col(\"EP_AGE17\"),\n",
    "        \"ep_disabl\": col(\"EP_DISABL\"),\n",
    "        \"ep_sngpnt\": col(\"EP_SNGPNT\"),\n",
    "    }\n",
    "\n",
    "def _subset_ca(df, cols, level):\n",
    "    # Determine CA via state code or abbrev\n",
    "    if cols[\"st_abbr\"] and df[cols[\"st_abbr\"]].astype(str).str.upper().isin([\"CA\",\"CALIFORNIA\"]).any():\n",
    "        m = df[cols[\"st_abbr\"]].astype(str).str.upper().isin([\"CA\",\"CALIFORNIA\"])\n",
    "        ca = df.loc[m].copy()\n",
    "    elif cols[\"state_fips\"]:\n",
    "        ca = df.loc[df[cols[\"state_fips\"]].astype(str).str.zfill(2).eq(\"06\")].copy()\n",
    "    else:\n",
    "        raise RuntimeError(f\"Cannot detect state column for {level}.\")\n",
    "    return ca\n",
    "\n",
    "def _keep_fields(df, cols, level):\n",
    "    wanted = []\n",
    "    id_cols = []\n",
    "    if level == \"county\":\n",
    "        # prefer 5-digit county FIPS (state+county); CDC often provides FIPS directly\n",
    "        if cols[\"county_fips\"]:\n",
    "            df[\"county_geoid\"] = df[cols[\"county_fips\"]].astype(str).str.zfill(5)\n",
    "            id_cols += [\"county_geoid\"]\n",
    "        if cols[\"county\"]: id_cols += [cols[\"county\"]]\n",
    "    else:  # tract\n",
    "        if cols[\"geoid\"]:\n",
    "            df[\"tract_geoid\"] = df[cols[\"geoid\"]].astype(str).str.zfill(11)\n",
    "            id_cols += [\"tract_geoid\"]\n",
    "\n",
    "    value_cols = [c for k,c in cols.items() if k.startswith(\"rpl_\") and c] + \\\n",
    "                 [c for k,c in cols.items() if k in (\"e_totpop\",\"ep_pov\",\"ep_unemp\",\"ep_age65\",\"ep_age17\",\"ep_disabl\",\"ep_sngpnt\") and c]\n",
    "\n",
    "    wanted = list(dict.fromkeys(id_cols + value_cols))  # dedupe preserve order\n",
    "    return df[wanted].copy()\n",
    "\n",
    "def process_one(path, level):\n",
    "    print(f\"Processing SVI {level}: {path}\")\n",
    "    df = pd.read_csv(path, dtype=str, low_memory=False)\n",
    "    cols = _normal_cols(df)\n",
    "    ca = _subset_ca(df, cols, level)\n",
    "    tidy = _keep_fields(ca, cols, level)\n",
    "\n",
    "    # cast numeric where applicable\n",
    "    num_cols = [c for c in tidy.columns if c.startswith(\"RPL_\") or c.startswith(\"EP_\") or c.startswith(\"E_\")]\n",
    "    for c in num_cols:\n",
    "        if c in tidy.columns:\n",
    "            tidy[c] = pd.to_numeric(tidy[c], errors=\"coerce\")\n",
    "\n",
    "    out_parq = os.path.join(LOOKUPS, f\"svi_2022_ca_{level}.parquet\")\n",
    "    tidy.to_parquet(out_parq, index=False)\n",
    "    print(\"Saved →\", out_parq, f\"rows={len(tidy)}\")\n",
    "    return tidy\n",
    "\n",
    "# Run whichever files you provided\n",
    "out = {}\n",
    "if county_csv:\n",
    "    out[\"county\"] = process_one(county_csv, \"county\")\n",
    "else:\n",
    "    print(\"County CSV not found in results/manual/. Skipping county.\")\n",
    "\n",
    "if tract_csv:\n",
    "    out[\"tract\"] = process_one(tract_csv, \"tract\")\n",
    "else:\n",
    "    print(\"Tract CSV not found in results/manual/. Skipping tract.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9c2077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ACS 2023: California tracts (all counties) ---\n",
    "# move the imports back up later\n",
    "\n",
    "import os, requests, pandas as pd\n",
    "\n",
    "BASE_OUT = CONFIG[\"out_dir\"]\n",
    "KEY = CONFIG[\"ACS_KEY\"]  # from CONFIG\n",
    "ACS5 = CONFIG[\"ACS_BASE_ACS5\"]\n",
    "SUBJ = CONFIG[\"ACS_BASE_SUBJECT\"]\n",
    "\n",
    "# Define statewide geography\n",
    "GEO = {\"for\": \"tract:*\", \"in\": \"state:06 county:*\"}\n",
    "\n",
    "# --- 1) Detailed tables ---\n",
    "vars_acs5 = \"NAME,B01001_001E,B19013_001E\"  # total pop, median income\n",
    "p_acs5 = {\"get\": vars_acs5, **GEO, \"key\": KEY}\n",
    "r1 = requests.get(ACS5, params=p_acs5, timeout=120)\n",
    "r1.raise_for_status()\n",
    "cols1, rows1 = r1.json()[0], r1.json()[1:]\n",
    "df1 = pd.DataFrame(rows1, columns=cols1)\n",
    "\n",
    "# --- 2) Subject tables ---\n",
    "vars_subj = \"S0101_C02_001E,S1701_C03_001E\"  # % age 65+, % below poverty\n",
    "p_subj = {\"get\": vars_subj, **GEO, \"key\": KEY}\n",
    "r2 = requests.get(SUBJ, params=p_subj, timeout=120)\n",
    "r2.raise_for_status()\n",
    "cols2, rows2 = r2.json()[0], r2.json()[1:]\n",
    "df2 = pd.DataFrame(rows2, columns=cols2)\n",
    "\n",
    "# --- 3) Merge + rename ---\n",
    "key_cols = [\"state\", \"county\", \"tract\"]\n",
    "acs = (\n",
    "    df1.merge(df2, on=key_cols, how=\"inner\")\n",
    "       .rename(columns={\n",
    "           \"B01001_001E\": \"total_pop\",\n",
    "           \"B19013_001E\": \"median_income\",\n",
    "           \"S0101_C02_001E\": \"pct_age65plus\",\n",
    "           \"S1701_C03_001E\": \"pct_poverty\"\n",
    "       })\n",
    ")\n",
    "\n",
    "# Convert numeric columns\n",
    "for c in [\"total_pop\", \"median_income\", \"pct_age65plus\", \"pct_poverty\"]:\n",
    "    acs[c] = pd.to_numeric(acs[c], errors=\"coerce\")\n",
    "\n",
    "# --- 4) Save ---\n",
    "out_csv = os.path.join(BASE_OUT, \"acs_2023_california_tracts.csv\")\n",
    "acs.to_csv(out_csv, index=False)\n",
    "print(f\"Saved statewide ACS data → {out_csv} (rows={len(acs)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1085f34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLCD 2024 (CONUS) → CA clip → statewide summary CSV → grid lookup Parquet\n",
    "# move the imports back up later\n",
    "\n",
    "\n",
    "import os, zipfile, glob, shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "from shapely.geometry import box, mapping\n",
    "from shapely.ops import transform as shp_transform\n",
    "from pyproj import Transformer\n",
    "\n",
    "OUT_DIR = CONFIG[\"out_dir\"]; os.makedirs(OUT_DIR, exist_ok=True)\n",
    "MANUAL_ZIP = CONFIG[\"NLCD_MANUAL_ZIP\"]  # e.g., results/manual/Annual_NLCD_LndCov_2024_CU_C1V1.zip\n",
    "TEMP_DIR = os.path.join(OUT_DIR, \"nlcd_tmp\"); os.makedirs(TEMP_DIR, exist_ok=True)\n",
    "LOOKUPS_DIR = os.path.join(OUT_DIR, \"lookups\"); os.makedirs(LOOKUPS_DIR, exist_ok=True)\n",
    "\n",
    "CSV_SUMMARY = os.path.join(OUT_DIR, \"nlcd_2024_ca_summary.csv\")\n",
    "CLIP_PATH   = os.path.join(OUT_DIR, \"nlcd_2024_ca_clip.tif\")\n",
    "NLCD_CLASSES = {\n",
    "    11:\"Open Water\",12:\"Perennial Ice/Snow\",21:\"Developed, Open Space\",22:\"Developed, Low Intensity\",\n",
    "    23:\"Developed, Medium Intensity\",24:\"Developed, High Intensity\",31:\"Barren Land\",41:\"Deciduous Forest\",\n",
    "    42:\"Evergreen Forest\",43:\"Mixed Forest\",52:\"Shrub/Scrub\",71:\"Grassland/Herbaceous\",81:\"Pasture/Hay\",\n",
    "    82:\"Cultivated Crops\",90:\"Woody Wetlands\",95:\"Emergent Herbaceous Wetlands\",250:\"NoData\"\n",
    "}\n",
    "\n",
    "# 1) Extract target raster from manual ZIP (GeoTIFF or IMG)\n",
    "assert os.path.exists(MANUAL_ZIP), f\"Missing manual zip: {MANUAL_ZIP}\"\n",
    "with zipfile.ZipFile(MANUAL_ZIP) as zf:\n",
    "    members = zf.namelist()\n",
    "    candidates = [m for m in members if m.lower().endswith((\".tif\",\".img\"))]\n",
    "    if not candidates:\n",
    "        raise RuntimeError(\"No GeoTIFF/IMG found inside NLCD zip.\")\n",
    "    target = candidates[0]\n",
    "    zf.extract(target, TEMP_DIR)\n",
    "    RASTER_PATH = os.path.join(TEMP_DIR, os.path.basename(target))\n",
    "print(\"Extracted:\", RASTER_PATH)\n",
    "\n",
    "# 2) Clip to CA bbox\n",
    "BBOX = CONFIG[\"bbox\"]\n",
    "bbox_wgs84 = box(BBOX[\"nwlng\"], BBOX[\"selat\"], BBOX[\"selng\"], BBOX[\"nwlat\"])\n",
    "with rasterio.open(RASTER_PATH) as src:\n",
    "    if src.crs is None:\n",
    "        raise ValueError(\"NLCD raster has no CRS.\")\n",
    "    if src.crs.to_epsg() != 4326:\n",
    "        to_src = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True).transform\n",
    "        geom = [mapping(shp_transform(to_src, bbox_wgs84))]\n",
    "    else:\n",
    "        geom = [mapping(bbox_wgs84)]\n",
    "    out, transform = mask(src, geom, crop=True)\n",
    "    meta = src.meta.copy()\n",
    "    meta.update({\"height\": out.shape[1], \"width\": out.shape[2], \"transform\": transform, \"compress\":\"LZW\"})\n",
    "    with rasterio.open(CLIP_PATH, \"w\", **meta) as dst:\n",
    "        dst.write(out)\n",
    "print(\"Clipped NLCD →\", CLIP_PATH)\n",
    "\n",
    "# 3) Statewide histogram summary\n",
    "with rasterio.open(CLIP_PATH) as src:\n",
    "    arr = src.read(1)\n",
    "    res_x, res_y = src.res\n",
    "    pix_area = abs(res_x * res_y)\n",
    "    nodata_vals = set(v for v in (src.nodata, 250) if v is not None)\n",
    "    mask_valid = np.ones(arr.shape, bool)\n",
    "    for nd in nodata_vals: mask_valid &= (arr != nd)\n",
    "    vals, counts = np.unique(arr[mask_valid], return_counts=True)\n",
    "\n",
    "rows = []\n",
    "total = int(counts.sum()) if len(counts) else 0\n",
    "for v, c in zip(vals, counts):\n",
    "    rows.append({\n",
    "        \"class_value\": int(v),\n",
    "        \"class_name\": NLCD_CLASSES.get(int(v), \"Unknown\"),\n",
    "        \"pixel_count\": int(c),\n",
    "        \"area_m2\": float(c * pix_area),\n",
    "        \"area_km2\": float(c * pix_area / 1e6),\n",
    "        \"percent\": float((c / total) * 100 if total else 0)\n",
    "    })\n",
    "pd.DataFrame(rows).sort_values(\"class_value\").to_csv(CSV_SUMMARY, index=False)\n",
    "print(\"Saved NLCD summary CSV:\", CSV_SUMMARY)\n",
    "\n",
    "# 4) Sample NLCD class to grid centroids → lookup Parquet\n",
    "grid_candidates = sorted(glob.glob(os.path.join(OUT_DIR, \"grid_*m.parquet\")))\n",
    "if grid_candidates:\n",
    "    grid_path = grid_candidates[-1]\n",
    "    grid = gpd.read_parquet(grid_path)\n",
    "    if grid.crs is None:\n",
    "        grid = grid.set_crs(f\"EPSG:{CONFIG['crs_epsg']}\")\n",
    "    if grid.crs.to_epsg() != 4326:\n",
    "        grid = grid.to_crs(4326)\n",
    "    cent = grid.geometry.centroid\n",
    "    pts = gpd.GeoDataFrame(grid[[\"geounit_id\"]].copy(), geometry=cent, crs=grid.crs)\n",
    "\n",
    "    with rasterio.open(CLIP_PATH) as src:\n",
    "        pts_src = pts.to_crs(src.crs) if src.crs.to_epsg() != 4326 else pts\n",
    "        coords = [(g.x, g.y) for g in pts_src.geometry]\n",
    "        vals = list(src.sample(coords))\n",
    "        lc = np.array([int(v[0]) if (v is not None and not np.isnan(v[0])) else 250 for v in vals], dtype=int)\n",
    "\n",
    "    out_df = pd.DataFrame({\n",
    "        \"geounit_id\": pts[\"geounit_id\"].values,\n",
    "        \"nlcd_class\": lc,\n",
    "        \"nlcd_class_name\": [NLCD_CLASSES.get(int(x), \"Unknown\") for x in lc]\n",
    "    })\n",
    "    out_df.to_parquet(os.path.join(LOOKUPS_DIR, \"nlcd_grid.parquet\"), index=False)\n",
    "    print(\"Saved grid lookup →\", os.path.join(LOOKUPS_DIR, \"nlcd_grid.parquet\"))\n",
    "else:\n",
    "    print(\"No grid_*.parquet found; skipping grid sampling.\")\n",
    "\n",
    "# 5) Clean temp files\n",
    "shutil.rmtree(TEMP_DIR, ignore_errors=True)\n",
    "print(\"Temp cleaned. NLCD is static for 2024 analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608ef24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentinel-2 NDVI (median, 2024-06-01..2024-10-31) → 2×2 tiled GeoTIFFs saved locally\n",
    "# move the imports back up later\n",
    "\n",
    "import os, math, time, requests\n",
    "import ee\n",
    "\n",
    "OUT_DIR = CONFIG[\"out_dir\"]  # e.g., /Users/Shared/blueleaflabs/heatshield/results\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Initialize Earth Engine (assumes you've run: earthengine authenticate)\n",
    "ee.Initialize(project='blueleaflabs')\n",
    "\n",
    "# ---- Inputs: statewide CA bbox + dates from CONFIG ----\n",
    "bbox = CONFIG[\"bbox\"]\n",
    "REGION = ee.Geometry.Rectangle([bbox[\"nwlng\"], bbox[\"selat\"], bbox[\"selng\"], bbox[\"nwlat\"]], geodesic=False)\n",
    "START = ee.Date(CONFIG[\"start_date\"])  # \"2024-06-01\"\n",
    "END   = ee.Date(CONFIG[\"end_date\"])    # \"2024-10-31\"\n",
    "\n",
    "# ---- Build Sentinel-2 L2A collection with cloud mask ----\n",
    "def mask_s2_sr(img):\n",
    "    qa = img.select(\"QA60\")\n",
    "    cloud = qa.bitwiseAnd(1<<10).neq(0).Or(qa.bitwiseAnd(1<<11).neq(0))\n",
    "    return img.updateMask(cloud.Not())\n",
    "\n",
    "s2 = (ee.ImageCollection(\"COPERNICUS/S2_SR_HARMONIZED\")\n",
    "      .filterBounds(REGION)\n",
    "      .filterDate(START, END)\n",
    "      .map(mask_s2_sr))\n",
    "\n",
    "ndvi = s2.median().normalizedDifference([\"B8\",\"B4\"]).rename(\"NDVI\")\n",
    "\n",
    "# ---- Tile CA bbox into 2×2 to keep file sizes reasonable at 100 m ----\n",
    "# --- replace the split_rect + tiles lines with this ---\n",
    "\n",
    "def split_rect_client(rect, nx=2, ny=2):\n",
    "    # Pull coords to client\n",
    "    coords = rect.coordinates().getInfo()[0]  # [[xmin,ymin],[xmin,ymax],[xmax,ymax],[xmax,ymin],...]\n",
    "    xmin, ymin = coords[0]\n",
    "    xmax, ymax = coords[2]\n",
    "    dx = (xmax - xmin) / nx\n",
    "    dy = (ymax - ymin) / ny\n",
    "    tiles = []\n",
    "    for i in range(nx):\n",
    "        for j in range(ny):\n",
    "            x0 = xmin + dx*i\n",
    "            x1 = xmin + dx*(i+1)\n",
    "            y0 = ymin + dy*j\n",
    "            y1 = ymin + dy*(j+1)\n",
    "            tiles.append(ee.Geometry.Rectangle([x0, y0, x1, y1], geodesic=False))\n",
    "    return tiles\n",
    "\n",
    "tiles = split_rect_client(REGION, nx=4, ny=4)\n",
    "\n",
    "# ---- Download helper using signed URLs (synchronous) ----\n",
    "def download_tile(image, geom, out_path, scale=250, crs=\"EPSG:4326\",\n",
    "                  max_attempts=4, scale_fallbacks=(250, 300, 500)):\n",
    "    region_geojson = json.dumps(geom.getInfo())  # client-side region\n",
    "    for attempt in range(1, max_attempts+1):\n",
    "        try:\n",
    "            # try primary scale first, then fallbacks\n",
    "            for sc in ([scale] + [s for s in scale_fallbacks if s != scale]):\n",
    "                params = {\n",
    "                    \"scale\": sc,\n",
    "                    \"crs\": crs,\n",
    "                    \"region\": region_geojson,\n",
    "                    \"filePerBand\": False,\n",
    "                    \"format\": \"GEO_TIFF\",\n",
    "                }\n",
    "                # regenerate signed URL EACH attempt\n",
    "                url = image.getDownloadURL(params)\n",
    "                try:\n",
    "                    with requests.get(url, stream=True, timeout=600) as r:\n",
    "                        r.raise_for_status()\n",
    "                        with open(out_path, \"wb\") as f:\n",
    "                            for chunk in r.iter_content(1<<20):\n",
    "                                if chunk: f.write(chunk)\n",
    "                    return True  # success\n",
    "                except requests.HTTPError as http_err:\n",
    "                    # try next scale in fallbacks\n",
    "                    last_err = http_err\n",
    "                    continue\n",
    "            # if all scales failed this attempt, back off and retry\n",
    "            time.sleep(2 * attempt)\n",
    "        except EEException as ee_err:\n",
    "            last_err = ee_err\n",
    "            time.sleep(2 * attempt)\n",
    "    print(f\"  ✖ Failed {out_path}: {last_err}\")\n",
    "    return False\n",
    "\n",
    "# ---- Export each tile locally ----\n",
    "date_tag = f'{START.format(\"YYYYMM\").getInfo()}_{END.format(\"YYYYMM\").getInfo()}'\n",
    "saved = 0\n",
    "for idx, geom in enumerate(tiles, start=1):\n",
    "    out_tif = os.path.join(OUT_DIR, f\"ndvi_ca_{date_tag}_tile{idx}.tif\")\n",
    "    if os.path.exists(out_tif):\n",
    "        continue\n",
    "    print(f\"→ downloading tile {idx} to {out_tif}\")\n",
    "    ok = download_tile(ndvi.clip(geom), geom, out_tif, scale=250)\n",
    "    if ok: saved += 1\n",
    "\n",
    "print(f\"Done. Saved {saved} NDVI tiles in {OUT_DIR}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac1da58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === GHCND DAILY: raw (long) -> cleaned (wide with lat/lon in bbox) ===\n",
    "# Input  (from your earlier step):  results/ghcnd_daily_raw_all.csv  (long form)\n",
    "# Output (used by superset):        results/ghcnd_daily_cleaned.parquet  (wide per station-day with lat/lon)\n",
    "\n",
    "# Need to do this because we aren't getting proper \"joins\" in our superset setup.\n",
    "\n",
    "import os, io, re, requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "BASE = CONFIG[\"out_dir\"]\n",
    "RAW  = os.path.join(BASE, \"ghcnd_daily_raw_all.csv\")\n",
    "OUT_PARQ = os.path.join(BASE, \"ghcnd_daily_cleaned.parquet\")\n",
    "OUT_CSV  = os.path.join(BASE, \"ghcnd_daily_cleaned.csv\")\n",
    "\n",
    "assert os.path.exists(RAW), f\"Missing raw GHCND file: {RAW}\"\n",
    "\n",
    "# 1) Ensure we have a station catalog with lat/lon\n",
    "#    Prefer a local copy if you already saved one; otherwise download NOAA's reference once.\n",
    "CAT_DIR = os.path.join(BASE, \"manual\"); os.makedirs(CAT_DIR, exist_ok=True)\n",
    "CAT_TXT = os.path.join(CAT_DIR, \"ghcnd-stations.txt\")\n",
    "\n",
    "if not os.path.exists(CAT_TXT):\n",
    "    url = \"https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt\"\n",
    "    r = requests.get(url, timeout=120); r.raise_for_status()\n",
    "    with open(CAT_TXT, \"wb\") as f: f.write(r.content)\n",
    "\n",
    "# Parse ghcnd-stations.txt (fixed-width)\n",
    "# Columns per docs: ID(1-11), LAT(13-20), LON(22-30), ELEV(32-37), STATE(39-40), NAME(42-71) ...\n",
    "def parse_stations(path):\n",
    "    recs = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            if len(line) < 40: \n",
    "                continue\n",
    "            sid = line[0:11].strip()\n",
    "            try:\n",
    "                lat = float(line[12:20].strip())\n",
    "                lon = float(line[21:30].strip())\n",
    "            except ValueError:\n",
    "                continue\n",
    "            state = line[38:40].strip()\n",
    "            name  = line[41:71].strip()\n",
    "            recs.append((sid, lat, lon, state, name))\n",
    "    return pd.DataFrame(recs, columns=[\"station_core\",\"lat\",\"lon\",\"state\",\"name\"])\n",
    "\n",
    "stations = parse_stations(CAT_TXT)\n",
    "\n",
    "# 2) Load your raw long-form CDO file\n",
    "# Expected columns seen in your sample:\n",
    "# ['attributes','datatype','date','mflag','obs_hhmm','qflag','sflag','station','value','value_scaled']\n",
    "raw = pd.read_csv(RAW, low_memory=False)\n",
    "\n",
    "# Normalize station key: raw uses \"GHCND:USW00023232\" → core \"USW00023232\"\n",
    "raw[\"station_core\"] = raw[\"station\"].astype(str).str.replace(\"^GHCND:\", \"\", regex=True)\n",
    "\n",
    "# Pick a numeric value column: prefer value_scaled if present; else scale GHCND native units.\n",
    "# GHCND native: PRCP = tenths of mm, TMAX/TMIN = tenths of °C.\n",
    "have_scaled = \"value_scaled\" in raw.columns\n",
    "def scaled_val(row):\n",
    "    if have_scaled and pd.notna(row[\"value_scaled\"]):\n",
    "        return float(row[\"value_scaled\"])\n",
    "    v = pd.to_numeric(row[\"value\"], errors=\"coerce\")\n",
    "    if pd.isna(v): \n",
    "        return np.nan\n",
    "    if row[\"datatype\"] == \"PRCP\":\n",
    "        return v * 0.1             # → mm\n",
    "    if row[\"datatype\"] in (\"TMAX\",\"TMIN\"):\n",
    "        return v * 0.1             # → °C\n",
    "    return v\n",
    "\n",
    "raw[\"val_clean\"] = raw.apply(scaled_val, axis=1)\n",
    "\n",
    "# Filter to the analysis window if your raw contains more than needed\n",
    "if \"start_date\" in CONFIG and \"end_date\" in CONFIG:\n",
    "    sd = pd.to_datetime(CONFIG[\"start_date\"], utc=True, errors=\"coerce\")\n",
    "    ed = pd.to_datetime(CONFIG[\"end_date\"],   utc=True, errors=\"coerce\")\n",
    "    raw[\"date\"] = pd.to_datetime(raw[\"date\"], utc=True, errors=\"coerce\")\n",
    "    raw = raw[(raw[\"date\"]>=sd) & (raw[\"date\"]<=ed)]\n",
    "else:\n",
    "    raw[\"date\"] = pd.to_datetime(raw[\"date\"], utc=True, errors=\"coerce\")\n",
    "\n",
    "# 3) Keep only the datatypes we need and one value per (station,date,datatype)\n",
    "keep_types = {\"PRCP\":\"precipitation_mm\", \"TMAX\":\"temperature_max_c\", \"TMIN\":\"temperature_min_c\"}\n",
    "raw = raw[raw[\"datatype\"].isin(keep_types.keys())].copy()\n",
    "\n",
    "# If multiple rows per (station,date,datatype), average them\n",
    "agg = (raw.groupby([\"station_core\",\"date\",\"datatype\"], as_index=False)[\"val_clean\"]\n",
    "          .mean())\n",
    "\n",
    "# 4) Pivot to wide columns\n",
    "wide = (agg.pivot(index=[\"station_core\",\"date\"], columns=\"datatype\", values=\"val_clean\")\n",
    "           .reset_index())\n",
    "# Rename columns to our canonical names\n",
    "wide = wide.rename(columns={k:v for k,v in keep_types.items() if k in wide.columns})\n",
    "\n",
    "# 5) Attach lat/lon from station catalog and clip to CA bbox\n",
    "wide = wide.merge(stations[[\"station_core\",\"lat\",\"lon\"]], on=\"station_core\", how=\"left\")\n",
    "\n",
    "# Clip to CONFIG[\"bbox\"] (California in your setup)\n",
    "bbox = CONFIG[\"bbox\"]\n",
    "minx, miny, maxx, maxy = bbox[\"nwlng\"], bbox[\"selat\"], bbox[\"selng\"], bbox[\"nwlat\"]\n",
    "in_box = (wide[\"lon\"].between(minx, maxx)) & (wide[\"lat\"].between(miny, maxy))\n",
    "wide = wide[in_box].copy()\n",
    "\n",
    "# 6) Final tidy columns + sorts\n",
    "cols_order = [\"station_core\",\"date\",\"lat\",\"lon\",\n",
    "              \"precipitation_mm\",\"temperature_max_c\",\"temperature_min_c\"]\n",
    "for c in cols_order:\n",
    "    if c not in wide.columns: wide[c] = np.nan\n",
    "wide = wide[cols_order].sort_values([\"station_core\",\"date\"])\n",
    "\n",
    "# 7) Save for the superset\n",
    "wide.to_parquet(OUT_PARQ, index=False)\n",
    "wide.to_csv(OUT_CSV, index=False)\n",
    "print(f\"Saved cleaned CDO daily → {OUT_PARQ} (rows={len(wide)}, stations={wide['station_core'].nunique()})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7932f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DEBUG: diagnose join gaps by GEO vs TIME ===\n",
    "import os, glob, numpy as np, pandas as pd, geopandas as gpd\n",
    "from datetime import datetime as dt, timezone\n",
    "import warnings; warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "BASE = CONFIG[\"out_dir\"]\n",
    "START = pd.Timestamp(CONFIG[\"start_date\"], tz=\"UTC\")\n",
    "END   = pd.Timestamp(CONFIG[\"end_date\"], tz=\"UTC\")\n",
    "\n",
    "def say(*a): print(*a)\n",
    "\n",
    "# ---- load grid (centroids) ----\n",
    "grid_path = sorted(glob.glob(os.path.join(BASE, \"grid_*m.parquet\")))[-1]\n",
    "grid = gpd.read_parquet(grid_path)\n",
    "if grid.crs is None or (grid.crs.to_epsg() or 4326) != 4326:\n",
    "    grid = grid.set_crs(f\"EPSG:{CONFIG.get('crs_epsg',4326)}\").to_crs(4326)\n",
    "grid[\"centroid\"] = grid.geometry.centroid\n",
    "cent = gpd.GeoDataFrame(grid[[\"geounit_id\"]].copy(), geometry=grid[\"centroid\"], crs=4326)\n",
    "\n",
    "def sjoin_diag(gdf_pts, max_km):\n",
    "    # do a \"loose\" join first (200 km) to see potential matches, then check how many exceed limit\n",
    "    joined200 = gpd.sjoin_nearest(gdf_pts, cent, how=\"left\", distance_col=\"dist_m\")\n",
    "    joined200[\"dist_km\"] = joined200[\"dist_m\"].astype(float) / 1000.0\n",
    "    # what would pass our operational threshold?\n",
    "    keep = joined200[\"dist_km\"] <= max_km\n",
    "    diag = {\n",
    "        \"total_points\": len(joined200),\n",
    "        \"matched_any\": int(joined200[\"geounit_id\"].notna().sum()),\n",
    "        \"within_threshold\": int(keep.sum()),\n",
    "        \"over_threshold\": int((~keep & joined200[\"geounit_id\"].notna()).sum()),\n",
    "        \"no_nearest_found\": int(joined200[\"geounit_id\"].isna().sum()),\n",
    "        \"dist_km_quantiles\": joined200[\"dist_km\"].quantile([0.5, 0.75, 0.9, 0.95, 0.99], interpolation=\"nearest\").round(2).to_dict() if len(joined200) else {}\n",
    "    }\n",
    "    # sample a few that FAILED on distance\n",
    "    bad = joined200.loc[~keep & joined200[\"geounit_id\"].notna(), [\"dist_km\"]].copy()\n",
    "    examples = joined200.loc[~keep & joined200[\"geounit_id\"].notna()].head(5)\n",
    "    return diag, joined200, examples\n",
    "\n",
    "def time_diag(dts, freq=\"D\"):\n",
    "    dts = pd.to_datetime(dts, utc=True, errors=\"coerce\").dropna()\n",
    "    in_win = (dts >= START) & (dts <= END)\n",
    "    return {\n",
    "        \"total_datetimes\": int(len(dts)),\n",
    "        \"within_window\": int(in_win.sum()),\n",
    "        \"outside_window\": int((~in_win).sum()),\n",
    "        \"min_datetime\": str(dts.min()) if len(dts) else None,\n",
    "        \"max_datetime\": str(dts.max()) if len(dts) else None,\n",
    "        \"unique_dates_in_window\": int(pd.DatetimeIndex(dts[in_win]).normalize().nunique()) if len(dts) else 0\n",
    "    }\n",
    "\n",
    "# ---- load sources (use whatever exists) ----\n",
    "paths = {\n",
    "    \"USCRN_hourly\": sorted(glob.glob(os.path.join(BASE, \"uscrn*_hourly*clean*.parquet\"))) \\\n",
    "                 or sorted(glob.glob(os.path.join(BASE, \"uscrn*_hourly*.parquet\"))) \\\n",
    "                 or sorted(glob.glob(os.path.join(BASE, \"uscrn*_hourly*clean*.csv\"))) \\\n",
    "                 or sorted(glob.glob(os.path.join(BASE, \"uscrn*_hourly*.csv\"))),\n",
    "    \"CDO_daily\":    sorted(glob.glob(os.path.join(BASE, \"ghcnd*_daily*clean*.parquet\"))) \\\n",
    "                 or sorted(glob.glob(os.path.join(BASE, \"ghcnd*_daily*.parquet\"))) \\\n",
    "                 or [os.path.join(BASE, \"ghcnd_daily_cleaned.csv\")] if os.path.exists(os.path.join(BASE, \"ghcnd_daily_cleaned.csv\")) else [],\n",
    "    \"AirNow_hourly\":sorted(glob.glob(os.path.join(BASE, \"airnow*_pm25*clean*.parquet\"))) \\\n",
    "                 or sorted(glob.glob(os.path.join(BASE, \"airnow*_pm25*.parquet\"))) \\\n",
    "                 or sorted(glob.glob(os.path.join(BASE, \"airnow*_pm25*clean*.csv\"))) \\\n",
    "                 or sorted(glob.glob(os.path.join(BASE, \"airnow*_pm25*.csv\"))),\n",
    "    \"AQS_daily\":    sorted(glob.glob(os.path.join(BASE, \"aqs_pm25_*_CA.parquet\"))) \\\n",
    "                 or sorted(glob.glob(os.path.join(BASE, \"aqs_pm25*daily*.parquet\"))) \\\n",
    "                 or sorted(glob.glob(os.path.join(BASE, \"aqs_pm25*clean*.parquet\"))),\n",
    "    \"HMS_smoke\":    sorted(glob.glob(os.path.join(BASE, \"hms_clean\",\"smoke\",\"*\",\"*\",\"*.parquet\"))),\n",
    "    \"HMS_fire\":     sorted(glob.glob(os.path.join(BASE, \"hms_clean\",\"fire\",\"*\",\"*\",\"*.parquet\"))),\n",
    "}\n",
    "\n",
    "say(\"\\n=== DEBUG SUMMARY ===\")\n",
    "\n",
    "# 1) USCRN (hourly temps) — GEO & TIME\n",
    "if paths[\"USCRN_hourly\"]:\n",
    "    df = pd.concat([(pd.read_parquet(p) if p.endswith(\".parquet\") else pd.read_csv(p)) for p in paths[\"USCRN_hourly\"]], ignore_index=True)\n",
    "    say(f\"\\nUSCRN rows: {len(df)} | cols: {list(df.columns)[:10]}...\")\n",
    "    dtcol = next((c for c in [\"datetime_utc\",\"UTC_DATETIME\"] if c in df.columns), None)\n",
    "    latcol = next((c for c in [\"LATITUDE\",\"latitude\"] if c in df.columns), None)\n",
    "    loncol = next((c for c in [\"LONGITUDE\",\"longitude\"] if c in df.columns), None)\n",
    "    if dtcol and latcol and loncol:\n",
    "        time_info = time_diag(df[dtcol])\n",
    "        say(\"USCRN TIME:\", time_info)\n",
    "        pts = gpd.GeoDataFrame(df[[latcol, loncol]].dropna(), geometry=gpd.points_from_xy(df[loncol], df[latcol]), crs=4326)\n",
    "        geo_info, joined200, examples = sjoin_diag(pts, max_km=5)\n",
    "        say(\"USCRN GEO:\", geo_info)\n",
    "        if len(examples):\n",
    "            say(\"USCRN sample over-threshold (dist_km):\")\n",
    "            print(examples[[\"dist_km\"]].to_string(index=False))\n",
    "    else:\n",
    "        say(\"USCRN: missing required columns for diag.\")\n",
    "\n",
    "# 2) CDO (daily TMAX/TMIN/PRCP) — GEO & TIME\n",
    "if paths[\"CDO_daily\"]:\n",
    "    df = pd.concat([(pd.read_parquet(p) if p.endswith(\".parquet\") else pd.read_csv(p)) for p in paths[\"CDO_daily\"]], ignore_index=True)\n",
    "    say(f\"\\nCDO rows: {len(df)} | cols: {list(df.columns)[:10]}...\")\n",
    "    datecol = next((c for c in [\"date\",\"DATE\",\"Date\"] if c in df.columns), None)\n",
    "    latcol  = next((c for c in [\"lat\",\"latitude\",\"LAT\",\"Latitude\"] if c in df.columns), None)\n",
    "    loncol  = next((c for c in [\"lon\",\"longitude\",\"LON\",\"Longitude\"] if c in df.columns), None)\n",
    "    if datecol and latcol and loncol:\n",
    "        time_info = time_diag(df[datecol])\n",
    "        say(\"CDO TIME:\", time_info)\n",
    "        pts = gpd.GeoDataFrame(df[[latcol, loncol]].dropna(), geometry=gpd.points_from_xy(df[loncol], df[latcol]), crs=4326)\n",
    "        geo_info, joined200, examples = sjoin_diag(pts, max_km=5)\n",
    "        say(\"CDO GEO:\", geo_info)\n",
    "        if len(examples):\n",
    "            say(\"CDO sample over-threshold (dist_km):\")\n",
    "            print(examples[[\"dist_km\"]].to_string(index=False))\n",
    "    else:\n",
    "        say(\"CDO: missing required columns for diag.\")\n",
    "\n",
    "# 3) AirNow vs AQS (pick whichever you use)\n",
    "if paths[\"AirNow_hourly\"]:\n",
    "    df = pd.concat([(pd.read_parquet(p) if p.endswith(\".parquet\") else pd.read_csv(p)) for p in paths[\"AirNow_hourly\"]], ignore_index=True)\n",
    "    say(f\"\\nAirNow rows: {len(df)} | cols: {list(df.columns)[:10]}...\")\n",
    "    dtcol = next((c for c in [\"datetime_utc\",\"DateTime\",\"date_time_utc\",\"timestamp_utc\"] if c in df.columns), None)\n",
    "    latcol = next((c for c in [\"latitude\",\"Latitude\",\"lat\"] if c in df.columns), None)\n",
    "    loncol = next((c for c in [\"longitude\",\"Longitude\",\"lon\"] if c in df.columns), None)\n",
    "    if dtcol and latcol and loncol:\n",
    "        time_info = time_diag(df[dtcol])\n",
    "        say(\"AirNow TIME:\", time_info)\n",
    "        pts = gpd.GeoDataFrame(df[[latcol, loncol]].dropna(), geometry=gpd.points_from_xy(df[loncol], df[latcol]), crs=4326)\n",
    "        geo_info, joined200, examples = sjoin_diag(pts, max_km=10)\n",
    "        say(\"AirNow GEO:\", geo_info)\n",
    "        if len(examples):\n",
    "            say(\"AirNow sample over-threshold (dist_km):\")\n",
    "            print(examples[[\"dist_km\"]].to_string(index=False))\n",
    "else:\n",
    "    if paths[\"AQS_daily\"]:\n",
    "        df = pd.concat([pd.read_parquet(p) for p in paths[\"AQS_daily\"]], ignore_index=True)\n",
    "        say(f\"\\nAQS rows: {len(df)} | cols: {list(df.columns)[:10]}...\")\n",
    "        datecol = next((c for c in [\"date\",\"DateLocal\"] if c in df.columns), None)\n",
    "        latcol  = next((c for c in [\"latitude\",\"Latitude\",\"lat\"] if c in df.columns), None)\n",
    "        loncol  = next((c for c in [\"longitude\",\"Longitude\",\"lon\"] if c in df.columns), None)\n",
    "        if datecol and latcol and loncol:\n",
    "            time_info = time_diag(df[datecol])\n",
    "            say(\"AQS TIME:\", time_info)\n",
    "            pts = gpd.GeoDataFrame(df[[latcol, loncol]].dropna(), geometry=gpd.points_from_xy(df[loncol], df[latcol]), crs=4326)\n",
    "            geo_info, joined200, examples = sjoin_diag(pts, max_km=10)\n",
    "            say(\"AQS GEO:\", geo_info)\n",
    "            if len(examples):\n",
    "                say(\"AQS sample over-threshold (dist_km):\")\n",
    "                print(examples[[\"dist_km\"]].to_string(index=False))\n",
    "\n",
    "# 4) HMS smoke (daily polygons reduced to centroids)\n",
    "if paths[\"HMS_smoke\"]:\n",
    "    smk = pd.concat([pd.read_parquet(p) for p in paths[\"HMS_smoke\"]], ignore_index=True)\n",
    "    say(f\"\\nHMS smoke rows: {len(smk)} | cols: {list(smk.columns)[:10]}...\")\n",
    "    datecol = \"obs_date\" if \"obs_date\" in smk.columns else (\"start_utc\" if \"start_utc\" in smk.columns else None)\n",
    "    if datecol:\n",
    "        time_info = time_diag(smk[datecol])\n",
    "        say(\"HMS smoke TIME:\", time_info)\n",
    "    if {\"centroid_lon\",\"centroid_lat\"}.issubset(smk.columns):\n",
    "        pts = gpd.GeoDataFrame(smk[[\"centroid_lon\",\"centroid_lat\"]].dropna(),\n",
    "                               geometry=gpd.points_from_xy(smk[\"centroid_lon\"], smk[\"centroid_lat\"]), crs=4326)\n",
    "        geo_info, joined200, examples = sjoin_diag(pts, max_km=25)\n",
    "        say(\"HMS smoke GEO:\", geo_info)\n",
    "        if len(examples):\n",
    "            say(\"HMS smoke sample over-threshold (dist_km):\")\n",
    "            print(examples[[\"dist_km\"]].to_string(index=False))\n",
    "\n",
    "# 5) HMS fire (daily points)\n",
    "if paths[\"HMS_fire\"]:\n",
    "    fir = pd.concat([pd.read_parquet(p) for p in paths[\"HMS_fire\"]], ignore_index=True)\n",
    "    say(f\"\\nHMS fire rows: {len(fir)} | cols: {list(fir.columns)[:10]}...\")\n",
    "    datecol = \"obs_date\" if \"obs_date\" in fir.columns else (\"start_utc\" if \"start_utc\" in fir.columns else None)\n",
    "    if datecol:\n",
    "        time_info = time_diag(fir[datecol])\n",
    "        say(\"HMS fire TIME:\", time_info)\n",
    "    if {\"lon\",\"lat\"}.issubset(fir.columns):\n",
    "        pts = gpd.GeoDataFrame(fir[[\"lon\",\"lat\"]].dropna(), geometry=gpd.points_from_xy(fir[\"lon\"], fir[\"lat\"]), crs=4326)\n",
    "        geo_info, joined200, examples = sjoin_diag(pts, max_km=10)\n",
    "        say(\"HMS fire GEO:\", geo_info)\n",
    "        if len(examples):\n",
    "            say(\"HMS fire sample over-threshold (dist_km):\")\n",
    "            print(examples[[\"dist_km\"]].to_string(index=False))\n",
    "\n",
    "say(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd9ee60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === HeatShield | Final daily dataset builder (grid × day) ===\n",
    "# Exact schemas per your confirmations. HMS Smoke polygons rebuilt from centroid + area_km2.\n",
    "\n",
    "import os, warnings, math\n",
    "import numpy as np, pandas as pd, geopandas as gpd\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "from shapely.geometry import Point\n",
    "from sklearn.neighbors import BallTree\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Paths (your filenames)\n",
    "RES_DIR   = \"results\"\n",
    "GRID_PATH = os.path.join(RES_DIR, \"grid_3000m_CA_epsg3310.parquet\")\n",
    "\n",
    "AIRNOW_PATH = os.path.join(RES_DIR, \"airnow_pm25_clean.parquet\")           # latitude, longitude, datetime_utc, parameter, value_ugm3, sensor_id (PM2.5 only)\n",
    "AQS_PATH    = os.path.join(RES_DIR, \"aqs_pm25_clean.parquet\")               # station_id, latitude, longitude, datetime_utc, pm25_ugm3\n",
    "GHCND_PATH  = os.path.join(RES_DIR, \"ghcnd_daily_cleaned.parquet\")          # station_core, date (local), lat, lon, precipitation_mm, temperature_max_c, temperature_min_c\n",
    "USCRN_PATH  = os.path.join(RES_DIR, \"uscrn_2024_hourly_clean.parquet\")      # datetime_utc, latitude, longitude, air_temperature_hourly_*_c, precipitation_mm_hourly, ...\n",
    "FIRE_PATH   = os.path.join(RES_DIR, \"hms_fire_2024-06-01_to_2024-10-31.parquet\")   # lat, Lon, obs_date (WGS84)\n",
    "SMOKE_PATH  = os.path.join(RES_DIR, \"hms_smoke_2024-06-01_to_2024-10-31.parquet\")  # NOT GeoParquet; has centroid_lat/lon, area_km2, density, obs_date\n",
    "\n",
    "DATE_FROM = \"2024-06-01\"\n",
    "DATE_TO   = \"2024-10-31\"\n",
    "\n",
    "OUT_DIR = CONFIG[\"out_dir\"]; os.makedirs(OUT_DIR, exist_ok=True)\n",
    "FINAL_PARQUET = os.path.join(OUT_DIR, f\"final_daily_grid_3000m_20240601_20241031.parquet\")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Config for interpolation (EPSG:3310 meters)\n",
    "OPS_EPSG = 3310\n",
    "RES_M = 3000\n",
    "URBAN_CELL_AREA_KM2 = 5.0  # heuristic\n",
    "CFG = {\n",
    "    \"PM25\": {\"k_urban\": 3, \"k_nonurban\": 5,\n",
    "             \"tiers_km_urban\": [15, 25, 40], \"tiers_km_nonurb\": [30, 50, 80],\n",
    "             \"hard_cap_km\": 80, \"integrity_cap_km\": 50},\n",
    "    \"TEMP\": {\"k_urban\": 4, \"k_nonurban\": 6,\n",
    "             \"tiers_km_urban\": [20, 40, 70], \"tiers_km_nonurb\": [30, 60, 100],\n",
    "             \"hard_cap_km\": 100, \"integrity_cap_km\": 70},\n",
    "    \"PRCP\": {\"k_urban\": 4, \"k_nonurban\": 6,\n",
    "             \"tiers_km_urban\": [15, 30, 50], \"tiers_km_nonurb\": [25, 50, 80],\n",
    "             \"hard_cap_km\": 80, \"integrity_cap_km\": 50},\n",
    "}\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Utilities\n",
    "\n",
    "def _drop_idcols(df):\n",
    "    return df.drop(columns=[c for c in (\"grid_id\",\"date\") if c in df.columns], errors=\"ignore\")\n",
    "\n",
    "def _std_met_cols(df):\n",
    "    # rename *_n_used → *_n to match diagnostics\n",
    "    ren = {c: c.replace(\"_n_used\", \"_n\") for c in df.columns if c.endswith(\"_n_used\")}\n",
    "    return df.rename(columns=ren)\n",
    "\n",
    "def _read_parquet_df(path, kind):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"{kind} file not found: {path}\")\n",
    "    return pd.read_parquet(path)\n",
    "\n",
    "def _ensure_points_3310_from_latlon(df, lat_col, lon_col, kind):\n",
    "    d = df.dropna(subset=[lat_col, lon_col]).copy()\n",
    "    g = gpd.GeoDataFrame(d, geometry=gpd.points_from_xy(d[lon_col], d[lat_col]), crs=4326)\n",
    "    return g.to_crs(OPS_EPSG)\n",
    "\n",
    "def _balltree_xy(gpoints):\n",
    "    xy = np.column_stack([gpoints.x.values, gpoints.y.values])\n",
    "    return BallTree(xy, leaf_size=40, metric=\"euclidean\"), xy\n",
    "\n",
    "def _idw(values, dists_m, power=1.5):\n",
    "    d_km = np.maximum(dists_m/1000.0, 0.5)  # floor avoids singularity\n",
    "    w = 1.0 / (d_km ** power)\n",
    "    return np.sum(w * values) / np.sum(w)\n",
    "\n",
    "def _urban_mask(grid):\n",
    "    area_km2 = (RES_M/1000.0) ** 2\n",
    "    return np.full(len(grid), area_km2 < URBAN_CELL_AREA_KM2, dtype=bool)\n",
    "\n",
    "def _choose_tiers(var_key, urban):\n",
    "    cfg = CFG[var_key]\n",
    "    return (cfg[\"tiers_km_urban\"] if urban else cfg[\"tiers_km_nonurb\"],\n",
    "            cfg[\"k_urban\"] if urban else cfg[\"k_nonurban\"],\n",
    "            cfg[\"hard_cap_km\"], cfg[\"integrity_cap_km\"])\n",
    "\n",
    "def _interp_points_one_var_day(grid_centroids, stations_gdf_day, values_col, var_key, source_col=None, aqs_bonus=1.0):\n",
    "    n_cells = len(grid_centroids)\n",
    "    if stations_gdf_day is None or len(stations_gdf_day) == 0:\n",
    "        return pd.DataFrame({\n",
    "            \"value\": [np.nan]*n_cells, \"n_used\": 0, \"maxdist_km\": np.nan,\n",
    "            \"method\": None, \"radius_tier\": None, \"integrity_warn\": 0\n",
    "        })\n",
    "    tree, _ = _balltree_xy(stations_gdf_day.geometry)\n",
    "    vals = stations_gdf_day[values_col].to_numpy()\n",
    "    src = stations_gdf_day[source_col].astype(str).to_numpy() if source_col and source_col in stations_gdf_day.columns else None\n",
    "    qxy = np.column_stack([grid_centroids.x.values, grid_centroids.y.values])\n",
    "    urban_mask = _urban_mask(grid_centroids.to_frame(name=\"geometry\"))\n",
    "\n",
    "    val_out = np.full(n_cells, np.nan, dtype=float)\n",
    "    n_out = np.zeros(n_cells, dtype=np.int16)\n",
    "    maxd_out = np.full(n_cells, np.nan, dtype=float)\n",
    "    meth_out = np.full(n_cells, None, dtype=object)\n",
    "    tier_out = np.full(n_cells, None, dtype=object)\n",
    "    warn_out = np.zeros(n_cells, dtype=np.int8)\n",
    "\n",
    "    for idx in range(n_cells):\n",
    "        tiers_km, k, hard_cap, integ_cap = _choose_tiers(var_key, urban_mask[idx])\n",
    "        found = False\n",
    "        for t_i, r_km in enumerate(tiers_km, start=1):\n",
    "            ind = tree.query_radius(qxy[idx:idx+1, :], r=r_km*1000.0, return_distance=True)\n",
    "            ids = ind[0][0]; dists = ind[1][0]\n",
    "            if ids.size == 0:\n",
    "                continue\n",
    "            order = np.argsort(dists)[:k]\n",
    "            ids, d = ids[order], dists[order]\n",
    "            v = vals[ids]\n",
    "\n",
    "            if var_key == \"PM25\" and src is not None:\n",
    "                sub_src = src[ids]\n",
    "                if t_i == 1 and np.any(sub_src == \"AQS\"):\n",
    "                    keep = (sub_src == \"AQS\")\n",
    "                    v = v[keep]; d = d[keep]\n",
    "                else:\n",
    "                    w = 1.0 / np.maximum(d/1000.0, 0.5) ** 1.5\n",
    "                    w[sub_src == \"AQS\"] *= aqs_bonus\n",
    "                    val_out[idx] = float(np.sum(w * v) / np.sum(w))\n",
    "                    n_out[idx] = int(len(v))\n",
    "                    maxd_out[idx] = float(d.max()/1000.0)\n",
    "                    meth_out[idx] = \"IDW\"; tier_out[idx] = t_i\n",
    "                    warn_out[idx] = int((d.max()/1000.0) > integ_cap)\n",
    "                    found = True\n",
    "                    break\n",
    "\n",
    "            if len(v) == 1:\n",
    "                val_out[idx] = float(v[0]); n_out[idx] = 1\n",
    "                maxd_out[idx] = float(d[0]/1000.0); meth_out[idx] = \"NN\"; tier_out[idx] = t_i\n",
    "                warn_out[idx] = int((d[0]/1000.0) > integ_cap)\n",
    "            else:\n",
    "                val_out[idx] = float(_idw(v, d, power=1.5)); n_out[idx] = int(len(v))\n",
    "                maxd_out[idx] = float(d.max()/1000.0); meth_out[idx] = \"IDW\"; tier_out[idx] = t_i\n",
    "                warn_out[idx] = int((d.max()/1000.0) > integ_cap)\n",
    "            found = True\n",
    "            break\n",
    "\n",
    "        if not found:\n",
    "            pass\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"value\": val_out, \"n_used\": n_out, \"maxdist_km\": maxd_out,\n",
    "        \"method\": meth_out, \"radius_tier\": tier_out, \"integrity_warn\": warn_out\n",
    "    })\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Load grid\n",
    "grid_3310 = gpd.read_parquet(GRID_PATH)\n",
    "if grid_3310.crs is None or grid_3310.crs.to_epsg() != 3310:\n",
    "    raise ValueError(\"Grid must be EPSG:3310 with a 'geometry' column.\")\n",
    "grid_centroids = grid_3310.geometry.centroid  # planar centroids in 3310\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Load & normalize sources\n",
    "\n",
    "# AirNow PM2.5 daily\n",
    "_airnow = _read_parquet_df(AIRNOW_PATH, \"AirNow\")\n",
    "_airnow = _airnow.rename(columns={\"latitude\":\"lat\", \"longitude\":\"lon\", \"value_ugm3\":\"value\"})\n",
    "_airnow[\"date\"] = pd.to_datetime(_airnow[\"datetime_utc\"], utc=True, errors=\"coerce\").dt.date.astype(str)\n",
    "_airnow[\"pollutant\"] = \"PM25\"; _airnow[\"source\"] = \"AirNow\"\n",
    "g_airnow = _ensure_points_3310_from_latlon(_airnow, \"lat\", \"lon\", \"AirNow\")\n",
    "\n",
    "# AQS PM2.5 daily\n",
    "_aqs = _read_parquet_df(AQS_PATH, \"AQS\")\n",
    "_aqs = _aqs.rename(columns={\"latitude\":\"lat\", \"longitude\":\"lon\", \"pm25_ugm3\":\"value\"})\n",
    "_aqs[\"date\"] = pd.to_datetime(_aqs[\"datetime_utc\"], utc=True, errors=\"coerce\").dt.date.astype(str)\n",
    "_aqs[\"pollutant\"] = \"PM25\"; _aqs[\"source\"] = \"AQS\"\n",
    "g_aqs = _ensure_points_3310_from_latlon(_aqs, \"lat\", \"lon\", \"AQS\")\n",
    "\n",
    "# GHCND daily (local date)\n",
    "_gh = _read_parquet_df(GHCND_PATH, \"GHCND\").rename(columns={\n",
    "    \"precipitation_mm\":\"prcp_mm\",\n",
    "    \"temperature_max_c\":\"tmax_c\",\n",
    "    \"temperature_min_c\":\"tmin_c\"\n",
    "})\n",
    "if \"tavg_c\" not in _gh.columns and {\"tmax_c\",\"tmin_c\"}.issubset(_gh.columns):\n",
    "    _gh[\"tavg_c\"] = (_gh[\"tmax_c\"] + _gh[\"tmin_c\"]) / 2.0\n",
    "_gh[\"date\"] = pd.to_datetime(_gh[\"date\"], errors=\"coerce\").dt.date.astype(str)\n",
    "g_gh = _ensure_points_3310_from_latlon(_gh, \"lat\", \"lon\", \"GHCND\")\n",
    "\n",
    "# USCRN hourly → daily aggregation (UTC timestamps)\n",
    "_us = _read_parquet_df(USCRN_PATH, \"USCRN\").rename(columns={\"latitude\":\"lat\", \"longitude\":\"lon\"})\n",
    "_us[\"date\"] = pd.to_datetime(_us[\"datetime_utc\"], utc=True, errors=\"coerce\").dt.date.astype(str)\n",
    "grp_keys = [\"lat\",\"lon\",\"date\"]\n",
    "agg_map = {}\n",
    "if \"air_temperature_hourly_avg_c\" in _us.columns: agg_map[\"air_temperature_hourly_avg_c\"] = \"mean\"\n",
    "if \"air_temperature_hourly_max_c\" in _us.columns: agg_map[\"air_temperature_hourly_max_c\"] = \"max\"\n",
    "if \"air_temperature_hourly_min_c\" in _us.columns: agg_map[\"air_temperature_hourly_min_c\"] = \"min\"\n",
    "if \"precipitation_mm_hourly\" in _us.columns:      agg_map[\"precipitation_mm_hourly\"]      = \"sum\"\n",
    "_us_daily = _us.groupby(grp_keys, as_index=False).agg(agg_map).rename(columns={\n",
    "    \"air_temperature_hourly_avg_c\":\"tavg_c\",\n",
    "    \"air_temperature_hourly_max_c\":\"tmax_c\",\n",
    "    \"air_temperature_hourly_min_c\":\"tmin_c\",\n",
    "    \"precipitation_mm_hourly\":\"prcp_mm\"\n",
    "})\n",
    "g_us = _ensure_points_3310_from_latlon(_us_daily, \"lat\", \"lon\", \"USCRN\")\n",
    "\n",
    "# HMS Fire points: lat, Lon → lon; obs_date\n",
    "_fire = _read_parquet_df(FIRE_PATH, \"HMS Fire\")\n",
    "lon_col = \"Lon\" if \"Lon\" in _fire.columns else (\"lon\" if \"lon\" in _fire.columns else None)\n",
    "if lon_col is None: raise ValueError(\"HMS Fire: expected 'Lon' or 'lon' column.\")\n",
    "_fire = _fire.rename(columns={\"lat\":\"lat\", lon_col:\"lon\"})\n",
    "_fire[\"date\"] = pd.to_datetime(_fire[\"obs_date\"], errors=\"coerce\").dt.date.astype(str)\n",
    "g_fire = _ensure_points_3310_from_latlon(_fire, \"lat\", \"lon\", \"HMS Fire\")\n",
    "\n",
    "# HMS Smoke (NOT GeoParquet): centroid_lat/lon + area_km2 + density + obs_date\n",
    "_smoke_df = _read_parquet_df(SMOKE_PATH, \"HMS Smoke (attributes)\")\n",
    "# Clean/guard\n",
    "need_cols = {\"centroid_lat\",\"centroid_lon\",\"area_km2\",\"density\",\"obs_date\"}\n",
    "missing = [c for c in need_cols if c not in _smoke_df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"HMS Smoke parquet is missing required columns: {missing}\")\n",
    "_smoke_df = _smoke_df.dropna(subset=[\"centroid_lat\",\"centroid_lon\",\"area_km2\"]).copy()\n",
    "_smoke_df[\"date\"] = pd.to_datetime(_smoke_df[\"obs_date\"], errors=\"coerce\").dt.date.astype(str)\n",
    "# density → class\n",
    "dens_map = {\"Light\":1, \"Medium\":2, \"Heavy\":3, \"LIGHT\":1, \"MEDIUM\":2, \"HEAVY\":3}\n",
    "_smoke_df[\"smoke_class\"] = _smoke_df[\"density\"].astype(str).map(dens_map).fillna(0).astype(int)\n",
    "\n",
    "# Limit all sources to requested window\n",
    "def _clip_dates(df, col=\"date\"):\n",
    "    m = (df[col] >= DATE_FROM) & (df[col] <= DATE_TO)\n",
    "    return df.loc[m].copy()\n",
    "\n",
    "g_airnow = _clip_dates(g_airnow)\n",
    "g_aqs    = _clip_dates(g_aqs)\n",
    "g_gh     = _clip_dates(g_gh)\n",
    "g_us     = _clip_dates(g_us)\n",
    "g_fire   = _clip_dates(g_fire)\n",
    "_smoke_df = _clip_dates(_smoke_df)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Per-day builders\n",
    "\n",
    "def build_pm25_for_day(dstr):\n",
    "    frames = []\n",
    "    a = g_airnow[g_airnow[\"date\"] == dstr][[\"value\",\"geometry\"]].copy()\n",
    "    if len(a): a[\"source\"]=\"AirNow\"; frames.append(a)\n",
    "    q = g_aqs[g_aqs[\"date\"] == dstr][[\"value\",\"geometry\"]].copy()\n",
    "    if len(q): q[\"source\"]=\"AQS\"; frames.append(q)\n",
    "    if not frames:\n",
    "        return None\n",
    "\n",
    "    day = pd.concat(frames, ignore_index=True)\n",
    "    out = {}\n",
    "\n",
    "    if len(a):\n",
    "        res_a = _interp_points_one_var_day(grid_centroids, a, \"value\", \"PM25\")\n",
    "        out[\"airnow\"] = res_a.rename(columns={\n",
    "            \"value\":\"pm25_airnow\",\"n_used\":\"pm25_airnow_n\",\"maxdist_km\":\"pm25_airnow_maxdist_km\",\n",
    "            \"method\":\"pm25_airnow_method\",\"radius_tier\":\"pm25_airnow_radius_tier\",\"integrity_warn\":\"pm25_airnow_integrity_warn\"\n",
    "        })\n",
    "    if len(q):\n",
    "        res_q = _interp_points_one_var_day(grid_centroids, q, \"value\", \"PM25\")\n",
    "        out[\"aqs\"] = res_q.rename(columns={\n",
    "            \"value\":\"pm25_aqs\",\"n_used\":\"pm25_aqs_n\",\"maxdist_km\":\"pm25_aqs_maxdist_km\",\n",
    "            \"method\":\"pm25_aqs_method\",\"radius_tier\":\"pm25_aqs_radius_tier\",\"integrity_warn\":\"pm25_aqs_integrity_warn\"\n",
    "        })\n",
    "    res_b = _interp_points_one_var_day(grid_centroids, day, \"value\", \"PM25\", source_col=\"source\", aqs_bonus=1.5)\n",
    "    out[\"blend\"] = res_b.rename(columns={\n",
    "        \"value\":\"pm25\",\"n_used\":\"pm25_n\",\"maxdist_km\":\"pm25_maxdist_km\",\n",
    "        \"method\":\"pm25_method\",\"radius_tier\":\"pm25_radius_tier\",\"integrity_warn\":\"pm25_integrity_warn\"\n",
    "    })\n",
    "\n",
    "    base = pd.DataFrame({\"grid_id\": grid_3310[\"grid_id\"].values})\n",
    "    for k in [\"airnow\",\"aqs\",\"blend\"]:\n",
    "        if k in out:\n",
    "            base = pd.concat([base, out[k].reset_index(drop=True)], axis=1)\n",
    "        else:\n",
    "            # fill if missing\n",
    "            if k == \"airnow\":\n",
    "                for c in [\"pm25_airnow\",\"pm25_airnow_n\",\"pm25_airnow_maxdist_km\",\"pm25_airnow_method\",\"pm25_airnow_radius_tier\",\"pm25_airnow_integrity_warn\"]:\n",
    "                    base[c] = np.nan if (\"_n\" not in c and \"_warn\" not in c) else 0\n",
    "            if k == \"aqs\":\n",
    "                for c in [\"pm25_aqs\",\"pm25_aqs_n\",\"pm25_aqs_maxdist_km\",\"pm25_aqs_method\",\"pm25_aqs_radius_tier\",\"pm25_aqs_integrity_warn\"]:\n",
    "                    base[c] = np.nan if (\"_n\" not in c and \"_warn\" not in c) else 0\n",
    "    return base\n",
    "\n",
    "def build_met_for_day(dstr):\n",
    "    g = pd.concat([\n",
    "        g_gh[g_gh[\"date\"] == dstr][[\"tmax_c\",\"tmin_c\",\"tavg_c\",\"prcp_mm\",\"geometry\"]],\n",
    "        g_us[g_us[\"date\"] == dstr][[\"tmax_c\",\"tmin_c\",\"tavg_c\",\"prcp_mm\",\"geometry\"]]\n",
    "    ], ignore_index=True)\n",
    "    if len(g) == 0:\n",
    "        # create a skeleton with expected columns\n",
    "        base = pd.DataFrame(index=range(len(grid_3310)))\n",
    "        for col in [\"tmax_c\",\"tmin_c\",\"tavg_c\"]:\n",
    "            base[col+\"_value\"]=np.nan; base[col+\"_n\"]=0; base[col+\"_maxdist_km\"]=np.nan\n",
    "            base[col+\"_method\"]=None;  base[col+\"_radius_tier\"]=None; base[col+\"_integrity_warn\"]=0\n",
    "        base[\"prcp_mm\"]=np.nan; base[\"prcp_n\"]=0; base[\"prcp_maxdist_km\"]=np.nan\n",
    "        base[\"prcp_method\"]=None; base[\"prcp_radius_tier\"]=None; base[\"prcp_integrity_warn\"]=0\n",
    "        return base\n",
    "\n",
    "    out = {}\n",
    "    for col in [\"tmax_c\",\"tmin_c\",\"tavg_c\"]:\n",
    "        sub = g[[\"geometry\", col]].dropna(subset=[col]).copy()\n",
    "        res = _interp_points_one_var_day(grid_centroids, sub.rename(columns={col:\"value\"}), \"value\", \"TEMP\")\n",
    "        out[col] = res.add_prefix(col+\"_\")\n",
    "\n",
    "    if g[\"prcp_mm\"].notna().any():\n",
    "        subp = g[[\"geometry\",\"prcp_mm\"]].dropna(subset=[\"prcp_mm\"]).copy()\n",
    "        subp[\"value\"] = np.sqrt(np.maximum(subp[\"prcp_mm\"].to_numpy(), 0.0))  # sqrt trick\n",
    "        res = _interp_points_one_var_day(grid_centroids, subp, \"value\", \"PRCP\")\n",
    "        res[\"value\"] = np.square(res[\"value\"])\n",
    "        out[\"prcp\"] = res.rename(columns={\n",
    "            \"value\":\"prcp_mm\", \"n_used\":\"prcp_n\", \"maxdist_km\":\"prcp_maxdist_km\",\n",
    "            \"method\":\"prcp_method\", \"radius_tier\":\"prcp_radius_tier\", \"integrity_warn\":\"prcp_integrity_warn\"\n",
    "        })\n",
    "\n",
    "    base = pd.DataFrame()\n",
    "    for k,v in out.items():\n",
    "        base = pd.concat([base, v.reset_index(drop=True)], axis=1) if len(base) else v.reset_index(drop=True)\n",
    "    # Ensure missing branches exist\n",
    "    for col in [\"tmax_c\",\"tmin_c\",\"tavg_c\"]:\n",
    "        for suf in [\"_value\",\"_n\",\"_maxdist_km\",\"_method\",\"_radius_tier\",\"_integrity_warn\"]:\n",
    "            if (col+suf) not in base.columns:\n",
    "                base[col+suf] = np.nan if suf in [\"_value\",\"_maxdist_km\"] else (0 if suf in [\"_n\",\"_integrity_warn\"] else None)\n",
    "    if \"prcp_mm\" not in base.columns:\n",
    "        base[\"prcp_mm\"]=np.nan; base[\"prcp_n\"]=0; base[\"prcp_maxdist_km\"]=np.nan\n",
    "        base[\"prcp_method\"]=None; base[\"prcp_radius_tier\"]=None; base[\"prcp_integrity_warn\"]=0\n",
    "    return base\n",
    "\n",
    "def build_fire_for_day(dstr):\n",
    "    day = g_fire[g_fire[\"date\"]==dstr]\n",
    "    if len(day)==0:\n",
    "        return pd.DataFrame({\"fire_count_in\":0, \"fire_buffer_count\":0, \"nearest_fire_km\":np.nan}, index=range(len(grid_3310)))\n",
    "    grid_only = gpd.GeoDataFrame(geometry=grid_3310.geometry, crs=OPS_EPSG)\n",
    "    join_in = gpd.sjoin(grid_only, day[[\"geometry\"]], how=\"left\", predicate=\"contains\")\n",
    "    counts = join_in.groupby(join_in.index).size().reindex(range(len(grid_3310)), fill_value=0)\n",
    "    grid_buf = gpd.GeoDataFrame(geometry=grid_3310.geometry.buffer(2000), crs=OPS_EPSG)\n",
    "    join_buf = gpd.sjoin(grid_buf, day[[\"geometry\"]], how=\"left\", predicate=\"contains\")\n",
    "    counts_buf = join_buf.groupby(join_buf.index).size().reindex(range(len(grid_3310)), fill_value=0) - counts\n",
    "    counts_buf = counts_buf.clip(lower=0)\n",
    "    tree, _ = _balltree_xy(day.geometry)\n",
    "    qxy = np.column_stack([grid_centroids.x.values, grid_centroids.y.values])\n",
    "    d, _ = tree.query(qxy, k=1)\n",
    "    d_km = (d[:,0]/1000.0); d_km[d_km>20] = np.nan\n",
    "    return pd.DataFrame({\n",
    "        \"fire_count_in\": counts.values.astype(np.int16),\n",
    "        \"fire_buffer_count\": counts_buf.values.astype(np.int16),\n",
    "        \"nearest_fire_km\": d_km.astype(float)\n",
    "    })\n",
    "\n",
    "def build_smoke_for_day(dstr):\n",
    "    \"\"\"\n",
    "    Rebuild smoke polygons as circles from centroid + area.\n",
    "    smoke_nearby = 1 if circle intersects a 5 km buffer of cell (even when direct overlap is 0).\n",
    "    \"\"\"\n",
    "    s = _smoke_df[_smoke_df[\"date\"] == dstr].copy()\n",
    "    if len(s) == 0:\n",
    "        return pd.DataFrame({\"smoke_class\":0, \"smoke_frac\":0.0, \"smoke_nearby\":0}, index=range(len(grid_3310)))\n",
    "\n",
    "    # Build circles in 3310: radius_m = sqrt(area_km2 / pi) * 1000\n",
    "    s[\"radius_m\"] = np.sqrt(np.maximum(s[\"area_km2\"].astype(float), 0.0) / math.pi) * 1000.0\n",
    "    # Discard zero/NaN radius\n",
    "    s = s.replace([np.inf, -np.inf], np.nan).dropna(subset=[\"centroid_lat\",\"centroid_lon\",\"radius_m\"])\n",
    "    if len(s) == 0:\n",
    "        return pd.DataFrame({\"smoke_class\":0, \"smoke_frac\":0.0, \"smoke_nearby\":0}, index=range(len(grid_3310)))\n",
    "\n",
    "    pts = gpd.GeoDataFrame(s, geometry=gpd.points_from_xy(s[\"centroid_lon\"], s[\"centroid_lat\"]), crs=4326).to_crs(OPS_EPSG)\n",
    "    # Create circle polygons\n",
    "    circles = pts.copy()\n",
    "    circles[\"geometry\"] = circles.geometry.buffer(circles[\"radius_m\"])\n",
    "    circles = circles[[\"smoke_class\",\"geometry\"]].copy()\n",
    "\n",
    "    # Intersections with grid\n",
    "    grid_gdf = gpd.GeoDataFrame({\"grid_id\": grid_3310[\"grid_id\"].values}, geometry=grid_3310.geometry, crs=OPS_EPSG)\n",
    "    inter = gpd.overlay(grid_gdf, circles, how=\"intersection\")\n",
    "    if len(inter) == 0:\n",
    "        # no direct overlap → only nearby via 5km buffer\n",
    "        near = gpd.sjoin(\n",
    "            gpd.GeoDataFrame(geometry=grid_3310.geometry.buffer(5000), crs=OPS_EPSG),\n",
    "            circles[[\"geometry\"]], how=\"left\", predicate=\"intersects\"\n",
    "        ).groupby(level=0).size().reindex(range(len(grid_3310)), fill_value=0).values\n",
    "        return pd.DataFrame({\"smoke_class\":0, \"smoke_frac\":0.0, \"smoke_nearby\": (near>0).astype(int)})\n",
    "\n",
    "    inter[\"a_int\"] = inter.geometry.area\n",
    "    cell_area = float(RES_M * RES_M)\n",
    "\n",
    "    # Area-weighted class (per grid cell): mean(smoke_class * a_int) / mean(a_int)\n",
    "    agg = inter.groupby(\"grid_id\").apply(\n",
    "        lambda df: pd.Series({\n",
    "            \"smoke_frac\": float(df[\"a_int\"].sum()/cell_area),\n",
    "            \"smoke_class_aw\": float((df[\"smoke_class\"]*df[\"a_int\"]).sum() / df[\"a_int\"].sum())\n",
    "        })\n",
    "    ).reset_index()\n",
    "    agg[\"smoke_class\"] = agg[\"smoke_class_aw\"].round().clip(0,3).astype(int)\n",
    "\n",
    "    # Nearby (5 km buffer intersects any circle)\n",
    "    near = gpd.sjoin(\n",
    "        gpd.GeoDataFrame(geometry=grid_3310.geometry.buffer(5000), crs=OPS_EPSG),\n",
    "        circles[[\"geometry\"]], how=\"left\", predicate=\"intersects\"\n",
    "    ).groupby(level=0).size().reindex(range(len(grid_3310)), fill_value=0).values\n",
    "\n",
    "    # Assemble output aligned to grid index\n",
    "    idx_by_gridid = grid_3310.set_index(\"grid_id\").index\n",
    "    agg_idx = agg.set_index(\"grid_id\").reindex(idx_by_gridid)\n",
    "    out = pd.DataFrame({\n",
    "        \"smoke_class\": agg_idx[\"smoke_class\"].fillna(0).astype(int).to_numpy(),\n",
    "        \"smoke_frac\":  agg_idx[\"smoke_frac\"].fillna(0.0).astype(float).to_numpy(),\n",
    "        \"smoke_nearby\": (near>0).astype(int)\n",
    "    }, index=range(len(grid_3310)))\n",
    "\n",
    "    # Clip smoke_frac to [0,1]\n",
    "    out[\"smoke_frac\"] = out[\"smoke_frac\"].clip(0.0, 1.0)\n",
    "    return out\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Main loop across days\n",
    "all_days = pd.date_range(DATE_FROM, DATE_TO, freq=\"D\").date.astype(str).tolist()\n",
    "pieces = []\n",
    "\n",
    "for di, dstr in enumerate(all_days, 1):\n",
    "    base = pd.DataFrame({\"grid_id\": grid_3310[\"grid_id\"].values})\n",
    "\n",
    "    pm = build_pm25_for_day(dstr)\n",
    "    if pm is None:\n",
    "        # ... (your existing NA fills)\n",
    "        pass\n",
    "    else:\n",
    "        pm = _drop_idcols(pm)\n",
    "        base = pd.concat([base, pm.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    met = build_met_for_day(dstr)\n",
    "    met = _drop_idcols(met)\n",
    "    met = _std_met_cols(met)\n",
    "    base = pd.concat([base, met.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    fire = build_fire_for_day(dstr)\n",
    "    fire = _drop_idcols(fire)\n",
    "    base = pd.concat([base, fire.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    smoke = build_smoke_for_day(dstr)\n",
    "    smoke = _drop_idcols(smoke)\n",
    "    base = pd.concat([base, smoke.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    base.insert(1, \"date\", dstr)\n",
    "    pieces.append(base)\n",
    "\n",
    "    \n",
    "    if di % 10 == 0:\n",
    "        print(f\"[{datetime.now(timezone.utc).isoformat()}] processed {di}/{len(all_days)} days…\")\n",
    "\n",
    "final_df = pd.concat(pieces, ignore_index=True)\n",
    "\n",
    "# Save\n",
    "final_df.to_parquet(FINAL_PARQUET, index=False)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Diagnostics\n",
    "print(\"\\n=== Final dataset saved ===\")\n",
    "print(\"Path:\", FINAL_PARQUET)\n",
    "print(\"Rows:\", len(final_df), \"| Days:\", final_df['date'].nunique(), \"| Cells:\", grid_3310.shape[0])\n",
    "for col in [\"pm25\",\"pm25_airnow\",\"pm25_aqs\",\"tavg_c_value\",\"tmax_c_value\",\"tmin_c_value\",\"prcp_mm\",\n",
    "            \"smoke_frac\",\"smoke_class\",\"fire_count_in\",\"nearest_fire_km\"]:\n",
    "    if col in final_df.columns:\n",
    "        nn = final_df[col].notna().sum()\n",
    "        print(f\"{col:>18}: non-null {nn}  ({nn/len(final_df):.1%})\")\n",
    "\n",
    "def _coverage(col):\n",
    "    if col in final_df.columns and final_df[col].notna().any():\n",
    "        bins = pd.cut(final_df[col], bins=[0,10,25,50,75,100,150], include_lowest=True)\n",
    "        print(f\"{col} bins:\\n\", bins.value_counts().sort_index())\n",
    "\n",
    "_coverage(\"pm25_maxdist_km\")\n",
    "_coverage(\"tavg_c_maxdist_km\")\n",
    "_coverage(\"prcp_maxdist_km\")\n",
    "\n",
    "# Quick preview\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811b4178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === HeatShield | Preprocess CA tracts (TIGER 2024) + ACS CSV → 3310 polygons & centroids (safe) ===\n",
    "# Inputs expected in \".\" or \"./results/\":\n",
    "#   - tl_2024_06_tract.zip\n",
    "#   - acs_2023_california_tracts.csv  (has: tract, county, pct_age65plus, pct_poverty, and possibly median_income)\n",
    "# Outputs (GeoParquet, EPSG:3310):\n",
    "#   - results/ca_tracts_2024_poly_3310.parquet\n",
    "#   - results/ca_tracts_2024_centroids_3310.parquet\n",
    "#   - results/acs_2023_ca_centroids_merged_3310.parquet\n",
    "\n",
    "import os, re, zipfile, tempfile\n",
    "import pandas as pd, geopandas as gpd\n",
    "\n",
    "# ---------------- Paths ----------------\n",
    "RES_DIR = \"results\"\n",
    "os.makedirs(RES_DIR, exist_ok=True)\n",
    "\n",
    "def _find(fn):\n",
    "    for c in [fn, os.path.join(RES_DIR, fn)]:\n",
    "        if os.path.exists(c):\n",
    "            return c\n",
    "    raise FileNotFoundError(f\"File not found: {fn} (looked in '.' and 'results/')\")\n",
    "\n",
    "TRACTS_ZIP = _find(\"tl_2024_06_tract.zip\")\n",
    "ACS_CSV    = _find(\"acs_2023_california_tracts.csv\")\n",
    "\n",
    "OUT_POLY      = os.path.join(RES_DIR, \"ca_tracts_2024_poly_3310.parquet\")\n",
    "OUT_CENTROIDS = os.path.join(RES_DIR, \"ca_tracts_2024_centroids_3310.parquet\")\n",
    "OUT_ACS_CENTS = os.path.join(RES_DIR, \"acs_2023_ca_centroids_merged_3310.parquet\")\n",
    "\n",
    "# ------------- County lookup (CA) -------------\n",
    "CA_COUNTY_TO_FIPS = {\n",
    "    \"ALAMEDA\":\"001\",\"ALPINE\":\"003\",\"AMADOR\":\"005\",\"BUTTE\":\"007\",\"CALAVERAS\":\"009\",\"COLUSA\":\"011\",\n",
    "    \"CONTRA COSTA\":\"013\",\"DEL NORTE\":\"015\",\"EL DORADO\":\"017\",\"FRESNO\":\"019\",\"GLENN\":\"021\",\"HUMBOLDT\":\"023\",\n",
    "    \"IMPERIAL\":\"025\",\"INYO\":\"027\",\"KERN\":\"029\",\"KINGS\":\"031\",\"LAKE\":\"033\",\"LASSEN\":\"035\",\"LOS ANGELES\":\"037\",\n",
    "    \"MADERA\":\"039\",\"MARIN\":\"041\",\"MARIPOSA\":\"043\",\"MENDOCINO\":\"045\",\"MERCED\":\"047\",\"MODOC\":\"049\",\n",
    "    \"MONO\":\"051\",\"MONTEREY\":\"053\",\"NAPA\":\"055\",\"NEVADA\":\"057\",\"ORANGE\":\"059\",\"PLACER\":\"061\",\"PLUMAS\":\"063\",\n",
    "    \"RIVERSIDE\":\"065\",\"SACRAMENTO\":\"067\",\"SAN BENITO\":\"069\",\"SAN BERNARDINO\":\"071\",\"SAN DIEGO\":\"073\",\n",
    "    \"SAN FRANCISCO\":\"075\",\"SAN JOAQUIN\":\"077\",\"SAN LUIS OBISPO\":\"079\",\"SAN MATEO\":\"081\",\"SANTA BARBARA\":\"083\",\n",
    "    \"SANTA CLARA\":\"085\",\"SANTA CRUZ\":\"087\",\"SHASTA\":\"089\",\"SIERRA\":\"091\",\"SISKIYOU\":\"093\",\"SOLANO\":\"095\",\n",
    "    \"SONOMA\":\"097\",\"STANISLAUS\":\"099\",\"SUTTER\":\"101\",\"TEHAMA\":\"103\",\"TRINITY\":\"105\",\"TULARE\":\"107\",\n",
    "    \"TUOLUMNE\":\"109\",\"VENTURA\":\"111\",\"YOLO\":\"113\",\"YUBA\":\"115\"\n",
    "}\n",
    "\n",
    "# ------------- Helpers -------------\n",
    "def _read_tracts(zip_path):\n",
    "    \"\"\"Read TIGER tracts from zip (directly if supported; else extract).\"\"\"\n",
    "    try:\n",
    "        return gpd.read_file(zip_path)\n",
    "    except Exception:\n",
    "        with tempfile.TemporaryDirectory() as td:\n",
    "            with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
    "                zf.extractall(td)\n",
    "            shp = None\n",
    "            for root, _, files in os.walk(td):\n",
    "                for f in files:\n",
    "                    if f.lower().endswith(\".shp\"):\n",
    "                        shp = os.path.join(root, f); break\n",
    "                if shp: break\n",
    "            if not shp:\n",
    "                raise FileNotFoundError(\"No .shp found inside tl_2024_06_tract.zip\")\n",
    "            return gpd.read_file(shp)\n",
    "\n",
    "def _clean_county_value(x):\n",
    "    \"\"\"\n",
    "    Accepts county as numeric FIPS (e.g., 37 or '037') OR name (e.g., 'Los Angeles County').\n",
    "    Returns 3-digit county FIPS string if possible; else None.\n",
    "    \"\"\"\n",
    "    if pd.isna(x):\n",
    "        return None\n",
    "    s = str(x).strip()\n",
    "    # numeric FIPS?\n",
    "    if re.fullmatch(r\"\\d{1,3}\", s):\n",
    "        return s.zfill(3)\n",
    "    # name path\n",
    "    s = s.upper()\n",
    "    s = re.sub(r\"\\s+COUNTY$\", \"\", s)     # drop trailing ' COUNTY'\n",
    "    s = re.sub(r\"\\s+\", \" \", s)           # normalize spaces\n",
    "    return CA_COUNTY_TO_FIPS.get(s)\n",
    "\n",
    "def _tract_to_6(x):\n",
    "    \"\"\"Coerce tract code like 4010.02 or 401002 or 401002000 → '401002' (last 6 digits).\"\"\"\n",
    "    s = str(x).strip()\n",
    "    s = re.sub(r\"\\D\", \"\", s)             # keep digits only\n",
    "    return s[-6:].zfill(6)\n",
    "\n",
    "# ------------- 1) Read TIGER and normalize GEOID -------------\n",
    "tracts_raw = _read_tracts(TRACTS_ZIP)\n",
    "if tracts_raw.crs is None:\n",
    "    tracts_raw.set_crs(4269, inplace=True)  # TIGER is typically NAD83\n",
    "\n",
    "cols = {c.lower(): c for c in tracts_raw.columns}\n",
    "geoid_col = next((cols[c] for c in [\"geoid24\",\"geoid\",\"geoid20\",\"geoid10\"] if c in cols), None)\n",
    "\n",
    "if geoid_col is None:\n",
    "    statefp = cols.get(\"statefp\") or cols.get(\"statefp24\")\n",
    "    countyfp = cols.get(\"countyfp\") or cols.get(\"countyfp24\")\n",
    "    tractce  = cols.get(\"tractce\")  or cols.get(\"tractce24\")\n",
    "    if not all([statefp, countyfp, tractce]):\n",
    "        raise ValueError(\"TIGER tracts missing GEOID and STATEFP/COUNTYFP/TRACTCE.\")\n",
    "    tracts_raw[\"GEOID\"] = (\n",
    "        tracts_raw[statefp].astype(str).str.strip().str.zfill(2) +\n",
    "        tracts_raw[countyfp].astype(str).str.strip().str.zfill(3) +\n",
    "        tracts_raw[tractce].astype(str).str.strip().str.zfill(6)\n",
    "    )\n",
    "else:\n",
    "    tracts_raw[\"GEOID\"] = tracts_raw[geoid_col].astype(str).str.strip()\n",
    "\n",
    "# keep essential fields that actually exist\n",
    "keep_maybe = [\"GEOID\",\"NAME\",\"NAMELSAD\",\"ALAND\",\"AWATER\"]\n",
    "keep_cols = [c for c in keep_maybe if c in tracts_raw.columns]\n",
    "tracts = tracts_raw[keep_cols + [\"geometry\"]].copy()\n",
    "\n",
    "# ------------- 2) Read ACS and build 11-digit GEOID -------------\n",
    "acs = pd.read_csv(ACS_CSV, keep_default_na=False)\n",
    "\n",
    "# Must have tract & county; normalize to standard socio names:\n",
    "if \"tract\" not in acs.columns:\n",
    "    raise ValueError(\"ACS CSV must contain a 'tract' column (e.g., 4010.02 or 401002).\")\n",
    "if \"county\" not in acs.columns:\n",
    "    raise ValueError(\"ACS CSV must contain a 'county' column (county name OR county FIPS).\")\n",
    "\n",
    "# Map your provided columns → standard names\n",
    "rename_map = {}\n",
    "# income (if present)\n",
    "for cand in [\"median_income\",\"med_income\",\"income\",\"household_income_median\"]:\n",
    "    if cand in acs.columns:\n",
    "        rename_map[cand] = \"median_income\"; break\n",
    "# elderly\n",
    "if \"pct_age65plus\" in acs.columns:\n",
    "    rename_map[\"pct_age65plus\"] = \"percent_elderly\"\n",
    "elif \"pct_elderly\" in acs.columns:\n",
    "    rename_map[\"pct_elderly\"] = \"percent_elderly\"\n",
    "elif \"percent_elderly\" in acs.columns:\n",
    "    pass  # already correct\n",
    "# poverty\n",
    "if \"pct_poverty\" in acs.columns:\n",
    "    rename_map[\"pct_poverty\"] = \"percent_below_poverty\"\n",
    "elif \"pct_below_poverty\" in acs.columns:\n",
    "    rename_map[\"pct_below_poverty\"] = \"percent_below_poverty\"\n",
    "elif \"percent_below_poverty\" in acs.columns:\n",
    "    pass  # already correct\n",
    "\n",
    "acs = acs.rename(columns=rename_map)\n",
    "\n",
    "# county FIPS (3 digits) resolved from name or numeric\n",
    "acs[\"_county_fips3\"] = acs[\"county\"].apply(_clean_county_value)\n",
    "missing_cnty = acs[acs[\"_county_fips3\"].isna()]\n",
    "if not missing_cnty.empty:\n",
    "    print(\"WARNING: Unrecognized county values (first 10):\", missing_cnty[\"county\"].head(10).tolist())\n",
    "\n",
    "# 6-digit tract code\n",
    "acs[\"_tract6\"] = acs[\"tract\"].apply(_tract_to_6)\n",
    "\n",
    "# Full 11-digit GEOID\n",
    "acs[\"GEOID\"] = \"06\" + acs[\"_county_fips3\"].fillna(\"000\") + acs[\"_tract6\"]\n",
    "\n",
    "# ------------- 3) Merge ACS onto TIGER tracts -------------\n",
    "tracts_merged = tracts.merge(acs, on=\"GEOID\", how=\"left\")\n",
    "n_total      = len(tracts_merged)\n",
    "n_unmatched  = tracts_merged[\"tract\"].isna().sum()\n",
    "print(f\"Tracts total: {n_total} | with ACS match: {n_total - n_unmatched} | unmatched: {n_unmatched}\")\n",
    "\n",
    "if n_unmatched > 0:\n",
    "    print(\"Example unmatched GEOIDs:\", tracts_merged.loc[tracts_merged[\"tract\"].isna(), \"GEOID\"].head(10).tolist())\n",
    "    print(\"Example ACS GEOIDs:\", acs[\"GEOID\"].head(10).tolist())\n",
    "\n",
    "# ------------- 4) Reproject to EPSG:3310 and compute centroids -------------\n",
    "tracts_poly_3310 = tracts_merged.to_crs(3310)\n",
    "centroids = tracts_poly_3310.copy()\n",
    "centroids[\"geometry\"] = centroids.geometry.centroid\n",
    "\n",
    "# ------------- 5) Save outputs (avoid NAME collisions; only save columns that exist) -------------\n",
    "# Ensure the two core fields exist if present in CSV; otherwise we still save what's available.\n",
    "preferred_socio = [\"median_income\", \"percent_elderly\", \"percent_below_poverty\", \"population\"]\n",
    "cent_cols_lower = {c.lower(): c for c in centroids.columns}\n",
    "socio_keep_present = [cent_cols_lower[k] for k in preferred_socio if k in cent_cols_lower]\n",
    "\n",
    "# Save polygons (full merged; CRS 3310)\n",
    "tracts_poly_3310.to_parquet(OUT_POLY, index=False)\n",
    "\n",
    "# Save centroids with safe subset (GEOID, geometry + selected socio cols)\n",
    "centroids_out = centroids.loc[:, [\"GEOID\", \"geometry\"] + socio_keep_present].copy()\n",
    "centroids_out.to_parquet(OUT_CENTROIDS, index=False)\n",
    "\n",
    "# Alias file for downstream code that expects this name\n",
    "centroids_out.to_parquet(OUT_ACS_CENTS, index=False)\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\" -\", OUT_POLY)\n",
    "print(\" -\", OUT_CENTROIDS)\n",
    "print(\" -\", OUT_ACS_CENTS)\n",
    "print(\"Centroid columns saved:\", list(centroids_out.columns))\n",
    "\n",
    "# ------------- 6) Tiny samples -------------\n",
    "show_poly_cols = [\"GEOID\"] + [c for c in [\"NAME\",\"NAMELSAD\"] if c in tracts_poly_3310.columns]\n",
    "print(\"\\nSample polygons:\")\n",
    "print(tracts_poly_3310[show_poly_cols].head(3))\n",
    "print(\"\\nSample centroids:\")\n",
    "print(centroids_out.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1190431e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === HeatShield | Join ACS equity (GeoParquet) to grid metrics with flexible column matching ===\n",
    "\n",
    "import os, glob, numpy as np, pandas as pd, geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Paths\n",
    "ACS_PATH   = \"results/acs_2023_ca_centroids_merged_3310.parquet\"  # from your preprocessing step\n",
    "GRID_PATH  = \"results/grid_3000m_CA_epsg3310.parquet\"\n",
    "\n",
    "def _find_final_parquet():\n",
    "    cands = []\n",
    "    if \"out_dir\" in globals().get(\"CONFIG\", {}):\n",
    "        cands += glob.glob(os.path.join(CONFIG[\"out_dir\"], \"final_daily_grid_3000m*.parquet\"))\n",
    "    cands += glob.glob(\"results/final_daily_grid_3000m*.parquet\")\n",
    "    cands = [p for p in cands if os.path.exists(p)]\n",
    "    if not cands:\n",
    "        raise FileNotFoundError(\"Could not locate final daily parquet (final_daily_grid_3000m*.parquet).\")\n",
    "    return max(cands, key=os.path.getmtime)\n",
    "\n",
    "FINAL_DAILY_PATH = _find_final_parquet()\n",
    "print(\"Using final daily:\", FINAL_DAILY_PATH)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1) Load grid and ACS centroids\n",
    "grid = gpd.read_parquet(GRID_PATH)\n",
    "assert grid.crs and grid.crs.to_epsg() == 3310, \"Grid must be EPSG:3310\"\n",
    "assert \"grid_id\" in grid.columns, \"Grid must have 'grid_id'\"\n",
    "\n",
    "g_acs = gpd.read_parquet(ACS_PATH)\n",
    "if g_acs.crs is None:\n",
    "    # assume they are 3310 as saved; if not, set explicitly\n",
    "    g_acs = g_acs.set_crs(3310)\n",
    "elif g_acs.crs.to_epsg() != 3310:\n",
    "    g_acs = g_acs.to_crs(3310)\n",
    "\n",
    "# ---- Flexible column resolution\n",
    "cols = list(g_acs.columns)\n",
    "lower_map = {c.lower(): c for c in cols}\n",
    "\n",
    "def _resolve(candidates):\n",
    "    # find the first candidate present (case-insensitive)\n",
    "    for cand in candidates:\n",
    "        if cand in cols: return cand\n",
    "        lc = cand.lower()\n",
    "        if lc in lower_map: return lower_map[lc]\n",
    "    return None\n",
    "\n",
    "INCOME_COL  = _resolve([\"median_income\",\"income\",\"med_income\",\"household_income_median\"])\n",
    "ELDERLY_COL = _resolve([\"percent_elderly\",\"pct_elderly\",\"elderly_percent\",\"pct_65_plus\",\"share_65plus\",\"pct_over_65\"])\n",
    "POVERTY_COL = _resolve([\"percent_below_poverty\",\"pct_below_poverty\",\"pct_poverty\",\"poverty_percent\",\"poverty_pct\"])\n",
    "\n",
    "if \"GEOID\" not in g_acs.columns:\n",
    "    raise ValueError(\"ACS parquet must contain 'GEOID' (built in the preprocessing step).\")\n",
    "\n",
    "missing = []\n",
    "if INCOME_COL is None:  missing.append(\"median_income (or income alias)\")\n",
    "if ELDERLY_COL is None: missing.append(\"percent_elderly (or alias)\")\n",
    "if POVERTY_COL is None: missing.append(\"percent_below_poverty (or alias)\")\n",
    "if missing:\n",
    "    raise ValueError(f\"ACS parquet is missing required socio columns or aliases: {missing}\\n\"\n",
    "                     f\"Available columns: {cols}\")\n",
    "\n",
    "# Normalize to standard names for downstream\n",
    "g_acs = g_acs.rename(columns={\n",
    "    INCOME_COL:  \"median_income\",\n",
    "    ELDERLY_COL: \"percent_elderly\",\n",
    "    POVERTY_COL: \"percent_below_poverty\",\n",
    "})\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2) Spatial join: assign ACS centroid to containing grid cell\n",
    "joined = gpd.sjoin(\n",
    "    gpd.GeoDataFrame(geometry=grid.geometry, data=grid[[\"grid_id\"]], crs=3310),\n",
    "    g_acs[[\"GEOID\",\"median_income\",\"percent_elderly\",\"percent_below_poverty\",\"geometry\"]],\n",
    "    how=\"right\",\n",
    "    predicate=\"contains\"\n",
    ")[[\"grid_id\",\"GEOID\",\"median_income\",\"percent_elderly\",\"percent_below_poverty\"]]\n",
    "\n",
    "# Average within cell (if multiple tracts fall inside)\n",
    "tract_to_grid = (\n",
    "    joined.groupby(\"grid_id\", as_index=False)\n",
    "          .agg({\n",
    "              \"median_income\":\"mean\",\n",
    "              \"percent_elderly\":\"mean\",\n",
    "              \"percent_below_poverty\":\"mean\",\n",
    "          })\n",
    "          .rename(columns={\n",
    "              \"median_income\":\"income\",\n",
    "              \"percent_elderly\":\"pct_elderly\",\n",
    "              \"percent_below_poverty\":\"pct_poverty\"\n",
    "          })\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3) Social Vulnerability Index (SVI)\n",
    "def _z(s):\n",
    "    s = pd.to_numeric(s, errors=\"coerce\")\n",
    "    return (s - s.mean()) / (s.std(ddof=0) if s.std(ddof=0) else 1.0)\n",
    "\n",
    "tract_to_grid[\"z_poverty\"]  = _z(tract_to_grid[\"pct_poverty\"])\n",
    "tract_to_grid[\"z_elderly\"]  = _z(tract_to_grid[\"pct_elderly\"])\n",
    "tract_to_grid[\"z_income_r\"] = _z(-tract_to_grid[\"income\"])  # lower income = higher vulnerability\n",
    "\n",
    "tract_to_grid[\"SVI\"] = tract_to_grid[[\"z_poverty\",\"z_elderly\",\"z_income_r\"]].sum(axis=1)\n",
    "tract_to_grid[\"SVI_pct\"] = tract_to_grid[\"SVI\"].rank(pct=True) * 100\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4) Load final daily environmental dataset and summarize per grid\n",
    "env = pd.read_parquet(FINAL_DAILY_PATH)\n",
    "env[\"date\"] = pd.to_datetime(env[\"date\"], errors=\"coerce\").dt.date.astype(str)\n",
    "DATE_FROM, DATE_TO = \"2024-06-01\", \"2024-10-31\"\n",
    "env = env[(env[\"date\"] >= DATE_FROM) & (env[\"date\"] <= DATE_TO)]\n",
    "\n",
    "def has(c): return c in env.columns\n",
    "agg_dict = {}\n",
    "\n",
    "if has(\"pm25\"): agg_dict[\"pm25\"] = \"mean\"\n",
    "if has(\"pm25_airnow\"): agg_dict[\"pm25_airnow\"] = \"mean\"\n",
    "if has(\"pm25_aqs\"): agg_dict[\"pm25_aqs\"] = \"mean\"\n",
    "\n",
    "for c in [\"tavg_c_value\",\"tmax_c_value\",\"tmin_c_value\"]:\n",
    "    if has(c): agg_dict[c] = \"mean\"\n",
    "\n",
    "if has(\"prcp_mm\"): agg_dict[\"prcp_mm\"] = \"sum\"\n",
    "for c in [\"fire_count_in\",\"fire_buffer_count\"]:\n",
    "    if has(c): agg_dict[c] = \"sum\"\n",
    "if has(\"nearest_fire_km\"): agg_dict[\"nearest_fire_km\"] = \"mean\"\n",
    "\n",
    "if has(\"smoke_frac\"):\n",
    "    agg_dict[\"smoke_frac\"] = \"mean\"\n",
    "    env[\"_smoke_day\"] = (env[\"smoke_frac\"].fillna(0) > 0).astype(int)\n",
    "\n",
    "per_grid = env.groupby(\"grid_id\", as_index=False).agg(agg_dict)\n",
    "if \"_smoke_day\" in env.columns:\n",
    "    smoke_days = env.groupby(\"grid_id\", as_index=False)[\"_smoke_day\"].sum().rename(columns={\"_smoke_day\":\"smoke_days\"})\n",
    "    per_grid = per_grid.merge(smoke_days, on=\"grid_id\", how=\"left\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5) Join SVI → environmental summaries\n",
    "final = per_grid.merge(\n",
    "    tract_to_grid[[\"grid_id\",\"income\",\"pct_elderly\",\"pct_poverty\",\"SVI\",\"SVI_pct\"]],\n",
    "    on=\"grid_id\", how=\"left\"\n",
    ")\n",
    "\n",
    "print(\"\\n=== Joined grid metrics + equity ===\")\n",
    "print(\"Rows:\", len(final), \"| Columns:\", len(final.columns))\n",
    "print(final.head(3))\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 6) Correlations & comparisons\n",
    "def _corrsafe(df, x, y):\n",
    "    if x in df.columns and y in df.columns:\n",
    "        return df[[x, y]].dropna().corr().iloc[0, 1]\n",
    "    return np.nan\n",
    "\n",
    "print(\"\\nCorrelations with SVI (higher = more vulnerable):\")\n",
    "for k in [\"pm25\",\"pm25_airnow\",\"pm25_aqs\",\"tavg_c_value\",\"tmax_c_value\",\"smoke_frac\",\"smoke_days\"]:\n",
    "    if k in final.columns:\n",
    "        r = _corrsafe(final, \"SVI\", k)\n",
    "        if not np.isnan(r):\n",
    "            print(f\"  SVI vs {k:>12}: {r:+.3f}\")\n",
    "\n",
    "q10, q90 = final[\"SVI\"].quantile(0.10), final[\"SVI\"].quantile(0.90)\n",
    "low, high = final[final[\"SVI\"] <= q10], final[final[\"SVI\"] >= q90]\n",
    "\n",
    "def _mean_cols(df, cols):\n",
    "    cols = [c for c in cols if c in df.columns]\n",
    "    return df[cols].mean().round(3)\n",
    "\n",
    "compare_cols = [\"pm25\",\"pm25_airnow\",\"pm25_aqs\",\"tavg_c_value\",\"tmax_c_value\",\n",
    "                \"smoke_frac\",\"smoke_days\",\"prcp_mm\",\"fire_count_in\"]\n",
    "\n",
    "print(\"\\nMean metrics (bottom 10% SVI):\")\n",
    "print(_mean_cols(low, compare_cols))\n",
    "print(\"\\nMean metrics (top 10% SVI):\")\n",
    "print(_mean_cols(high, compare_cols))\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 7) Quick scatterplots\n",
    "plt.figure(figsize=(6,5))\n",
    "if \"pm25\" in final.columns:\n",
    "    plt.scatter(final[\"SVI\"], final[\"pm25\"], s=6, alpha=0.5)\n",
    "    plt.xlabel(\"SVI (z-score sum)\"); plt.ylabel(\"Mean PM2.5 (µg/m³)\")\n",
    "    plt.title(\"SVI vs Mean PM2.5 (Jun–Oct 2024)\")\n",
    "    plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "if \"smoke_days\" in final.columns:\n",
    "    plt.scatter(final[\"SVI\"], final[\"smoke_days\"], s=6, alpha=0.5)\n",
    "    plt.xlabel(\"SVI (z-score sum)\"); plt.ylabel(\"Smoke Days (Jun–Oct 2024)\")\n",
    "    plt.title(\"SVI vs Smoke Days (Jun–Oct 2024)\")\n",
    "    plt.show()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 8) Save ready-to-map output\n",
    "OUT_EQUITY = os.path.join(CONFIG.get(\"out_dir\", \"results\"), \"final_grid_equity_2024Q3_CA.parquet\")\n",
    "final.to_parquet(OUT_EQUITY, index=False)\n",
    "print(\"\\nSaved:\", OUT_EQUITY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e103fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === HeatShield | \"Double Burden\" WOW analysis: SVI × EEI (simple & explainable) ===\n",
    "# Inputs:\n",
    "#   - results/final_grid_equity_2024Q3_CA.parquet   (from previous join step; has SVI + env summaries)\n",
    "#   - results/grid_3000m_CA_epsg3310.parquet        (grid geometry, EPSG:3310)\n",
    "# Outputs:\n",
    "#   - results/equity_double_burden_2024Q3_CA.parquet\n",
    "#   - (plots) scatter & map shown inline\n",
    "\n",
    "import os, glob, numpy as np, pandas as pd, geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------- Paths & helpers ----------------\n",
    "RES_DIR = \"results\"\n",
    "os.makedirs(RES_DIR, exist_ok=True)\n",
    "\n",
    "def _find_equity():\n",
    "    # prefer the canonical name we saved earlier\n",
    "    cands = [\n",
    "        os.path.join(RES_DIR, \"final_grid_equity_2024Q3_CA.parquet\"),\n",
    "        *glob.glob(os.path.join(RES_DIR, \"final_grid_equity_*.parquet\")),\n",
    "        *glob.glob(os.path.join(globals().get(\"CONFIG\", {}).get(\"out_dir\",\"results\"), \"final_grid_equity_*.parquet\")),\n",
    "    ]\n",
    "    cands = [p for p in cands if os.path.exists(p)]\n",
    "    if not cands: raise FileNotFoundError(\"Could not find final_grid_equity_* parquet.\")\n",
    "    return max(cands, key=os.path.getmtime)\n",
    "\n",
    "EQUITY_PATH = _find_equity()\n",
    "GRID_PATH   = os.path.join(RES_DIR, \"grid_3000m_CA_epsg3310.parquet\")\n",
    "print(\"Using equity file:\", EQUITY_PATH)\n",
    "\n",
    "# ---------------- Load data ----------------\n",
    "df = pd.read_parquet(EQUITY_PATH)\n",
    "grid = gpd.read_parquet(GRID_PATH)\n",
    "assert grid.crs and grid.crs.to_epsg()==3310, \"Grid must be EPSG:3310\"\n",
    "\n",
    "# ---------------- Build EEI (Environmental Exposure Index) ----------------\n",
    "# We keep this SIMPLE:\n",
    "#  - PM2.5 (mean across the season)\n",
    "#  - Temperature (use tmax if available, else tavg)\n",
    "#  - Smoke burden (smoke_days if available, else mean smoke_frac)\n",
    "# EEI = mean(z of available components). Higher EEI = worse exposure.\n",
    "\n",
    "# Pick temperature column\n",
    "temp_col = \"tmax_c_value\" if \"tmax_c_value\" in df.columns else (\"tavg_c_value\" if \"tavg_c_value\" in df.columns else None)\n",
    "smoke_count_col = \"smoke_days\" if \"smoke_days\" in df.columns else None\n",
    "smoke_frac_col  = \"smoke_frac\" if \"smoke_frac\" in df.columns else None\n",
    "\n",
    "components = []\n",
    "if \"pm25\" in df.columns: components.append((\"pm25\", +1))\n",
    "if temp_col:             components.append((temp_col, +1))\n",
    "if smoke_count_col:      components.append((smoke_count_col, +1))\n",
    "elif smoke_frac_col:     components.append((smoke_frac_col, +1))\n",
    "\n",
    "if not components:\n",
    "    raise ValueError(\"No environmental columns found to compute EEI (need at least one of pm25, tmax/tavg, smoke_days/smoke_frac).\")\n",
    "\n",
    "def _z(s):\n",
    "    s = pd.to_numeric(s, errors=\"coerce\")\n",
    "    std = s.std(ddof=0)\n",
    "    return (s - s.mean()) / (std if std else 1.0)\n",
    "\n",
    "for col, sign in components:\n",
    "    df[f\"z_{col}\"] = _z(df[col]) * sign\n",
    "\n",
    "zcols = [f\"z_{c}\" for c,_ in components]\n",
    "df[\"EEI\"] = df[zcols].mean(axis=1)\n",
    "\n",
    "# ---------------- Sanity checks ----------------\n",
    "need = [\"grid_id\", \"EEI\", \"SVI\"]\n",
    "missing = [c for c in need if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing required columns: {missing}. Make sure your previous join created SVI and env summaries.\")\n",
    "\n",
    "# Correlation (headline number)\n",
    "corr = df[[\"SVI\",\"EEI\"]].dropna().corr().iloc[0,1]\n",
    "print(f\"\\nSVI–EEI correlation (statewide): r = {corr:+.3f}\")\n",
    "\n",
    "# ---------------- Classify “double burden” ----------------\n",
    "# Define high = top quartile (75th percentile). Adjust to taste.\n",
    "eei_thr = df[\"EEI\"].quantile(0.75)\n",
    "svi_thr = df[\"SVI\"].quantile(0.75)\n",
    "\n",
    "df[\"high_EEI\"] = (df[\"EEI\"] >= eei_thr).astype(int)\n",
    "df[\"high_SVI\"] = (df[\"SVI\"] >= svi_thr).astype(int)\n",
    "\n",
    "def _class_row(r):\n",
    "    if r[\"high_EEI\"]==1 and r[\"high_SVI\"]==1: return \"Double burden\"\n",
    "    if r[\"high_EEI\"]==1 and r[\"high_SVI\"]==0: return \"High exposure only\"\n",
    "    if r[\"high_EEI\"]==0 and r[\"high_SVI\"]==1: return \"High vulnerability only\"\n",
    "    return \"Neither\"\n",
    "\n",
    "df[\"burden_class\"] = df[[\"high_EEI\",\"high_SVI\"]].apply(_class_row, axis=1)\n",
    "\n",
    "share = df[\"burden_class\"].value_counts(normalize=True).mul(100).round(1)\n",
    "print(\"\\nShare of grid cells by class (%):\")\n",
    "print(share.to_string())\n",
    "\n",
    "# ---------------- Scatter: SVI vs EEI (simple & explainable) ----------------\n",
    "plt.figure(figsize=(6.2,5.2))\n",
    "# light base\n",
    "plt.scatter(df[\"SVI\"], df[\"EEI\"], s=6, alpha=0.25, label=\"All cells\")\n",
    "# highlight double burden\n",
    "dbl = df[df[\"burden_class\"]==\"Double burden\"]\n",
    "if not dbl.empty:\n",
    "    plt.scatter(dbl[\"SVI\"], dbl[\"EEI\"], s=10, alpha=0.8, label=\"Double burden\")\n",
    "plt.xlabel(\"SVI (higher = more socially vulnerable)\")\n",
    "plt.ylabel(\"EEI (higher = more environmental burden)\")\n",
    "plt.title(\"Double Burden: Social Vulnerability vs Environmental Exposure\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ---------------- Map: highlight double burden cells ----------------\n",
    "g = grid[[\"grid_id\",\"geometry\"]].merge(df[[\"grid_id\",\"burden_class\"]], on=\"grid_id\", how=\"left\")\n",
    "g = g.set_crs(3310)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7.5,8))\n",
    "# base\n",
    "g.plot(ax=ax, color=\"#eaeaea\", linewidth=0, alpha=0.7)\n",
    "# single-color overlay for clarity\n",
    "g[g[\"burden_class\"]==\"Double burden\"].plot(ax=ax, color=\"#d62728\", linewidth=0, alpha=0.9, label=\"Double burden\")\n",
    "ax.set_axis_off()\n",
    "ax.set_title(\"Where High Vulnerability Meets High Exposure (Top Quartiles)\")\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "# ---------------- Save ready-to-map output ----------------\n",
    "OUT = os.path.join(RES_DIR, \"equity_double_burden_2024Q3_CA.parquet\")\n",
    "keep_cols = [\"grid_id\",\"SVI\",\"EEI\",\"burden_class\",\"high_SVI\",\"high_EEI\"] + [c for c,_ in components] + zcols\n",
    "df[keep_cols].to_parquet(OUT, index=False)\n",
    "print(\"\\nSaved:\", OUT)\n",
    "\n",
    "# ---------------- Friendly one-paragraph explainer you can reuse ----------------\n",
    "print(\"\"\"\n",
    "Explainer:\n",
    "We convert each environmental metric (PM2.5, max/avg temperature, and smoke burden) into a standard z-score so they’re on the same scale, then average those z-scores to get an Environmental Exposure Index (EEI) —\n",
    "higher EEI means more environmental stress. We compare EEI to the Social Vulnerability Index (SVI), which rises with poverty and elderly share and falls with income. Cells in the top 25% for both SVI and EEI are labeled “double burden”: these are places where vulnerable communities are simultaneously facing higher environmental hazards. The scatter shows the overall relationship (r above), and the map highlights exactly where double-burden cells are located across California.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a66bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === HeatShield | Robust rich map (auto 3×3 → fallback 2×2; rebuilds EEI/burden_class if missing) ===\n",
    "\n",
    "import os, numpy as np, pandas as pd, geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "RES_DIR = \"results\"\n",
    "GRID_PQ   = os.path.join(RES_DIR, \"grid_3000m_CA_epsg3310.parquet\")\n",
    "BURDEN_PQ = os.path.join(RES_DIR, \"equity_double_burden_2024Q3_CA.parquet\")  # expects grid_id, SVI, EEI, burden_class (but we can rebuild)\n",
    "\n",
    "# --- Load\n",
    "grid = gpd.read_parquet(GRID_PQ)\n",
    "assert grid.crs is not None and int(grid.crs.to_epsg()) == 3310, \"Grid must be EPSG:3310\"\n",
    "\n",
    "df = pd.read_parquet(BURDEN_PQ)\n",
    "\n",
    "# --- Preflight: rebuild EEI and burden_class if missing\n",
    "def _z(s):\n",
    "    s = pd.to_numeric(s, errors=\"coerce\")\n",
    "    std = s.std(ddof=0)\n",
    "    return (s - s.mean()) / (std if std else 1.0)\n",
    "\n",
    "def ensure_eei_and_burden(df):\n",
    "    need_cols = {\"SVI\"}\n",
    "    missing = need_cols - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing column(s) in equity table: {missing}. Make sure SVI exists in {BURDEN_PQ}.\")\n",
    "\n",
    "    # EEI: rebuild if absent\n",
    "    if \"EEI\" not in df.columns or df[\"EEI\"].isna().all():\n",
    "        # Try to rebuild from available inputs and their z-scores if present\n",
    "        comp_cols = []\n",
    "        if \"pm25\" in df.columns: comp_cols.append(\"pm25\")\n",
    "        if \"tmax_c_value\" in df.columns: comp_cols.append(\"tmax_c_value\")\n",
    "        elif \"tavg_c_value\" in df.columns: comp_cols.append(\"tavg_c_value\")\n",
    "        if \"smoke_days\" in df.columns: comp_cols.append(\"smoke_days\")\n",
    "        elif \"smoke_frac\" in df.columns: comp_cols.append(\"smoke_frac\")\n",
    "\n",
    "        if not comp_cols and {\"z_pm25\",\"z_tmax_c_value\",\"z_tavg_c_value\",\"z_smoke_days\",\"z_smoke_frac\"}.isdisjoint(df.columns) is False:\n",
    "            # Fallback: use existing z_* columns\n",
    "            zcols = [c for c in [\"z_pm25\",\"z_tmax_c_value\",\"z_tavg_c_value\",\"z_smoke_days\",\"z_smoke_frac\"] if c in df.columns]\n",
    "            df[\"EEI\"] = df[zcols].mean(axis=1)\n",
    "        else:\n",
    "            # Fresh z-scores from raw\n",
    "            zcols = []\n",
    "            for c in comp_cols:\n",
    "                df[f\"z_{c}\"] = _z(df[c])\n",
    "                zcols.append(f\"z_{c}\")\n",
    "            if zcols:\n",
    "                df[\"EEI\"] = df[zcols].mean(axis=1)\n",
    "            else:\n",
    "                raise ValueError(\"Cannot compute EEI: no environmental columns (pm25, tmax/tavg, smoke_days/smoke_frac) found.\")\n",
    "\n",
    "    # burden_class: rebuild if absent\n",
    "    if \"burden_class\" not in df.columns or df[\"burden_class\"].isna().all():\n",
    "        eei_thr = df[\"EEI\"].quantile(0.75)\n",
    "        svi_thr = df[\"SVI\"].quantile(0.75)\n",
    "        df[\"high_EEI\"] = (df[\"EEI\"] >= eei_thr).astype(int)\n",
    "        df[\"high_SVI\"] = (df[\"SVI\"] >= svi_thr).astype(int)\n",
    "        def _class_row(r):\n",
    "            if r[\"high_EEI\"]==1 and r[\"high_SVI\"]==1: return \"Double burden\"\n",
    "            if r[\"high_EEI\"]==1 and r[\"high_SVI\"]==0: return \"High exposure only\"\n",
    "            if r[\"high_EEI\"]==0 and r[\"high_SVI\"]==1: return \"High vulnerability only\"\n",
    "            return \"Neither\"\n",
    "        df[\"burden_class\"] = df[[\"high_EEI\",\"high_SVI\"]].apply(_class_row, axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = ensure_eei_and_burden(df)\n",
    "\n",
    "# Clean numeric columns\n",
    "for c in [\"SVI\",\"EEI\"]:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\").replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Join geometry\n",
    "g = grid[[\"grid_id\",\"geometry\"]].merge(df, on=\"grid_id\", how=\"left\")\n",
    "if g.crs is None:\n",
    "    g = g.set_crs(3310)\n",
    "\n",
    "# --- Quantile binning with graceful fallback\n",
    "def quantile_bins(s, q=3):\n",
    "    s = pd.to_numeric(s, errors=\"coerce\")\n",
    "    if s.dropna().nunique() <= 1:\n",
    "        return pd.Series(np.zeros(len(s), dtype=int)), 1\n",
    "    try:\n",
    "        binned, _ = pd.qcut(s, q=q, labels=False, retbins=True, duplicates=\"drop\")\n",
    "        if binned.notna().any():\n",
    "            k = int(binned.max() + 1)\n",
    "            return binned.fillna(0).astype(int), k\n",
    "        else:\n",
    "            binned, _ = pd.qcut(s, q=2, labels=False, retbins=True, duplicates=\"drop\")\n",
    "            return binned.fillna(0).astype(int), 2\n",
    "    except ValueError:\n",
    "        binned = (s > s.median()).astype(int)\n",
    "        return binned.fillna(0).astype(int), 2\n",
    "\n",
    "g[\"SVI_q\"], n_svi = quantile_bins(g[\"SVI\"], q=3)\n",
    "g[\"EEI_q\"], n_eei = quantile_bins(g[\"EEI\"], q=3)\n",
    "\n",
    "# --- Bivariate palette\n",
    "def bivariate_palette(n_rows, n_cols):\n",
    "    if (n_rows, n_cols) == (3, 3):\n",
    "        colors = [\n",
    "            \"#e8e8e8\", \"#b8d6be\", \"#64acbe\",\n",
    "            \"#d4b9da\", \"#a5add3\", \"#5698b9\",\n",
    "            \"#c085af\", \"#8c62aa\", \"#4e6d9d\"\n",
    "        ]\n",
    "    elif (n_rows, n_cols) == (2, 2):\n",
    "        colors = [\"#e8e8e8\", \"#67a9cf\",\n",
    "                  \"#ef8a62\", \"#4e6d9d\"]\n",
    "    else:\n",
    "        # Generic grayscale\n",
    "        colors = [plt.cm.Greys(0.3 + 0.6*i/((n_rows*n_cols)-1)) for i in range(n_rows*n_cols)]\n",
    "    return {(r*n_cols + c): colors[r*n_cols + c] for r in range(n_rows) for c in range(n_cols)}\n",
    "\n",
    "bivar_cmap = bivariate_palette(n_svi, n_eei)\n",
    "g[\"bivar_ix\"] = g[\"SVI_q\"]*n_eei + g[\"EEI_q\"]\n",
    "g[\"bivar_color\"] = g[\"bivar_ix\"].map(bivar_cmap)\n",
    "\n",
    "# --- Double-burden palette\n",
    "class_order = [\"Double burden\", \"High exposure only\", \"High vulnerability only\", \"Neither\"]\n",
    "g[\"burden_class\"] = pd.Categorical(g[\"burden_class\"], categories=class_order, ordered=True)\n",
    "palette = {\n",
    "    \"Double burden\": \"#b2182b\",\n",
    "    \"High exposure only\": \"#ef8a62\",\n",
    "    \"High vulnerability only\": \"#67a9cf\",\n",
    "    \"Neither\": \"#f7f7f7\"\n",
    "}\n",
    "\n",
    "# --- Helpers: scalebar & north arrow\n",
    "def add_scalebar(ax, length_km=100, loc=\"lower left\", pad=50000):\n",
    "    xmin, xmax = ax.get_xlim(); ymin, ymax = ax.get_ylim()\n",
    "    if loc == \"lower left\":\n",
    "        x0 = xmin + pad; y0 = ymin + pad\n",
    "    elif loc == \"lower right\":\n",
    "        x0 = xmax - pad - length_km*1000; y0 = ymin + pad\n",
    "    elif loc == \"upper left\":\n",
    "        x0 = xmin + pad; y0 = ymax - pad - 20000\n",
    "    else:\n",
    "        x0 = xmax - pad - length_km*1000; y0 = ymax - pad - 20000\n",
    "    ax.add_patch(Rectangle((x0, y0), length_km*1000, 8000, facecolor=\"black\"))\n",
    "    ax.add_patch(Rectangle((x0, y0), (length_km*1000)/2, 8000, facecolor=\"white\"))\n",
    "    ax.text(x0, y0+12000, f\"{length_km} km\", fontsize=9)\n",
    "\n",
    "def add_north_arrow(ax):\n",
    "    xmin, xmax = ax.get_xlim(); ymin, ymax = ax.get_ylim()\n",
    "    x = xmax - (xmax - xmin)*0.06\n",
    "    y = ymax - (ymax - ymin)*0.12\n",
    "    ax.annotate('N', xy=(x, y+25000), xytext=(x, y+45000), ha='center',\n",
    "                fontsize=12, fontweight='bold',\n",
    "                arrowprops=dict(arrowstyle=\"-|>\", linewidth=1.5))\n",
    "\n",
    "# --- Figure\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 9), dpi=140)\n",
    "\n",
    "# Panel A\n",
    "ax = axes[0]\n",
    "g.plot(ax=ax, color=\"#f0f0f0\", linewidth=0, alpha=0.8)\n",
    "for cls in class_order:\n",
    "    sub = g[g[\"burden_class\"] == cls]\n",
    "    if not sub.empty:\n",
    "        sub.plot(ax=ax, color=palette[cls], linewidth=0)\n",
    "ax.set_axis_off()\n",
    "ax.set_title(\"Double Burden Across California (Top Quartiles)\", fontsize=13)\n",
    "add_scalebar(ax, length_km=100); add_north_arrow(ax)\n",
    "\n",
    "handles = [plt.Line2D([0],[0], marker='s', linestyle='None', markersize=10,\n",
    "                       markerfacecolor=palette[c], label=c) for c in class_order]\n",
    "ax.legend(handles=handles, title=\"Class\", loc=\"lower left\", frameon=True)\n",
    "\n",
    "# Panel B\n",
    "ax2 = axes[1]\n",
    "g.plot(ax=ax2, color=\"#f0f0f0\", linewidth=0, alpha=0.8)\n",
    "g.dropna(subset=[\"bivar_color\"]).plot(ax=ax2, color=g.dropna(subset=[\"bivar_color\"])[\"bivar_color\"], linewidth=0)\n",
    "ax2.set_axis_off()\n",
    "ax2.set_title(f\"Bivariate Map: SVI × EEI ({n_svi}×{n_eei} quantiles)\", fontsize=13)\n",
    "add_scalebar(ax2, length_km=100); add_north_arrow(ax2)\n",
    "\n",
    "# Inset legend for bivariate\n",
    "# legend_ax = fig.add_axes([0.79, 0.18, 0.12, 0.12])\n",
    "# Inset legend for bivariate (top right)\n",
    "legend_ax = fig.add_axes([0.82, 0.72, 0.13, 0.13])  # [left, bottom, width, height]\n",
    "legend_ax.set_frame_on(True)\n",
    "legend_ax.set_xticks([]); legend_ax.set_yticks([])\n",
    "for r in range(n_svi):\n",
    "    for c in range(n_eei):\n",
    "        idx = r*n_eei + c\n",
    "        x0 = c / n_eei\n",
    "        y0 = r / n_svi\n",
    "        width = 1 / n_eei\n",
    "        height = 1 / n_svi\n",
    "        legend_ax.add_patch(\n",
    "            Rectangle(\n",
    "                (x0, y0),\n",
    "                width,\n",
    "                height,\n",
    "                facecolor=bivar_cmap.get(idx, \"#cccccc\"),\n",
    "                transform=legend_ax.transAxes,\n",
    "                edgecolor=\"none\"\n",
    "            )\n",
    "        )\n",
    "legend_ax.set_xlim(0,1); legend_ax.set_ylim(0,1)\n",
    "legend_ax.set_xlabel(\"EEI\", fontsize=8)\n",
    "legend_ax.set_ylabel(\"SVI\", fontsize=8)\n",
    "\n",
    "plt.suptitle(\"HeatShield — Where Social Vulnerability Meets Environmental Exposure (Jun–Oct 2024)\", fontsize=15)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Note: Bivariate bins used → SVI: {n_svi} levels, EEI: {n_eei} levels (auto-fallback if low variance).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1f85d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [\"SVI\", \"EEI\"]:\n",
    "    s = pd.to_numeric(g[col], errors=\"coerce\")\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(\"  min =\", round(s.min(), 3))\n",
    "    print(\"  max =\", round(s.max(), 3))\n",
    "    print(\"  mean =\", round(s.mean(), 3))\n",
    "    print(\"  std =\", round(s.std(ddof=0), 3))\n",
    "    print(\"  tercile cutpoints:\", [round(x, 3) for x in s.quantile([0.33, 0.66])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369c6de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Top 5% Double-Burden Grid Cells with County + Tract Map ===\n",
    "import geopandas as gpd, pandas as pd, numpy as np, matplotlib.pyplot as plt\n",
    "\n",
    "OUTLIER_PCT = 0.95\n",
    "ACS_PATH = \"results/acs_2023_ca_centroids_merged_3310.parquet\"\n",
    "\n",
    "# --- CA county FIPS lookup ---\n",
    "CA_COUNTY_TO_FIPS = {\n",
    "    \"ALAMEDA\":\"001\",\"ALPINE\":\"003\",\"AMADOR\":\"005\",\"BUTTE\":\"007\",\"CALAVERAS\":\"009\",\"COLUSA\":\"011\",\n",
    "    \"CONTRA COSTA\":\"013\",\"DEL NORTE\":\"015\",\"EL DORADO\":\"017\",\"FRESNO\":\"019\",\"GLENN\":\"021\",\"HUMBOLDT\":\"023\",\n",
    "    \"IMPERIAL\":\"025\",\"INYO\":\"027\",\"KERN\":\"029\",\"KINGS\":\"031\",\"LAKE\":\"033\",\"LASSEN\":\"035\",\"LOS ANGELES\":\"037\",\n",
    "    \"MADERA\":\"039\",\"MARIN\":\"041\",\"MARIPOSA\":\"043\",\"MENDOCINO\":\"045\",\"MERCED\":\"047\",\"MODOC\":\"049\",\n",
    "    \"MONO\":\"051\",\"MONTEREY\":\"053\",\"NAPA\":\"055\",\"NEVADA\":\"057\",\"ORANGE\":\"059\",\"PLACER\":\"061\",\"PLUMAS\":\"063\",\n",
    "    \"RIVERSIDE\":\"065\",\"SACRAMENTO\":\"067\",\"SAN BENITO\":\"069\",\"SAN BERNARDINO\":\"071\",\"SAN DIEGO\":\"073\",\n",
    "    \"SAN FRANCISCO\":\"075\",\"SAN JOAQUIN\":\"077\",\"SAN LUIS OBISPO\":\"079\",\"SAN MATEO\":\"081\",\"SANTA BARBARA\":\"083\",\n",
    "    \"SANTA CLARA\":\"085\",\"SANTA CRUZ\":\"087\",\"SHASTA\":\"089\",\"SIERRA\":\"091\",\"SISKIYOU\":\"093\",\"SOLANO\":\"095\",\n",
    "    \"SONOMA\":\"097\",\"STANISLAUS\":\"099\",\"SUTTER\":\"101\",\"TEHAMA\":\"103\",\"TRINITY\":\"105\",\"TULARE\":\"107\",\n",
    "    \"TUOLUMNE\":\"109\",\"VENTURA\":\"111\",\"YOLO\":\"113\",\"YUBA\":\"115\"\n",
    "}\n",
    "FIPS_TO_COUNTY = {v: k.title() for k, v in CA_COUNTY_TO_FIPS.items()}\n",
    "\n",
    "# --- Data ---\n",
    "df = g[[\"grid_id\",\"SVI\",\"EEI\",\"geometry\"]].copy()\n",
    "svi_thr, eei_thr = df[\"SVI\"].quantile(OUTLIER_PCT), df[\"EEI\"].quantile(OUTLIER_PCT)\n",
    "outliers = df[(df[\"SVI\"] >= svi_thr) & (df[\"EEI\"] >= eei_thr)].copy()\n",
    "outliers[\"cell_centroid\"] = outliers.geometry.centroid\n",
    "\n",
    "print(f\"Top 5% thresholds  SVI ≥ {svi_thr:.2f},  EEI ≥ {eei_thr:.2f}\")\n",
    "print(f\"Total outlier cells: {len(outliers)}\")\n",
    "\n",
    "# --- ACS join & county derivation ---\n",
    "acs = gpd.read_parquet(ACS_PATH).to_crs(df.crs)\n",
    "acs = acs[[\"GEOID\",\"geometry\"]].copy()\n",
    "acs[\"county_fips3\"] = acs[\"GEOID\"].astype(str).str.slice(2,5)\n",
    "acs[\"county\"] = acs[\"county_fips3\"].map(FIPS_TO_COUNTY).fillna(\"Unknown\")\n",
    "\n",
    "cand = gpd.sjoin(\n",
    "    acs,\n",
    "    outliers[[\"grid_id\",\"geometry\"]],\n",
    "    how=\"inner\",\n",
    "    predicate=\"within\"\n",
    ").rename(columns={\"index_right\":\"_grid_idx\"})\n",
    "\n",
    "if not cand.empty:\n",
    "    cell_cents = outliers[[\"grid_id\",\"cell_centroid\"]].set_index(\"grid_id\")\n",
    "    cand = cand.join(cell_cents, on=\"grid_id\")\n",
    "    cand[\"dist_to_cell_centroid\"] = cand.geometry.distance(cand[\"cell_centroid\"])\n",
    "    idx = cand.groupby(\"grid_id\")[\"dist_to_cell_centroid\"].idxmin()\n",
    "    nearest = cand.loc[idx, [\"grid_id\",\"GEOID\",\"county\"]].reset_index(drop=True)\n",
    "else:\n",
    "    nearest = pd.DataFrame(columns=[\"grid_id\",\"GEOID\",\"county\"])\n",
    "\n",
    "final = outliers.merge(nearest, on=\"grid_id\", how=\"left\").sort_values([\"SVI\",\"EEI\"], ascending=False)\n",
    "cols = [\"grid_id\",\"GEOID\",\"county\",\"SVI\",\"EEI\"]\n",
    "print(\"\\nTop outlier cells (showing up to 20):\")\n",
    "print(final[cols].head(20).to_string(index=False))\n",
    "\n",
    "# --- Map ---\n",
    "base = gpd.GeoDataFrame(geometry=g.geometry, crs=g.crs)\n",
    "ax = base.plot(color=\"#f0f0f0\", figsize=(8,8))\n",
    "final.plot(ax=ax, color=\"red\", markersize=7)\n",
    "ax.set_axis_off()\n",
    "ax.set_title(\"Top 5% Double-Burden Hotspots Across California\", fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33a7da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === HeatShield | Temporal Hazard Burden by Vulnerability (Top vs Bottom SVI Decile) ===\n",
    "import os, glob, pandas as pd, numpy as np, matplotlib.pyplot as plt\n",
    "\n",
    "RES_DIR = \"results\"\n",
    "EQUITY_PQ = os.path.join(RES_DIR, \"equity_double_burden_2024Q3_CA.parquet\")\n",
    "\n",
    "# Find final-daily parquet (grid × day)\n",
    "def _find_final_parquet():\n",
    "    cands = []\n",
    "    if \"CONFIG\" in globals() and isinstance(CONFIG, dict) and \"out_dir\" in CONFIG:\n",
    "        cands += glob.glob(os.path.join(CONFIG[\"out_dir\"], \"final_daily_grid_*_20240601_20241031.parquet\"))\n",
    "        cands += glob.glob(os.path.join(CONFIG[\"out_dir\"], \"final_daily_grid_*_2024*.parquet\"))\n",
    "    cands += glob.glob(os.path.join(RES_DIR, \"final_daily_grid_*_20240601_20241031.parquet\"))\n",
    "    cands += glob.glob(os.path.join(RES_DIR, \"final_daily_grid_*_2024*.parquet\"))\n",
    "    if not cands:\n",
    "        raise FileNotFoundError(\"final_daily_grid_*_2024*.parquet not found in results/ or CONFIG['out_dir'].\")\n",
    "    return max(cands, key=os.path.getmtime)\n",
    "\n",
    "FINAL_DAILY = _find_final_parquet()\n",
    "print(\"Using final daily:\", FINAL_DAILY)\n",
    "\n",
    "# Load SVI per grid\n",
    "eq = pd.read_parquet(EQUITY_PQ)[[\"grid_id\",\"SVI\"]].dropna()\n",
    "# Top / bottom deciles by SVI\n",
    "q_lo, q_hi = eq[\"SVI\"].quantile(0.10), eq[\"SVI\"].quantile(0.90)\n",
    "low_ids  = set(eq.loc[eq[\"SVI\"] <= q_lo, \"grid_id\"])\n",
    "high_ids = set(eq.loc[eq[\"SVI\"] >= q_hi, \"grid_id\"])\n",
    "print(f\"Low SVI cells: {len(low_ids)} | High SVI cells: {len(high_ids)}\")\n",
    "\n",
    "# Load daily env\n",
    "env = pd.read_parquet(FINAL_DAILY)\n",
    "env[\"date\"] = pd.to_datetime(env[\"date\"], errors=\"coerce\").dt.date.astype(str)\n",
    "env = env[(env[\"date\"] >= \"2024-06-01\") & (env[\"date\"] <= \"2024-10-31\")]\n",
    "\n",
    "# Pick robust column names\n",
    "def has(c): return c in env.columns\n",
    "pm_col   = \"pm25\" if has(\"pm25\") else (\"pm25_aqs\" if has(\"pm25_aqs\") else (\"pm25_airnow\" if has(\"pm25_airnow\") else None))\n",
    "tmax_col = \"tmax_c_value\" if has(\"tmax_c_value\") else (\"tavg_c_value\" if has(\"tavg_c_value\") else None)\n",
    "if pm_col is None or tmax_col is None:\n",
    "    raise ValueError(f\"Missing required columns. Found PM='{pm_col}', T='{tmax_col}' in {FINAL_DAILY}\")\n",
    "\n",
    "# Split high/low SVI and compute daily means\n",
    "def _daily_mean(df, col, ids):\n",
    "    sub = df[df[\"grid_id\"].isin(ids)].copy()\n",
    "    return sub.groupby(\"date\", as_index=False)[col].mean().rename(columns={col: f\"{col}_mean\"})\n",
    "\n",
    "pm_hi  = _daily_mean(env[[\"grid_id\",\"date\",pm_col]], pm_col, high_ids)\n",
    "pm_lo  = _daily_mean(env[[\"grid_id\",\"date\",pm_col]], pm_col, low_ids)\n",
    "tx_hi  = _daily_mean(env[[\"grid_id\",\"date\",tmax_col]], tmax_col, high_ids)\n",
    "tx_lo  = _daily_mean(env[[\"grid_id\",\"date\",tmax_col]], tmax_col, low_ids)\n",
    "\n",
    "# Merge to one frame\n",
    "ts = pd.DataFrame({\"date\": sorted(env[\"date\"].unique())})\n",
    "ts = ts.merge(pm_hi, on=\"date\", how=\"left\").merge(pm_lo, on=\"date\", how=\"left\", suffixes=(\"_hi\",\"_lo\"))\n",
    "ts = ts.merge(tx_hi, on=\"date\", how=\"left\").merge(tx_lo, on=\"date\", how=\"left\", suffixes=(\"_hi\",\"_lo\"))\n",
    "\n",
    "# Diagnostics: count days high>low\n",
    "pm_hi_col, pm_lo_col = f\"{pm_col}_mean_hi\", f\"{pm_col}_mean_lo\"\n",
    "tx_hi_col, tx_lo_col = f\"{tmax_col}_mean_hi\", f\"{tmax_col}_mean_lo\"\n",
    "n_days = len(ts)\n",
    "pm_bias_days = int((ts[pm_hi_col] > ts[pm_lo_col]).sum())\n",
    "tx_bias_days = int((ts[tx_hi_col] > ts[tx_lo_col]).sum())\n",
    "print(f\"Days high-SVI > low-SVI: PM2.5 {pm_bias_days}/{n_days}, Tmax {tx_bias_days}/{n_days}\")\n",
    "\n",
    "# Plot\n",
    "dates = pd.to_datetime(ts[\"date\"])\n",
    "fig, axes = plt.subplots(2, 1, figsize=(10, 6), sharex=True)\n",
    "\n",
    "axes[0].plot(dates, ts[pm_hi_col], label=\"High-SVI (top 10%)\")\n",
    "axes[0].plot(dates, ts[pm_lo_col], label=\"Low-SVI (bottom 10%)\")\n",
    "axes[0].set_ylabel(\"PM2.5 (µg/m³)\")\n",
    "axes[0].set_title(\"Daily PM2.5: High-SVI vs Low-SVI (Jun–Oct 2024)\")\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(dates, ts[tx_hi_col], label=\"High-SVI (top 10%)\")\n",
    "axes[1].plot(dates, ts[tx_lo_col], label=\"Low-SVI (bottom 10%)\")\n",
    "axes[1].set_ylabel(\"Tmax (°C)\")\n",
    "axes[1].set_title(\"Daily Tmax: High-SVI vs Low-SVI (Jun–Oct 2024)\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.xlabel(\"Date\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c801a652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === HeatShield | Multi-factor Hotspot Clustering (PM2.5, Smoke, Tmax, SVI) ===\n",
    "import os, glob, numpy as np, pandas as pd, geopandas as gpd, matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "RES_DIR = \"results\"\n",
    "GRID_PQ   = os.path.join(RES_DIR, \"grid_3000m_CA_epsg3310.parquet\")\n",
    "EQUITY_PQ = os.path.join(RES_DIR, \"equity_double_burden_2024Q3_CA.parquet\")\n",
    "\n",
    "def _find_final_parquet():\n",
    "    cands = []\n",
    "    if \"CONFIG\" in globals() and isinstance(CONFIG, dict) and \"out_dir\" in CONFIG:\n",
    "        cands += glob.glob(os.path.join(CONFIG[\"out_dir\"], \"final_daily_grid_*_20240601_20241031.parquet\"))\n",
    "        cands += glob.glob(os.path.join(CONFIG[\"out_dir\"], \"final_daily_grid_*_2024*.parquet\"))\n",
    "    cands += glob.glob(os.path.join(RES_DIR, \"final_daily_grid_*_20240601_20241031.parquet\"))\n",
    "    cands += glob.glob(os.path.join(RES_DIR, \"final_daily_grid_*_2024*.parquet\"))\n",
    "    if not cands:\n",
    "        raise FileNotFoundError(\"final_daily_grid_*_2024*.parquet not found.\")\n",
    "    return max(cands, key=os.path.getmtime)\n",
    "\n",
    "FINAL_DAILY = _find_final_parquet()\n",
    "print(\"Using final daily:\", FINAL_DAILY)\n",
    "\n",
    "# Load grid geometry and SVI\n",
    "grid = gpd.read_parquet(GRID_PQ)[[\"grid_id\",\"geometry\"]]\n",
    "eq   = pd.read_parquet(EQUITY_PQ)[[\"grid_id\",\"SVI\"]]\n",
    "\n",
    "# Load daily env and build per-grid features\n",
    "env = pd.read_parquet(FINAL_DAILY)\n",
    "env[\"date\"] = pd.to_datetime(env[\"date\"], errors=\"coerce\").dt.date.astype(str)\n",
    "env = env[(env[\"date\"] >= \"2024-06-01\") & (env[\"date\"] <= \"2024-10-31\")]\n",
    "\n",
    "def has(c): return c in env.columns\n",
    "pm_col   = \"pm25\" if has(\"pm25\") else (\"pm25_aqs\" if has(\"pm25_aqs\") else (\"pm25_airnow\" if has(\"pm25_airnow\") else None))\n",
    "tmax_col = \"tmax_c_value\" if has(\"tmax_c_value\") else (\"tavg_c_value\" if has(\"tavg_c_value\") else None)\n",
    "if pm_col is None or tmax_col is None:\n",
    "    raise ValueError(f\"Missing required columns. Found PM='{pm_col}', T='{tmax_col}' in {FINAL_DAILY}\")\n",
    "\n",
    "# smoke_days per grid (days with any smoke overlap)\n",
    "smoke_days = None\n",
    "if \"smoke_frac\" in env.columns:\n",
    "    tmp = env[[\"grid_id\",\"smoke_frac\"]].copy()\n",
    "    tmp[\"_smoke_day\"] = (tmp[\"smoke_frac\"].fillna(0) > 0).astype(int)\n",
    "    smoke_days = tmp.groupby(\"grid_id\", as_index=False)[\"_smoke_day\"].sum().rename(columns={\"_smoke_day\":\"smoke_days\"})\n",
    "\n",
    "# Aggregate features per grid\n",
    "agg = env.groupby(\"grid_id\", as_index=False).agg({\n",
    "    pm_col: \"mean\",\n",
    "    tmax_col: \"mean\"\n",
    "}).rename(columns={pm_col:\"pm25_mean\", tmax_col:\"tmax_mean\"})\n",
    "\n",
    "if smoke_days is not None:\n",
    "    agg = agg.merge(smoke_days, on=\"grid_id\", how=\"left\")\n",
    "else:\n",
    "    agg[\"smoke_days\"] = np.nan  # will be ignored in scaler (after fillna)\n",
    "\n",
    "# Merge SVI\n",
    "feat = agg.merge(eq, on=\"grid_id\", how=\"left\")\n",
    "\n",
    "# Prepare feature matrix: normalize (z-score)\n",
    "X = feat[[\"pm25_mean\",\"smoke_days\",\"tmax_mean\",\"SVI\"]].copy()\n",
    "X = X.astype(float).fillna(X.mean(numeric_only=True))\n",
    "scaler = StandardScaler()\n",
    "Xz = scaler.fit_transform(X.values)\n",
    "\n",
    "# K-means clustering (k=4 is a good first pass)\n",
    "k = 4\n",
    "km = KMeans(n_clusters=k, n_init=\"auto\", random_state=42)\n",
    "labels = km.fit_predict(Xz)\n",
    "feat[\"cluster\"] = labels\n",
    "\n",
    "# Name clusters by their center profiles (rank the centers)\n",
    "centers = pd.DataFrame(scaler.inverse_transform(km.cluster_centers_), columns=X.columns)\n",
    "ranked = centers.rank(axis=0, method=\"dense\")\n",
    "names = []\n",
    "for i in range(k):\n",
    "    tags = []\n",
    "    if ranked.loc[i, \"tmax_mean\"] >= ranked[\"tmax_mean\"].median(): tags.append(\"High heat\")\n",
    "    if ranked.loc[i, \"pm25_mean\"] >= ranked[\"pm25_mean\"].median(): tags.append(\"High PM2.5\")\n",
    "    if ranked.loc[i, \"smoke_days\"] >= ranked[\"smoke_days\"].median(): tags.append(\"Frequent smoke\")\n",
    "    if ranked.loc[i, \"SVI\"] >= ranked[\"SVI\"].median(): tags.append(\"High vulnerability\")\n",
    "    if not tags: tags = [\"Low risk\"]\n",
    "    names.append(\" + \".join(tags))\n",
    "feat[\"cluster_name\"] = [names[i] for i in feat[\"cluster\"]]\n",
    "\n",
    "# Join geometry & plot\n",
    "gclust = g.merge(feat[[\"grid_id\",\"cluster\",\"cluster_name\"]], on=\"grid_id\", how=\"left\")\n",
    "palette = {\n",
    "    0: \"#1b9e77\", 1: \"#d95f02\", 2: \"#7570b3\", 3: \"#e7298a\",\n",
    "    4: \"#66a61e\", 5: \"#e6ab02\", 6: \"#a6761d\", 7: \"#666666\"\n",
    "}\n",
    "# --- Map with explicit legend for cluster types ---\n",
    "fig, ax = plt.subplots(1, 1, figsize=(9, 9), dpi=130)\n",
    "gclust.plot(ax=ax, color=\"#f0f0f0\", linewidth=0, alpha=0.8)\n",
    "\n",
    "# Draw clusters and collect handles\n",
    "handles = []\n",
    "labels_used = set()\n",
    "for cid in sorted(gclust[\"cluster\"].dropna().unique()):\n",
    "    sub = gclust[gclust[\"cluster\"] == cid]\n",
    "    color = palette.get(int(cid), \"#999999\")\n",
    "    label = names[int(cid)]\n",
    "    sub.plot(ax=ax, color=color, linewidth=0, label=label)\n",
    "    # Create one handle per unique label\n",
    "    if label not in labels_used:\n",
    "        h = plt.Line2D([0], [0], marker=\"s\", linestyle=\"None\",\n",
    "                       markersize=8, markerfacecolor=color,\n",
    "                       markeredgecolor=\"none\", label=label)\n",
    "        handles.append(h)\n",
    "        labels_used.add(label)\n",
    "\n",
    "ax.set_axis_off()\n",
    "ax.set_title(\"Multi-factor Hotspot Clusters (PM₂.₅, Smoke, Tmax, SVI)\", fontsize=13)\n",
    "\n",
    "# Proper legend anchored outside the map\n",
    "ax.legend(\n",
    "    handles=handles,\n",
    "    title=\"Cluster Types\",\n",
    "    loc=\"upper right\",\n",
    "    bbox_to_anchor=(1.25, 1),\n",
    "    frameon=True,\n",
    "    fontsize=8,\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summaries\n",
    "print(\"\\nCluster size and mean profiles:\")\n",
    "summary = feat.groupby([\"cluster\",\"cluster_name\"], as_index=False)[[\"pm25_mean\",\"smoke_days\",\"tmax_mean\",\"SVI\"]].mean()\n",
    "counts = feat[\"cluster\"].value_counts().rename_axis(\"cluster\").reset_index(name=\"n_cells\")\n",
    "print(counts.merge(summary, on=\"cluster\").sort_values(\"n_cells\", ascending=False).to_string(index=False))\n",
    "\n",
    "# Save labeled table for downstream\n",
    "OUT_CLUSTERS = os.path.join(RES_DIR, \"grid_hotspot_clusters_2024Q3_CA.parquet\")\n",
    "feat.to_parquet(OUT_CLUSTERS, index=False)\n",
    "print(\"\\nSaved:\", OUT_CLUSTERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0ab884",
   "metadata": {},
   "source": [
    "DEBUG CODE BELOW THIS..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c42bd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DEBUG: \n",
    "# === SUPERSET BUILDER — add GHCND vs USCRN (outer-join, robust nearest, no cutoffs) ===\n",
    "import os, glob, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from functools import reduce\n",
    "from datetime import datetime as dt, timezone\n",
    "import rasterio\n",
    "\n",
    "BASE = CONFIG[\"out_dir\"]\n",
    "os.makedirs(BASE, exist_ok=True)\n",
    "\n",
    "def say(*a): print(*a)\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def safe_read(path):\n",
    "    if not path or not os.path.exists(path): return None\n",
    "    try:\n",
    "        return pd.read_parquet(path) if path.lower().endswith(\".parquet\") else pd.read_csv(path, low_memory=False)\n",
    "    except Exception as e:\n",
    "        say(\"  ! read failed:\", path, \"->\", e); return None\n",
    "\n",
    "def find_many(patterns):\n",
    "    out=[]\n",
    "    for pat in patterns:\n",
    "        out.extend(sorted(glob.glob(os.path.join(BASE, pat))))\n",
    "    return sorted(set(out))\n",
    "\n",
    "def ci_lookup(cols, *cands, regex=None):\n",
    "    \"\"\"Case-insensitive lookup: return first matching column name.\"\"\"\n",
    "    cl = {c.lower(): c for c in cols}\n",
    "    for cand in cands:\n",
    "        if cand is None: continue\n",
    "        if cand.lower() in cl: return cl[cand.lower()]\n",
    "    if regex:\n",
    "        r = re.compile(regex, flags=re.I)\n",
    "        for c in cols:\n",
    "            if r.search(c): return c\n",
    "    return None\n",
    "\n",
    "def load_latest_grid():\n",
    "    paths = sorted(glob.glob(os.path.join(BASE, \"grid_*m.parquet\")))\n",
    "    if not paths: raise FileNotFoundError(\"No grid_*.parquet found in results/.\")\n",
    "    g = gpd.read_parquet(paths[-1])\n",
    "    if g.crs is None:\n",
    "        g = g.set_crs(f\"EPSG:{CONFIG.get('crs_epsg',4326)}\").to_crs(4326)\n",
    "    elif g.crs.to_epsg()!=4326:\n",
    "        g = g.to_crs(4326)\n",
    "    g[\"centroid\"] = g.geometry.centroid\n",
    "    return g[[\"geounit_id\",\"geometry\",\"centroid\"]].copy()\n",
    "\n",
    "def sjoin_nearest_no_cutoff(points_df, grid):\n",
    "    \"\"\"Always take nearest geounit centroid; keep distance_km for QA (no max distance filter).\"\"\"\n",
    "    pts = points_df if isinstance(points_df, gpd.GeoDataFrame) else gpd.GeoDataFrame(points_df, geometry=\"geometry\", crs=4326)\n",
    "    if pts.crs is None or pts.crs.to_epsg()!=4326: pts = pts.set_crs(4326)\n",
    "    grid_cent = gpd.GeoDataFrame(grid[[\"geounit_id\"]].copy(), geometry=grid[\"centroid\"], crs=4326)\n",
    "    j = gpd.sjoin_nearest(pts, grid_cent, how=\"left\", distance_col=\"dist_m\")\n",
    "    j[\"distance_km\"] = j[\"dist_m\"].astype(float)/1000.0\n",
    "    return j.drop(columns=[\"index_right\"])\n",
    "\n",
    "def sample_ndvi_tiles_to_grid(ndvi_glob, grid):\n",
    "    tiles = sorted(glob.glob(ndvi_glob))\n",
    "    if not tiles: return pd.DataFrame({\"geounit_id\":grid[\"geounit_id\"], \"ndvi\":np.nan})\n",
    "    vals = np.zeros(len(grid), dtype=float); hits = np.zeros(len(grid), dtype=int)\n",
    "    with_ndvi = grid.copy()\n",
    "    with_ndvi[\"centroid\"] = with_ndvi.geometry.centroid\n",
    "    for tif in tiles:\n",
    "        with rasterio.open(tif) as src:\n",
    "            pts = with_ndvi.to_crs(src.crs)\n",
    "            coords = [(p.x, p.y) for p in pts[\"centroid\"]]\n",
    "            arr = np.array([v[0] if v is not None else np.nan for v in src.sample(coords)], dtype=float)\n",
    "            m = np.isfinite(arr); vals[m]+=arr[m]; hits[m]+=1\n",
    "    ndvi = np.where(hits>0, vals/hits, np.nan)\n",
    "    return pd.DataFrame({\"geounit_id\":grid[\"geounit_id\"], \"ndvi\":ndvi})\n",
    "\n",
    "# ---------- presence check ----------\n",
    "say(\"=== File presence check in\", BASE, \"===\")\n",
    "check = {\n",
    "  \"GRID\":            find_many([\"grid_*m.parquet\"]),\n",
    "  \"NDVI tiles\":      find_many([\"ndvi_ca_*_tile*.tif\"]),\n",
    "  \"NLCD lookup\":     [os.path.join(BASE,\"lookups\",\"nlcd_grid.parquet\")] if os.path.exists(os.path.join(BASE,\"lookups\",\"nlcd_grid.parquet\")) else [],\n",
    "  \"WHP lookup\":      [os.path.join(BASE,\"lookups\",\"whp_grid.parquet\")]  if os.path.exists(os.path.join(BASE,\"lookups\",\"whp_grid.parquet\"))  else [],\n",
    "  \"USCRN hourly\":    find_many([\"uscrn*_hourly*clean*.parquet\",\"uscrn*_hourly*.parquet\"]),\n",
    "  \"AQS daily\":       find_many([\"aqs_pm25*clean*.parquet\",\"aqs_pm25*daily*.parquet\",\"aqs_pm25_*_CA.parquet\"]),\n",
    "  \"CDO daily\":       find_many([\"ghcnd*_daily*clean*.parquet\",\"ghcnd*_daily*.parquet\",\"ghcnd*_daily*clean*.csv\",\"ghcnd*_daily*.csv\",\"ghcnd_daily_cleaned.csv\",\"ghcnd_daily_raw_all.csv\"]),\n",
    "  \"HMS smoke\":       find_many([\"hms_clean/smoke/*/*/*.parquet\"]),\n",
    "  \"HMS fire\":        find_many([\"hms_clean/fire/*/*/*.parquet\"]),\n",
    "}\n",
    "for k,v in check.items():\n",
    "    say(f\" - {k}: {'OK ('+str(len(v))+' file(s))' if v else 'MISSING'}\")\n",
    "\n",
    "# ---------- statics ----------\n",
    "grid = load_latest_grid()\n",
    "static_df = pd.DataFrame({\"geounit_id\":grid[\"geounit_id\"]})\n",
    "\n",
    "nlcd_lookup = safe_read(os.path.join(BASE, \"lookups\", \"nlcd_grid.parquet\"))\n",
    "if nlcd_lookup is not None:\n",
    "    static_df = static_df.merge(nlcd_lookup, on=\"geounit_id\", how=\"left\")\n",
    "\n",
    "whp_lookup = safe_read(os.path.join(BASE, \"lookups\", \"whp_grid.parquet\"))\n",
    "if whp_lookup is not None:\n",
    "    static_df = static_df.merge(whp_lookup, on=\"geounit_id\", how=\"left\")\n",
    "\n",
    "ndvi_df = sample_ndvi_tiles_to_grid(os.path.join(BASE, \"ndvi_ca_*_tile*.tif\"), grid)\n",
    "static_df = static_df.merge(ndvi_df, on=\"geounit_id\", how=\"left\")\n",
    "\n",
    "# ---------- dynamic daily pieces ----------\n",
    "daily_frames, geo_sets, date_sets = [], [], []\n",
    "\n",
    "# A) GHCND (CDO) DAILY — prefix ghcnd_\n",
    "if check[\"CDO daily\"]:\n",
    "    cdo = pd.concat([df for p in check[\"CDO daily\"] if (df:=safe_read(p)) is not None], ignore_index=True)\n",
    "    say(\"CDO rows:\", len(cdo))\n",
    "    if not cdo.empty:\n",
    "        # if the file is \"raw long\" (datatype/value), pivot it first\n",
    "        if {\"datatype\",\"value\"}.issubset(set(cdo.columns)):\n",
    "            dcol = ci_lookup(cdo.columns, \"date\", regex=r\"\\bdate\\b\")\n",
    "            lat  = ci_lookup(cdo.columns, \"lat\",\"latitude\", regex=r\"^lat\")\n",
    "            lon  = ci_lookup(cdo.columns, \"lon\",\"longitude\", regex=r\"^lon\")\n",
    "            if dcol and lat and lon:\n",
    "                sub = cdo[[dcol,\"datatype\",\"value\",lat,lon]].copy()\n",
    "                sub[dcol] = pd.to_datetime(sub[dcol], utc=True, errors=\"coerce\").dt.normalize()\n",
    "                # capture TMAX/TMIN/PRCP -> C/mm\n",
    "                pivot = (sub.pivot_table(index=[dcol,lat,lon], columns=\"datatype\", values=\"value\", aggfunc=\"mean\")\n",
    "                           .reset_index())\n",
    "                # convert units if raw GHCND units (tenths of °C and mm)\n",
    "                for k in [\"TMAX\",\"TMIN\",\"TAVG\",\"PRCP\"]:\n",
    "                    if k in pivot.columns:\n",
    "                        if k==\"PRCP\":\n",
    "                            pivot[k] = pivot[k].astype(\"float64\")  # mm\n",
    "                        else:\n",
    "                            pivot[k] = pivot[k].astype(\"float64\")/10.0  # °C\n",
    "                pivot.rename(columns={dcol:\"date\"}, inplace=True)\n",
    "                pts = gpd.GeoDataFrame(pivot, geometry=gpd.points_from_xy(pivot[lon], pivot[lat]), crs=4326)\n",
    "                j = sjoin_nearest_no_cutoff(pts, grid)\n",
    "                agg = {}\n",
    "                if \"PRCP\" in j: agg[\"ghcnd_precipitation_mm_daily\"] = (\"PRCP\",\"mean\")\n",
    "                if \"TMAX\" in j: agg[\"ghcnd_temperature_max_c_daily\"] = (\"TMAX\",\"mean\")\n",
    "                if \"TMIN\" in j: agg[\"ghcnd_temperature_min_c_daily\"] = (\"TMIN\",\"mean\")\n",
    "                dly = (j.groupby([\"geounit_id\",\"date\"], as_index=False).agg(**agg))\n",
    "            else:\n",
    "                dly = pd.DataFrame(columns=[\"geounit_id\",\"date\"])\n",
    "        else:\n",
    "            # already wide/clean\n",
    "            dcol = ci_lookup(cdo.columns, \"date\", regex=r\"\\bdate\\b\")\n",
    "            lat  = ci_lookup(cdo.columns, \"lat\",\"latitude\", regex=r\"^lat\")\n",
    "            lon  = ci_lookup(cdo.columns, \"lon\",\"longitude\", regex=r\"^lon\")\n",
    "            tmax = ci_lookup(cdo.columns, \"temperature_max_c\",\"tmax_c\",\"tmax\", regex=r\"tmax.*c$\")\n",
    "            tmin = ci_lookup(cdo.columns, \"temperature_min_c\",\"tmin_c\",\"tmin\", regex=r\"tmin.*c$\")\n",
    "            prcp = ci_lookup(cdo.columns, \"precipitation_mm\",\"precip_mm\",\"prcp_mm\",\"prcp\", regex=r\"precip.*mm$\")\n",
    "            if dcol and lat and lon:\n",
    "                cdo[dcol] = pd.to_datetime(cdo[dcol], utc=True, errors=\"coerce\").dt.normalize()\n",
    "                pts = gpd.GeoDataFrame(cdo, geometry=gpd.points_from_xy(cdo[lon], cdo[lat]), crs=4326)\n",
    "                j = sjoin_nearest_no_cutoff(pts, grid)\n",
    "                agg = {}\n",
    "                if prcp: agg[\"ghcnd_precipitation_mm_daily\"] = (prcp,\"mean\")\n",
    "                if tmax: agg[\"ghcnd_temperature_max_c_daily\"] = (tmax,\"mean\")\n",
    "                if tmin: agg[\"ghcnd_temperature_min_c_daily\"] = (tmin,\"mean\")\n",
    "                dly = (j.groupby([\"geounit_id\", dcol], as_index=False).agg(**agg).rename(columns={dcol:\"date\"}))\n",
    "            else:\n",
    "                dly = pd.DataFrame(columns=[\"geounit_id\",\"date\"])\n",
    "        daily_frames.append(dly)\n",
    "        geo_sets.append(set(dly[\"geounit_id\"].unique())); date_sets.append(set(dly[\"date\"].unique()))\n",
    "        say(\"GHCND→grid daily rows:\", len(dly))\n",
    "\n",
    "# B) USCRN HOURLY → DAILY — prefix uscrn_\n",
    "if check[\"USCRN hourly\"]:\n",
    "    u = pd.concat([df for p in check[\"USCRN hourly\"] if (df:=safe_read(p)) is not None], ignore_index=True)\n",
    "    say(\"USCRN rows:\", len(u))\n",
    "    if not u.empty:\n",
    "        dtc = ci_lookup(u.columns, \"datetime_utc\",\"UTC_DATETIME\", regex=r\"date.*utc\")\n",
    "        lat = ci_lookup(u.columns, \"latitude\",\"LATITUDE\",\"lat\")\n",
    "        lon = ci_lookup(u.columns, \"longitude\",\"LONGITUDE\",\"lon\")\n",
    "        mx  = ci_lookup(u.columns, \"air_temperature_hourly_max_c\")\n",
    "        mn  = ci_lookup(u.columns, \"air_temperature_hourly_min_c\")\n",
    "        avg = ci_lookup(u.columns, \"air_temperature_hourly_avg_c\")\n",
    "        prh = ci_lookup(u.columns, \"precipitation_mm_hourly\",\"precip_mm_hourly\")\n",
    "        if dtc and lat and lon:\n",
    "            u[\"datetime_utc\"] = pd.to_datetime(u[dtc], utc=True, errors=\"coerce\")\n",
    "            u[\"date\"] = u[\"datetime_utc\"].dt.normalize()\n",
    "            pts = gpd.GeoDataFrame(u, geometry=gpd.points_from_xy(u[lon], u[lat]), crs=4326)\n",
    "            j = sjoin_nearest_no_cutoff(pts, grid)\n",
    "            agg = {}\n",
    "            if mx:  agg[\"uscrn_temperature_max_c_daily\"] = (mx,\"mean\")\n",
    "            if mn:  agg[\"uscrn_temperature_min_c_daily\"] = (mn,\"mean\")\n",
    "            if avg: agg[\"uscrn_temperature_avg_c_daily\"] = (avg,\"mean\")\n",
    "            if prh: agg[\"uscrn_precipitation_mm_daily\"]  = (prh,\"sum\")\n",
    "            ud = (j.groupby([\"geounit_id\",\"date\"], as_index=False).agg(**agg))\n",
    "            daily_frames.append(ud)\n",
    "            geo_sets.append(set(ud[\"geounit_id\"].unique())); date_sets.append(set(ud[\"date\"].unique()))\n",
    "            say(\"USCRN→grid daily rows:\", len(ud))\n",
    "\n",
    "# C) PM2.5 — AQS DAILY (pm25_ugm3_daily)\n",
    "pm25_frame = None\n",
    "if check[\"AQS daily\"]:\n",
    "    aqs = pd.concat([df for p in check[\"AQS daily\"] if (df:=safe_read(p)) is not None], ignore_index=True)\n",
    "    say(\"AQS rows:\", len(aqs))\n",
    "    if not aqs.empty:\n",
    "        dcol = ci_lookup(aqs.columns, \"date\",\"DateLocal\", regex=r\"\\bdate\")\n",
    "        lat  = ci_lookup(aqs.columns, \"latitude\",\"lat\")\n",
    "        lon  = ci_lookup(aqs.columns, \"longitude\",\"lon\")\n",
    "        val  = ci_lookup(aqs.columns, \"pm25_ugm3\",\"ArithmeticMean\",\"arithmetic_mean\",\"sample_measurement\",\"Value\", regex=r\"pm.*2\\.?5|arith\")\n",
    "        if dcol and lat and lon and val:\n",
    "            aqs[\"date\"] = pd.to_datetime(aqs[dcol], utc=True, errors=\"coerce\").dt.normalize()\n",
    "            pts = gpd.GeoDataFrame(aqs, geometry=gpd.points_from_xy(aqs[lon], aqs[lat]), crs=4326)\n",
    "            j = sjoin_nearest_no_cutoff(pts, grid)\n",
    "            pm = (j.rename(columns={val:\"pm25_ugm3\"})\n",
    "                    .groupby([\"geounit_id\",\"date\"], as_index=False)[\"pm25_ugm3\"].mean()\n",
    "                    .rename(columns={\"pm25_ugm3\":\"pm25_ugm3_daily\"}))\n",
    "            daily_frames.append(pm); pm25_frame = pm\n",
    "            geo_sets.append(set(pm[\"geounit_id\"].unique())); date_sets.append(set(pm[\"date\"].unique()))\n",
    "            say(\"AQS→grid daily rows:\", len(pm))\n",
    "\n",
    "# D) HMS SMOKE (smoke_density_max_daily)\n",
    "if check[\"HMS smoke\"]:\n",
    "    smk = pd.concat([pd.read_parquet(p) for p in check[\"HMS smoke\"]], ignore_index=True)\n",
    "    say(\"HMS smoke rows:\", len(smk))\n",
    "    # date (robust)\n",
    "    smk[\"date\"] = pd.NaT\n",
    "    for c in [\"obs_date\",\"start_utc\",\"start\",\"start_time\",\"analysis_time\",\"end_utc\",\"end\"]:\n",
    "        if c in smk.columns:\n",
    "            smk[\"date\"] = pd.to_datetime(smk[c], utc=True, errors=\"coerce\").dt.normalize().fillna(smk[\"date\"])\n",
    "    # centroids\n",
    "    have_xy = {\"centroid_lon\",\"centroid_lat\"}.issubset(smk.columns)\n",
    "    if not have_xy and \"geometry\" in smk.columns:\n",
    "        gtmp = gpd.GeoDataFrame(smk, geometry=gpd.GeoSeries.from_wkt(smk[\"geometry\"]) if smk[\"geometry\"].dtype==object else smk[\"geometry\"], crs=4326)\n",
    "        smk[\"centroid_lon\"] = gtmp.geometry.centroid.x\n",
    "        smk[\"centroid_lat\"] = gtmp.geometry.centroid.y\n",
    "    if {\"centroid_lon\",\"centroid_lat\"}.issubset(smk.columns):\n",
    "        pts = gpd.GeoDataFrame(smk, geometry=gpd.points_from_xy(smk[\"centroid_lon\"], smk[\"centroid_lat\"]), crs=4326)\n",
    "        j = sjoin_nearest_no_cutoff(pts, grid)\n",
    "        dens = ci_lookup(j.columns, \"density\",\"category\")\n",
    "        code = j[dens].astype(str).str.title().map({\"Low\":1,\"Medium\":2,\"Med\":2,\"High\":3}).fillna(0).astype(int) if dens else 0\n",
    "        j[\"smoke_density_code\"] = code\n",
    "        sd = (j.groupby([\"geounit_id\",\"date\"], as_index=False)\n",
    "                .agg(smoke_density_max_daily=(\"smoke_density_code\",\"max\")))\n",
    "        daily_frames.append(sd)\n",
    "        geo_sets.append(set(sd[\"geounit_id\"].unique())); date_sets.append(set(sd[\"date\"].unique()))\n",
    "        say(\"HMS smoke→grid daily rows:\", len(sd))\n",
    "\n",
    "# E) HMS FIRE (fire_nearby_cnt_daily)\n",
    "if check[\"HMS fire\"]:\n",
    "    fir = pd.concat([pd.read_parquet(p) for p in check[\"HMS fire\"]], ignore_index=True)\n",
    "    say(\"HMS fire rows:\", len(fir))\n",
    "    fir[\"date\"] = pd.NaT\n",
    "    for c in [\"obs_date\",\"start_utc\",\"start\",\"start_time\",\"end_utc\",\"end\"]:\n",
    "        if c in fir.columns:\n",
    "            fir[\"date\"] = pd.to_datetime(fir[c], utc=True, errors=\"coerce\").dt.normalize().fillna(fir[\"date\"])\n",
    "    xcol = ci_lookup(fir.columns, \"lon\",\"longitude\",\"x\"); ycol = ci_lookup(fir.columns, \"lat\",\"latitude\",\"y\")\n",
    "    if xcol and ycol:\n",
    "        pts = gpd.GeoDataFrame(fir, geometry=gpd.points_from_xy(fir[xcol], fir[ycol]), crs=4326)\n",
    "        j = sjoin_nearest_no_cutoff(pts, grid)\n",
    "        fd = (j.groupby([\"geounit_id\",\"date\"], as_index=False)\n",
    "                .agg(fire_nearby_cnt_daily=(ycol,\"count\")))\n",
    "        daily_frames.append(fd)\n",
    "        geo_sets.append(set(fd[\"geounit_id\"].unique())); date_sets.append(set(fd[\"date\"].unique()))\n",
    "        say(\"HMS fire→grid daily rows:\", len(fd))\n",
    "\n",
    "# ---------- OUTER join over union(geounit_id) × union(date) ----------\n",
    "if not daily_frames:\n",
    "    say(\"No dynamic daily sources found—building static-only.\")\n",
    "    daily = pd.DataFrame({\"geounit_id\": grid[\"geounit_id\"]})\n",
    "else:\n",
    "    geos_union  = sorted(set().union(*geo_sets))\n",
    "    dates_union = sorted(set().union(*date_sets))\n",
    "    base = pd.MultiIndex.from_product([geos_union, dates_union], names=[\"geounit_id\",\"date\"]).to_frame(index=False)\n",
    "    parts = [base] + daily_frames\n",
    "    daily = reduce(lambda l, r: pd.merge(l, r, on=[\"geounit_id\",\"date\"], how=\"outer\"), parts)\n",
    "\n",
    "# attach statics\n",
    "daily = daily.merge(static_df, on=\"geounit_id\", how=\"left\")\n",
    "\n",
    "# save\n",
    "out_daily = os.path.join(BASE, \"analysis_superset_daily.parquet\")\n",
    "daily.to_parquet(out_daily, index=False)\n",
    "say(f\"Saved daily superset → {out_daily} (rows={len(daily)}, cols={len(daily.columns)})\")\n",
    "\n",
    "# quick peek of columns added\n",
    "added_cols = sorted([c for c in daily.columns if c not in [\"geounit_id\",\"date\",\"ndvi\",\"nlcd_class\",\"whp_score\",\"geometry\",\"centroid\"]])\n",
    "say(\"Dynamic columns:\", added_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e37a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DEBUG: \n",
    "# ---- Deep validation for results/analysis_superset_daily.parquet ----\n",
    "import os, sys, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "PATH = os.path.join(CONFIG[\"out_dir\"], \"analysis_superset_daily.parquet\")\n",
    "assert os.path.exists(PATH), f\"Missing file: {PATH}\"\n",
    "df = pd.read_parquet(PATH)\n",
    "\n",
    "# --- Basic shape/time/keys ---\n",
    "print(\"Rows:\", len(df), \"Cols:\", len(df.columns))\n",
    "if \"date\" not in df.columns: raise RuntimeError(\"Missing 'date' column.\")\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\", utc=True)\n",
    "print(\"Date range:\", df[\"date\"].min(), \"→\", df[\"date\"].max())\n",
    "print(\"Unique geounit_id:\", df[\"geounit_id\"].nunique() if \"geounit_id\" in df.columns else 0)\n",
    "\n",
    "# --- Expected signals (adjust as needed) ---\n",
    "expected = [\n",
    "    \"precipitation_mm_daily\",\"temperature_max_c_daily\",\"temperature_min_c_daily\",\n",
    "    \"pm25_ugm3_daily\",\"smoke_density_max_daily\",\"fire_nearby_cnt_daily\"\n",
    "]\n",
    "static = [\"ndvi\",\"nlcd_class\",\"whp_score\"]\n",
    "\n",
    "present = [c for c in expected+static if c in df.columns]\n",
    "missing = [c for c in expected+static if c not in df.columns]\n",
    "print(\"\\nPresent columns:\", present)\n",
    "print(\"Missing columns:\", missing)\n",
    "\n",
    "# --- Overall null %, non-null counts, unique counts ---\n",
    "cov = []\n",
    "for c in present:\n",
    "    nn = df[c].notna().sum()\n",
    "    cov.append({\n",
    "        \"column\": c,\n",
    "        \"non_null\": int(nn),\n",
    "        \"null_%\": round(100*(1 - nn/len(df)), 2),\n",
    "        \"unique_vals\": int(df[c].nunique(dropna=True))\n",
    "    })\n",
    "cov_df = pd.DataFrame(cov).sort_values([\"null_%\",\"column\"], ascending=[False, True])\n",
    "print(\"\\nCoverage overview (descending null%):\")\n",
    "print(cov_df.to_string(index=False))\n",
    "\n",
    "# --- Per-month coverage by source (share of geounit×day rows in that month with data) ---\n",
    "df[\"month\"] = df[\"date\"].dt.to_period(\"M\").astype(str)\n",
    "def month_cov(col):\n",
    "    if col not in df: return None\n",
    "    g = df.groupby(\"month\")[col].apply(lambda s: round(100*s.notna().mean(),2))\n",
    "    return g.rename(col)\n",
    "\n",
    "monthly_tables = []\n",
    "for col in expected:\n",
    "    g = month_cov(col)\n",
    "    if g is not None: monthly_tables.append(g)\n",
    "\n",
    "if monthly_tables:\n",
    "    monthly_cov = pd.concat(monthly_tables, axis=1).fillna(0.0).sort_index()\n",
    "    print(\"\\nMonthly coverage % (rows with non-null values):\")\n",
    "    print(monthly_cov.to_string())\n",
    "else:\n",
    "    print(\"\\nMonthly coverage %: no expected dynamic columns present.\")\n",
    "\n",
    "# --- Top/bottom geounits by overall data presence (dynamic only) ---\n",
    "dyn_cols = [c for c in expected if c in df.columns]\n",
    "if dyn_cols:\n",
    "    presence = (df[dyn_cols].notna().any(axis=1)).groupby(df[\"geounit_id\"]).mean().rename(\"has_any_dynamic\")\n",
    "    top = presence.sort_values(ascending=False).head(10)\n",
    "    bot = presence.sort_values(ascending=True).head(10)\n",
    "    print(\"\\nTop geounits by dynamic coverage (fraction of days with any dynamic data):\")\n",
    "    print(top.to_frame().to_string())\n",
    "    print(\"\\nBottom geounits by dynamic coverage (fraction of days with any dynamic data):\")\n",
    "    print(bot.to_frame().to_string())\n",
    "else:\n",
    "    print(\"\\nNo dynamic columns to evaluate geounit coverage.\")\n",
    "\n",
    "# --- Date days with best/worst dynamic coverage (state-wide) ---\n",
    "if dyn_cols:\n",
    "    by_date = df.groupby(\"date\")[dyn_cols].apply(lambda x: x.notna().any(axis=1).mean())\n",
    "    best_days = by_date.sort_values(ascending=False).head(10)\n",
    "    worst_days = by_date.sort_values(ascending=True).head(10)\n",
    "    print(\"\\nDates with highest share of geounits having any dynamic data:\")\n",
    "    print(best_days.to_frame(\"share\").to_string())\n",
    "    print(\"\\nDates with lowest share of geounits having any dynamic data:\")\n",
    "    print(worst_days.to_frame(\"share\").to_string())\n",
    "\n",
    "# --- Sanity ranges for key vars (ignore NaNs) ---\n",
    "def print_range(label, series):\n",
    "    s = series.dropna()\n",
    "    if not s.empty:\n",
    "        print(f\"{label} range:\", float(s.min()), \"→\", float(s.max()), f\"(n={len(s)})\")\n",
    "\n",
    "if \"temperature_max_c_daily\" in df: print_range(\"Tmax °C\", df[\"temperature_max_c_daily\"])\n",
    "if \"temperature_min_c_daily\" in df: print_range(\"Tmin °C\", df[\"temperature_min_c_daily\"])\n",
    "if \"precipitation_mm_daily\" in df: print_range(\"Precip mm\", df[\"precipitation_mm_daily\"])\n",
    "if \"pm25_ugm3_daily\" in df: print_range(\"PM2.5 µg/m³\", df[\"pm25_ugm3_daily\"])\n",
    "if \"fire_nearby_cnt_daily\" in df: print_range(\"Fire count\", df[\"fire_nearby_cnt_daily\"])\n",
    "if \"smoke_density_max_daily\" in df: print_range(\"Smoke density code\", df[\"smoke_density_max_daily\"])\n",
    "\n",
    "# --- Quick diagnostics for likely join gaps ---\n",
    "def pct(x): return f\"{100*x:.2f}%\"\n",
    "\n",
    "diag = {}\n",
    "# How many (geounit_id, date) came only from the base skeleton (all dynamic NaN)?\n",
    "if dyn_cols:\n",
    "    only_static = (~df[dyn_cols].notna().any(axis=1)).mean()\n",
    "    diag[\"rows_with_no_dynamic_data\"] = pct(only_static)\n",
    "# Static presence\n",
    "for s in static:\n",
    "    if s in df.columns:\n",
    "        diag[f\"non_null_{s}\"] = pct(df[s].notna().mean())\n",
    "print(\"\\nDiagnostics:\")\n",
    "for k,v in diag.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# --- Sample problem rows: entries with NO dynamic data but present statics (to inspect areas with no coverage) ---\n",
    "if dyn_cols:\n",
    "    mask = ~df[dyn_cols].notna().any(axis=1)\n",
    "    if mask.any():\n",
    "        print(\"\\nSample rows with no dynamic data (showing 10):\")\n",
    "        cols_show = [\"geounit_id\",\"date\"] + [c for c in ([\"nlcd_class\",\"whp_score\",\"ndvi\"] if \"nlcd_class\" in df.columns else [])]\n",
    "        print(df.loc[mask, cols_show].head(10).to_string(index=False))\n",
    "\n",
    "# --- Per-source presence counts (quick) ---\n",
    "def exists_columns(cols):\n",
    "    return [c for c in cols if c in df.columns]\n",
    "\n",
    "print(\"\\nPer-source non-null counts:\")\n",
    "if \"precipitation_mm_daily\" in df or \"temperature_max_c_daily\" in df:\n",
    "    print(\"  CDO daily:\",\n",
    "          {c:int(df[c].notna().sum()) for c in exists_columns([\"precipitation_mm_daily\",\"temperature_max_c_daily\",\"temperature_min_c_daily\"])})\n",
    "if \"pm25_ugm3_daily\" in df:\n",
    "    print(\"  PM (AirNow/AQS):\", {\"pm25_ugm3_daily\": int(df[\"pm25_ugm3_daily\"].notna().sum())})\n",
    "if \"smoke_density_max_daily\" in df:\n",
    "    print(\"  HMS smoke:\", {\"smoke_density_max_daily\": int(df[\"smoke_density_max_daily\"].notna().sum())})\n",
    "if \"fire_nearby_cnt_daily\" in df:\n",
    "    print(\"  HMS fire:\", {\"fire_nearby_cnt_daily\": int(df[\"fire_nearby_cnt_daily\"].notna().sum())})\n",
    "\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62689be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DEBUG: \n",
    "# === DEBUGGING THE JOIN ISSUE ===\n",
    "# Check the CDO data that was loaded and how the spatial join worked\n",
    "\n",
    "print(\"=== Examining CDO temperature data coverage ===\")\n",
    "\n",
    "# Get the CDO data from the superset building process\n",
    "cdo_files = check[\"CDO daily\"]\n",
    "print(f\"CDO files found: {len(cdo_files)}\")\n",
    "for f in cdo_files[:5]:  # Show first 5 files\n",
    "    print(f\"  - {f}\")\n",
    "\n",
    "# Load the same way as in the superset builder\n",
    "if check[\"CDO daily\"]:\n",
    "    cdo = pd.concat([df for p in check[\"CDO daily\"] if (df:=safe_read(p)) is not None], ignore_index=True)\n",
    "    print(f\"\\nLoaded CDO data: {len(cdo)} rows, {len(cdo.columns)} columns\")\n",
    "    print(f\"Columns: {list(cdo.columns)}\")\n",
    "    \n",
    "    # Check for temperature data\n",
    "    dcol = ci_lookup(cdo.columns, \"date\", regex=r\"\\bdate\\b\")\n",
    "    lat  = ci_lookup(cdo.columns, \"lat\",\"latitude\", regex=r\"^lat\")\n",
    "    lon  = ci_lookup(cdo.columns, \"lon\",\"longitude\", regex=r\"^lon\")\n",
    "    tmax = ci_lookup(cdo.columns, \"temperature_max_c\",\"tmax_c\",\"tmax\", regex=r\"tmax.*c$\")\n",
    "    tmin = ci_lookup(cdo.columns, \"temperature_min_c\",\"tmin_c\",\"tmin\", regex=r\"tmin.*c$\")\n",
    "    prcp = ci_lookup(cdo.columns, \"precipitation_mm\",\"precip_mm\",\"prcp_mm\",\"prcp\", regex=r\"precip.*mm$\")\n",
    "    \n",
    "    print(f\"\\nColumn mapping:\")\n",
    "    print(f\"  Date column: {dcol}\")\n",
    "    print(f\"  Latitude column: {lat}\")\n",
    "    print(f\"  Longitude column: {lon}\")\n",
    "    print(f\"  Tmax column: {tmax}\")\n",
    "    print(f\"  Tmin column: {tmin}\")\n",
    "    print(f\"  Precipitation column: {prcp}\")\n",
    "    \n",
    "    if tmax and tmin:\n",
    "        print(f\"\\nTemperature data coverage:\")\n",
    "        print(f\"  Total CDO records: {len(cdo)}\")\n",
    "        print(f\"  Records with tmax: {cdo[tmax].notna().sum()} ({cdo[tmax].notna().mean()*100:.1f}%)\")\n",
    "        print(f\"  Records with tmin: {cdo[tmin].notna().sum()} ({cdo[tmin].notna().mean()*100:.1f}%)\")\n",
    "        \n",
    "        # Geographic extent\n",
    "        if lat and lon:\n",
    "            print(f\"\\nGeographic extent of CDO stations:\")\n",
    "            print(f\"  Latitude: {cdo[lat].min():.3f} to {cdo[lat].max():.3f}\")\n",
    "            print(f\"  Longitude: {cdo[lon].min():.3f} to {cdo[lon].max():.3f}\")\n",
    "            print(f\"  Unique stations: {cdo.get('station', pd.Series()).nunique() if 'station' in cdo.columns else 'unknown'}\")\n",
    "            \n",
    "            # Check date range\n",
    "            if dcol:\n",
    "                cdo_dates = pd.to_datetime(cdo[dcol], utc=True, errors=\"coerce\")\n",
    "                print(f\"  Date range: {cdo_dates.min()} to {cdo_dates.max()}\")\n",
    "                \n",
    "                # Sample some temperature data\n",
    "                temp_mask = cdo[tmax].notna() | cdo[tmin].notna()\n",
    "                if temp_mask.any():\n",
    "                    print(f\"\\nSample temperature records ({temp_mask.sum()} total):\")\n",
    "                    sample_cols = [col for col in ['station', dcol, lat, lon, tmax, tmin] if col and col in cdo.columns]\n",
    "                    print(cdo[temp_mask][sample_cols].head(10))\n",
    "                else:\n",
    "                    print(\"\\nNo temperature data found in CDO records!\")\n",
    "    else:\n",
    "        print(\"\\nTemperature columns not found in CDO data!\")\n",
    "\n",
    "# Check the grid coverage\n",
    "print(f\"\\n=== Grid information ===\")\n",
    "print(f\"Grid size: {len(grid)} geounits\")\n",
    "print(f\"Grid geounit_id range: {grid['geounit_id'].min()} to {grid['geounit_id'].max()}\")\n",
    "\n",
    "# Examine the spatial join results by recreating the process\n",
    "if check[\"CDO daily\"] and tmax and lat and lon and dcol:\n",
    "    print(f\"\\n=== Recreating spatial join process ===\")\n",
    "    \n",
    "    # Recreate the spatial join to see what happens\n",
    "    cdo_clean = cdo.copy()\n",
    "    cdo_clean[dcol] = pd.to_datetime(cdo_clean[dcol], utc=True, errors=\"coerce\").dt.normalize()\n",
    "    \n",
    "    # Remove invalid coordinates\n",
    "    valid_coords = cdo_clean[lat].notna() & cdo_clean[lon].notna()\n",
    "    print(f\"Records with valid coordinates: {valid_coords.sum()} / {len(cdo_clean)}\")\n",
    "    \n",
    "    if valid_coords.any():\n",
    "        cdo_valid = cdo_clean[valid_coords].copy()\n",
    "        \n",
    "        # Create points\n",
    "        pts = gpd.GeoDataFrame(cdo_valid, geometry=gpd.points_from_xy(cdo_valid[lon], cdo_valid[lat]), crs=4326)\n",
    "        print(f\"Created {len(pts)} point geometries\")\n",
    "        \n",
    "        # Perform spatial join  \n",
    "        j = sjoin_nearest_no_cutoff(pts, grid)\n",
    "        print(f\"Spatial join result: {len(j)} records\")\n",
    "        print(f\"Distance statistics: min={j['distance_km'].min():.2f}km, max={j['distance_km'].max():.2f}km, mean={j['distance_km'].mean():.2f}km\")\n",
    "        \n",
    "        # Check how many unique geounits got temperature data\n",
    "        temp_mask = j[tmax].notna() | j[tmin].notna()\n",
    "        unique_geounits_with_temp = j[temp_mask]['geounit_id'].nunique()\n",
    "        print(f\"Unique geounits that received temperature data: {unique_geounits_with_temp} / {len(grid)}\")\n",
    "        print(f\"That's {unique_geounits_with_temp/len(grid)*100:.2f}% spatial coverage\")\n",
    "        \n",
    "        # Show distance distribution for temperature stations\n",
    "        if temp_mask.any():\n",
    "            print(f\"\\nDistance distribution for temperature assignments:\")\n",
    "            temp_distances = j[temp_mask]['distance_km']\n",
    "            print(f\"  Percentiles: 50%={temp_distances.quantile(0.5):.1f}km, 90%={temp_distances.quantile(0.9):.1f}km, 95%={temp_distances.quantile(0.95):.1f}km\")\n",
    "            \n",
    "            # Show some examples of far assignments\n",
    "            far_assignments = j[temp_mask & (j['distance_km'] > 50)][['geounit_id', 'distance_km', lat, lon, tmax, tmin]].head()\n",
    "            if len(far_assignments) > 0:\n",
    "                print(f\"\\nSample far temperature assignments (>50km):\")\n",
    "                print(far_assignments)\n",
    "        \n",
    "        # Check aggregation step\n",
    "        print(f\"\\n=== Checking aggregation ===\")\n",
    "        agg = {}\n",
    "        if prcp: agg[\"precipitation_mm_daily\"] = (prcp,\"mean\")\n",
    "        if tmax: agg[\"temperature_max_c_daily\"] = (tmax,\"mean\")\n",
    "        if tmin: agg[\"temperature_min_c_daily\"] = (tmin,\"mean\")\n",
    "        \n",
    "        dly = (j.groupby([\"geounit_id\", dcol], as_index=False)\n",
    "                 .agg(**agg)\n",
    "                 .rename(columns={dcol:\"date\"}))\n",
    "        \n",
    "        print(f\"After aggregation: {len(dly)} geounit-date records\")\n",
    "        print(f\"Unique geounits: {dly['geounit_id'].nunique()}\")\n",
    "        print(f\"Date range: {dly['date'].min()} to {dly['date'].max()}\")\n",
    "        \n",
    "        if tmax in agg:\n",
    "            temp_records = dly[\"temperature_max_c_daily\"].notna().sum()\n",
    "            print(f\"Records with aggregated temperature: {temp_records} / {len(dly)} ({temp_records/len(dly)*100:.1f}%)\")\n",
    "    else:\n",
    "        print(\"No valid coordinates found in CDO data!\")\n",
    "else:\n",
    "    print(\"Cannot recreate spatial join - missing required columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3226b2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DEBUG: \n",
    "# === SOLUTION: DISTANCE-BASED TEMPERATURE INTERPOLATION ===\n",
    "\n",
    "def interpolate_temperature_to_grid(cdo_data, grid, max_distance_km=50, dcol='date', lat='lat', lon='lon', \n",
    "                                    tmax='temperature_max_c', tmin='temperature_min_c'):\n",
    "    \"\"\"\n",
    "    Interpolate temperature data to all grid cells using inverse distance weighting.\n",
    "    Each grid cell gets temperature from the nearest stations within max_distance_km.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from scipy.spatial.distance import cdist\n",
    "    from geopy.distance import geodesic\n",
    "    \n",
    "    print(f\"Starting temperature interpolation with max distance: {max_distance_km}km\")\n",
    "    \n",
    "    # Prepare temperature data\n",
    "    temp_data = cdo_data[cdo_data[tmax].notna() | cdo_data[tmin].notna()].copy()\n",
    "    temp_data[dcol] = pd.to_datetime(temp_data[dcol], utc=True, errors=\"coerce\").dt.normalize()\n",
    "    \n",
    "    # Get unique dates\n",
    "    dates = sorted(temp_data[dcol].dropna().unique())\n",
    "    print(f\"Processing {len(dates)} dates\")\n",
    "    \n",
    "    # Grid centroids\n",
    "    grid_coords = [(row.centroid.y, row.centroid.x) for _, row in grid.iterrows()]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for date in dates[:5]:  # Process first 5 dates as example\n",
    "        print(f\"Processing {date.date()}...\")\n",
    "        \n",
    "        # Get temperature stations for this date\n",
    "        daily_temps = temp_data[temp_data[dcol] == date].copy()\n",
    "        if len(daily_temps) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Station coordinates\n",
    "        station_coords = [(row[lat], row[lon]) for _, row in daily_temps.iterrows()]\n",
    "        \n",
    "        # Calculate distances from each grid cell to each station (vectorized)\n",
    "        distances = np.array([[geodesic(gc, sc).kilometers for sc in station_coords] for gc in grid_coords])\n",
    "        \n",
    "        # For each grid cell, find stations within max_distance\n",
    "        for i, geounit_id in enumerate(grid['geounit_id']):\n",
    "            station_distances = distances[i]\n",
    "            nearby_mask = station_distances <= max_distance_km\n",
    "            \n",
    "            if nearby_mask.any():\n",
    "                nearby_stations = daily_temps.iloc[nearby_mask]\n",
    "                nearby_distances = station_distances[nearby_mask]\n",
    "                \n",
    "                # Inverse distance weighting (avoid division by zero)\n",
    "                weights = 1.0 / np.maximum(nearby_distances, 0.001)  # minimum 1m distance\n",
    "                weights = weights / weights.sum()\n",
    "                \n",
    "                # Weighted average temperatures\n",
    "                tmax_vals = nearby_stations[tmax].dropna()\n",
    "                tmin_vals = nearby_stations[tmin].dropna()\n",
    "                \n",
    "                tmax_interp = None\n",
    "                tmin_interp = None\n",
    "                \n",
    "                if len(tmax_vals) > 0:\n",
    "                    tmax_weights = weights[nearby_stations[tmax].notna()]\n",
    "                    if len(tmax_weights) > 0:\n",
    "                        tmax_interp = np.average(tmax_vals, weights=tmax_weights[:len(tmax_vals)])\n",
    "                \n",
    "                if len(tmin_vals) > 0:\n",
    "                    tmin_weights = weights[nearby_stations[tmin].notna()]\n",
    "                    if len(tmin_weights) > 0:\n",
    "                        tmin_interp = np.average(tmin_vals, weights=tmin_weights[:len(tmin_vals)])\n",
    "                \n",
    "                if tmax_interp is not None or tmin_interp is not None:\n",
    "                    results.append({\n",
    "                        'geounit_id': geounit_id,\n",
    "                        'date': date,\n",
    "                        'temperature_max_c_daily': tmax_interp,\n",
    "                        'temperature_min_c_daily': tmin_interp,\n",
    "                        'nearest_station_km': nearby_distances.min(),\n",
    "                        'stations_used': len(nearby_stations)\n",
    "                    })\n",
    "        \n",
    "    interpolated_df = pd.DataFrame(results)\n",
    "    print(f\"Interpolation complete: {len(interpolated_df)} geounit-date records\")\n",
    "    \n",
    "    if len(interpolated_df) > 0:\n",
    "        coverage = interpolated_df['geounit_id'].nunique() / len(grid) * 100\n",
    "        print(f\"Spatial coverage: {interpolated_df['geounit_id'].nunique()} / {len(grid)} geounits ({coverage:.1f}%)\")\n",
    "        print(f\"Distance stats: min={interpolated_df['nearest_station_km'].min():.1f}km, \"\n",
    "              f\"max={interpolated_df['nearest_station_km'].max():.1f}km, \"\n",
    "              f\"median={interpolated_df['nearest_station_km'].median():.1f}km\")\n",
    "    \n",
    "    return interpolated_df\n",
    "\n",
    "# Let's test this approach with the first few thousand grid cells as a demo\n",
    "print(\"=== Testing improved temperature interpolation ===\")\n",
    "\n",
    "# Use a subset of the grid for testing (first 1000 cells)\n",
    "grid_subset = grid.head(1000).copy()\n",
    "print(f\"Testing with {len(grid_subset)} grid cells...\")\n",
    "\n",
    "# Test the interpolation\n",
    "try:\n",
    "    interpolated_temps = interpolate_temperature_to_grid(\n",
    "        cdo, grid_subset, max_distance_km=25,  # 25km radius\n",
    "        dcol='date', lat='lat', lon='lon', \n",
    "        tmax='temperature_max_c', tmin='temperature_min_c'\n",
    "    )\n",
    "    \n",
    "    if len(interpolated_temps) > 0:\n",
    "        print(\"\\nSample interpolated results:\")\n",
    "        print(interpolated_temps[['geounit_id', 'date', 'temperature_max_c_daily', 'temperature_min_c_daily', \n",
    "                                  'nearest_station_km', 'stations_used']].head(10))\n",
    "        \n",
    "        print(f\"\\nImprovement summary:\")\n",
    "        print(f\"  Original approach: {574} geounits with temp data (0.03% of full grid)\")\n",
    "        print(f\"  Interpolation approach: {interpolated_temps['geounit_id'].nunique()} geounits with temp data \"\n",
    "              f\"({interpolated_temps['geounit_id'].nunique()/1000*100:.1f}% of test subset)\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error in interpolation: {e}\")\n",
    "    print(\"Let's try a simpler distance-based approach...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39d1ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DEBUG: \n",
    "# === SIMPLER SOLUTION: EXTENDED SPATIAL JOIN ===\n",
    "\n",
    "def improved_temperature_join(cdo_data, grid, max_distance_km=25, dcol='date', lat='lat', lon='lon', \n",
    "                             tmax='temperature_max_c', tmin='temperature_min_c', prcp='precipitation_mm'):\n",
    "    \"\"\"\n",
    "    Improved spatial join that assigns temperature data to grid cells within max_distance_km of stations.\n",
    "    Uses the existing sjoin_nearest_no_cutoff but filters by distance and aggregates multiple stations.\n",
    "    \"\"\"\n",
    "    print(f\"Starting improved temperature join with max distance: {max_distance_km}km\")\n",
    "    \n",
    "    # Prepare temperature data  \n",
    "    temp_data = cdo_data.copy()\n",
    "    temp_data[dcol] = pd.to_datetime(temp_data[dcol], utc=True, errors=\"coerce\").dt.normalize()\n",
    "    \n",
    "    # Remove invalid coordinates\n",
    "    valid_coords = temp_data[lat].notna() & temp_data[lon].notna()\n",
    "    temp_data = temp_data[valid_coords].copy()\n",
    "    \n",
    "    print(f\"Valid coordinate records: {len(temp_data)}\")\n",
    "    \n",
    "    # Create points for temperature stations\n",
    "    pts = gpd.GeoDataFrame(temp_data, geometry=gpd.points_from_xy(temp_data[lon], temp_data[lat]), crs=4326)\n",
    "    \n",
    "    # Instead of joining to nearest, join to ALL geounits within max distance\n",
    "    # We'll do this by expanding the search radius in the spatial join\n",
    "    \n",
    "    # Create a buffer around each temperature station\n",
    "    pts_buffered = pts.copy()\n",
    "    # Convert to projected CRS for accurate distance buffering (California Albers)\n",
    "    pts_projected = pts_buffered.to_crs('EPSG:3310')  # California Albers\n",
    "    pts_projected['geometry'] = pts_projected.geometry.buffer(max_distance_km * 1000)  # buffer in meters\n",
    "    pts_buffered = pts_projected.to_crs(4326)\n",
    "    \n",
    "    # Spatial join: find all grid cells that intersect with station buffers\n",
    "    grid_centroids = gpd.GeoDataFrame(grid[['geounit_id']].copy(), geometry=grid['centroid'], crs=4326)\n",
    "    \n",
    "    # Use spatial join to find overlaps\n",
    "    joined = gpd.sjoin(grid_centroids, pts_buffered, how='inner', predicate='intersects')\n",
    "    \n",
    "    print(f\"Grid cells within {max_distance_km}km of stations: {len(joined)}\")\n",
    "    print(f\"Unique geounits covered: {joined['geounit_id'].nunique()} / {len(grid)} ({joined['geounit_id'].nunique()/len(grid)*100:.1f}%)\")\n",
    "    \n",
    "    # Calculate actual distances for weighting\n",
    "    # For each geounit-station pair, calculate distance\n",
    "    distance_data = []\n",
    "    for _, row in joined.iterrows():\n",
    "        geounit_id = row['geounit_id']\n",
    "        station_idx = row['index_right']\n",
    "        \n",
    "        # Get grid centroid and station location  \n",
    "        grid_pt = grid[grid['geounit_id'] == geounit_id]['centroid'].iloc[0]\n",
    "        station_pt = pts.iloc[station_idx]['geometry']\n",
    "        \n",
    "        # Calculate distance in km (approximate using degrees)\n",
    "        # 1 degree ≈ 111 km at equator, adjust for latitude\n",
    "        lat_rad = np.radians(grid_pt.y)\n",
    "        lat_diff = grid_pt.y - station_pt.y\n",
    "        lon_diff = (grid_pt.x - station_pt.x) * np.cos(lat_rad)\n",
    "        dist_km = np.sqrt(lat_diff**2 + lon_diff**2) * 111.0\n",
    "        \n",
    "        if dist_km <= max_distance_km:  # Double-check distance\n",
    "            distance_data.append({\n",
    "                'geounit_id': geounit_id,\n",
    "                'station_idx': station_idx,\n",
    "                'distance_km': dist_km,\n",
    "                'date': pts.iloc[station_idx][dcol],\n",
    "                'tmax': pts.iloc[station_idx][tmax] if pd.notna(pts.iloc[station_idx][tmax]) else None,\n",
    "                'tmin': pts.iloc[station_idx][tmin] if pd.notna(pts.iloc[station_idx][tmin]) else None,\n",
    "                'prcp': pts.iloc[station_idx][prcp] if pd.notna(pts.iloc[station_idx][prcp]) else None\n",
    "            })\n",
    "    \n",
    "    distance_df = pd.DataFrame(distance_data)\n",
    "    print(f\"Geounit-station-date combinations: {len(distance_df)}\")\n",
    "    \n",
    "    if len(distance_df) == 0:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Group by geounit and date, then calculate weighted averages\n",
    "    def weighted_average(group):\n",
    "        \"\"\"Calculate inverse distance weighted average\"\"\"\n",
    "        # Inverse distance weighting (avoid division by zero)\n",
    "        weights = 1.0 / np.maximum(group['distance_km'], 0.001)\n",
    "        weights = weights / weights.sum()\n",
    "        \n",
    "        result = {'geounit_id': group['geounit_id'].iloc[0], 'date': group['date'].iloc[0]}\n",
    "        \n",
    "        # Temperature max\n",
    "        tmax_vals = group['tmax'].dropna()\n",
    "        if len(tmax_vals) > 0:\n",
    "            tmax_weights = weights[group['tmax'].notna()][:len(tmax_vals)]\n",
    "            result['temperature_max_c_daily'] = np.average(tmax_vals, weights=tmax_weights)\n",
    "        \n",
    "        # Temperature min  \n",
    "        tmin_vals = group['tmin'].dropna()\n",
    "        if len(tmin_vals) > 0:\n",
    "            tmin_weights = weights[group['tmin'].notna()][:len(tmin_vals)]\n",
    "            result['temperature_min_c_daily'] = np.average(tmin_vals, weights=tmin_weights)\n",
    "            \n",
    "        # Precipitation (sum, not average)\n",
    "        prcp_vals = group['prcp'].dropna()\n",
    "        if len(prcp_vals) > 0:\n",
    "            result['precipitation_mm_daily'] = prcp_vals.sum()\n",
    "            \n",
    "        result['nearest_station_km'] = group['distance_km'].min()\n",
    "        result['stations_used'] = len(group)\n",
    "        \n",
    "        return pd.Series(result)\n",
    "    \n",
    "    # Apply weighted averaging\n",
    "    interpolated = distance_df.groupby(['geounit_id', 'date']).apply(weighted_average).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Final interpolated records: {len(interpolated)}\")\n",
    "    print(f\"Unique geounits with data: {interpolated['geounit_id'].nunique()}\")\n",
    "    print(f\"Date range: {interpolated['date'].min()} to {interpolated['date'].max()}\")\n",
    "    \n",
    "    return interpolated\n",
    "\n",
    "# Test the improved approach\n",
    "print(\"=== Testing improved spatial join approach ===\")\n",
    "\n",
    "# Test with a smaller grid subset first\n",
    "grid_test = grid.head(5000).copy()  # Use first 5000 cells\n",
    "print(f\"Testing with {len(grid_test)} grid cells...\")\n",
    "\n",
    "try:\n",
    "    improved_temps = improved_temperature_join(\n",
    "        cdo, grid_test, max_distance_km=20,  # 20km radius\n",
    "        dcol='date', lat='lat', lon='lon', \n",
    "        tmax='temperature_max_c', tmin='temperature_min_c', prcp='precipitation_mm'\n",
    "    )\n",
    "    \n",
    "    if len(improved_temps) > 0:\n",
    "        print(\"\\nSample improved results:\")\n",
    "        display_cols = ['geounit_id', 'date', 'temperature_max_c_daily', 'temperature_min_c_daily', \n",
    "                       'nearest_station_km', 'stations_used']\n",
    "        print(improved_temps[display_cols].head(10))\n",
    "        \n",
    "        # Compare coverage\n",
    "        original_coverage = 574 / len(grid) * 100  # From original analysis\n",
    "        test_coverage = improved_temps['geounit_id'].nunique() / len(grid_test) * 100\n",
    "        \n",
    "        print(f\"\\nCoverage comparison:\")\n",
    "        print(f\"  Original approach: 0.03% of full grid\")\n",
    "        print(f\"  Improved approach: {test_coverage:.1f}% of test subset\")\n",
    "        print(f\"  Improvement factor: ~{test_coverage/0.03:.0f}x better coverage\")\n",
    "        \n",
    "        # Show distance distribution\n",
    "        print(f\"\\nDistance distribution:\")\n",
    "        distances = improved_temps['nearest_station_km']\n",
    "        print(f\"  Min: {distances.min():.1f}km\")\n",
    "        print(f\"  Median: {distances.median():.1f}km\") \n",
    "        print(f\"  Max: {distances.max():.1f}km\")\n",
    "        print(f\"  Mean stations per geounit: {improved_temps['stations_used'].mean():.1f}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error in improved join: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa84ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DEBUG: \n",
    "# === PRACTICAL SOLUTIONS FOR BETTER TEMPERATURE COVERAGE ===\n",
    "\n",
    "print(\"=== RECOMMENDED SOLUTIONS ===\")\n",
    "print()\n",
    "\n",
    "print(\"1. USE A COARSER GRID (IMMEDIATE FIX)\")\n",
    "print(\"   Your current grid: 1.83M cells (750m resolution)\")\n",
    "print(\"   Recommended alternatives:\")\n",
    "print(\"   - 10km grid: ~2,000 cells for California\")  \n",
    "print(\"   - 5km grid: ~8,000 cells for California\")\n",
    "print(\"   - 2km grid: ~50,000 cells for California\")\n",
    "print()\n",
    "\n",
    "print(\"2. MODIFY THE SPATIAL JOIN TO USE DISTANCE BUFFERING\")\n",
    "print(\"   Instead of exact point matching, use:\")\n",
    "print(\"   - Buffer weather stations by 10-25km radius\")\n",
    "print(\"   - Assign temperature to all grid cells within buffer\")\n",
    "print(\"   - Use inverse distance weighting for overlapping stations\")\n",
    "print()\n",
    "\n",
    "print(\"3. SIMPLE FIX: Modify sjoin_nearest_no_cutoff function\")\n",
    "print(\"   Current: assigns to nearest grid cell only\")\n",
    "print(\"   Better: assign to all grid cells within X km of each station\")\n",
    "print()\n",
    "\n",
    "# Show the specific code change needed\n",
    "print(\"4. CODE MODIFICATION FOR CURRENT PIPELINE:\")\n",
    "print()\n",
    "print(\"Replace this in your superset builder:\")\n",
    "print(\"```python\")\n",
    "print(\"j = sjoin_nearest_no_cutoff(pts, grid)\")\n",
    "print(\"```\")\n",
    "print()\n",
    "print(\"With this:\")\n",
    "print(\"```python\")\n",
    "print(\"# Buffer stations by 15km and find all intersecting grid cells\")\n",
    "print(\"pts_buffered = pts.to_crs('EPSG:3310')  # CA Albers for accurate distance\")\n",
    "print(\"pts_buffered.geometry = pts_buffered.geometry.buffer(15000)  # 15km buffer\") \n",
    "print(\"pts_buffered = pts_buffered.to_crs(4326)\")\n",
    "print()\n",
    "print(\"# Join to all grid cells within buffer\")\n",
    "print(\"grid_centroids = gpd.GeoDataFrame(grid[['geounit_id']], \")\n",
    "print(\"                                  geometry=grid['centroid'], crs=4326)\")\n",
    "print(\"j = gpd.sjoin(grid_centroids, pts_buffered, how='inner', predicate='intersects')\")\n",
    "print(\"```\")\n",
    "print()\n",
    "\n",
    "print(\"5. ANALYSIS OF YOUR CURRENT DATA:\")\n",
    "print(f\"   - Temperature stations: {cdo['station'].nunique()} unique stations\")\n",
    "print(f\"   - Temperature records: {(cdo['temperature_max_c'].notna()).sum():,} records\")\n",
    "print(f\"   - Coverage: Only {574} out of {len(grid):,} grid cells get temperature\")\n",
    "print(f\"   - That's {574/len(grid)*100:.4f}% spatial coverage\")\n",
    "print()\n",
    "\n",
    "print(\"6. EXPECTED IMPROVEMENT WITH BUFFERING:\")\n",
    "print(\"   - 10km buffer: ~50-80% of grid cells would get temperature data\")\n",
    "print(\"   - 15km buffer: ~70-90% of grid cells would get temperature data\") \n",
    "print(\"   - 25km buffer: ~90-95% of grid cells would get temperature data\")\n",
    "print()\n",
    "\n",
    "print(\"RECOMMENDATION: Start with option #4 (buffer modification) as it requires\")\n",
    "print(\"minimal code changes to your existing pipeline.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6fb21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DEBUG: \n",
    "# --- File wiring for your case ---\n",
    "PATHS = {\n",
    "    \"airnow\":   \"results/airnow_pm25_clean.parquet\",\n",
    "    \"aqs\":      \"results/aqs_pm25_clean.parquet\",\n",
    "    \"ghcnd\":    \"results/ghcnd_daily_cleaned.parquet\",\n",
    "    \"uscrn\":    \"results/uscrn_2024_hourly_clean.parquet\",\n",
    "    \"hms_fire\": \"results/hms_fire_2024-06-01_to_2024-10-31.parquet\",\n",
    "    \"hms_smoke\":\"results/hms_smoke_2024-06-01_to_2024-10-31.parquet\"\n",
    "}\n",
    "\n",
    "# Load grid\n",
    "import geopandas as gpd, os\n",
    "GRID_PATH = \"results/grid_3000m_CA_epsg3310.parquet\"\n",
    "grid_3310 = gpd.read_parquet(GRID_PATH)\n",
    "print(\"Grid loaded:\", GRID_PATH, \"| cells:\", len(grid_3310))\n",
    "\n",
    "# Inject into CONFIG for future cells\n",
    "CONFIG[\"paths\"] = PATHS\n",
    "\n",
    "# Then run the wiring+build cell from above unchanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3ae429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DEBUG: \n",
    "# --- Wire your files → normalized DataFrames → final daily Parquet ---\n",
    "\n",
    "import os, pandas as pd, geopandas as gpd, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# 0) paths\n",
    "def _find(p):\n",
    "    \"\"\"Try ./, ./results/, or exact path.\"\"\"\n",
    "    cands = [p, os.path.join(\"results\", p)]\n",
    "    for c in cands:\n",
    "        if os.path.exists(c): return c\n",
    "    raise FileNotFoundError(p)\n",
    "\n",
    "GRID_PATH = _find(\"grid_3000m_CA_epsg3310.parquet\")\n",
    "AIRNOW_PATH = _find(\"airnow_pm25_clean.parquet\")\n",
    "AQS_PATH    = _find(\"aqs_pm25_clean.parquet\")\n",
    "GHCND_PATH  = _find(\"ghcnd_daily_cleaned.parquet\")\n",
    "USCRN_PATH  = _find(\"uscrn_2024_hourly_clean.parquet\")\n",
    "FIRE_PATH   = _find(\"hms_fire_2024-06-01_to_2024-10-31.parquet\")\n",
    "SMOKE_PATH  = _find(\"hms_smoke_2024-06-01_to_2024-10-31.parquet\")\n",
    "\n",
    "# 1) grid\n",
    "grid_3310 = gpd.read_parquet(GRID_PATH)\n",
    "assert grid_3310.crs and grid_3310.crs.to_epsg()==3310, \"grid must be EPSG:3310\"\n",
    "\n",
    "# 2) helpers\n",
    "DATE_KEYS = [\"date\",\"datetime_utc\",\"datetime\",\"obs_date\",\"timestamp\",\"time\"]\n",
    "def _norm_date(df, name):\n",
    "    for k in DATE_KEYS:\n",
    "        if k in df.columns:\n",
    "            s = pd.to_datetime(df[k], errors=\"coerce\", utc=True)\n",
    "            if s.notna().any():\n",
    "                d = df.copy()\n",
    "                d[\"date\"] = s.dt.date.astype(str)\n",
    "                return d\n",
    "    raise ValueError(f\"{name}: no parseable date column among {DATE_KEYS}\")\n",
    "\n",
    "def _ensure_xy(df, name):\n",
    "    lat = next((c for c in df.columns if c.lower() in (\"lat\",\"latitude\")), None)\n",
    "    lon = next((c for c in df.columns if c.lower() in (\"lon\",\"lng\",\"longitude\")), None)\n",
    "    if not lat or not lon:\n",
    "        raise ValueError(f\"{name}: missing lat/lon\")\n",
    "    d = df.rename(columns={lat:\"lat\", lon:\"lon\"}).dropna(subset=[\"lat\",\"lon\"]).copy()\n",
    "    return d\n",
    "\n",
    "# 3) load sources\n",
    "def _read_any(p):\n",
    "    ext = Path(p).suffix.lower()\n",
    "    if ext in (\".parquet\",\".pq\"): return pd.read_parquet(p)\n",
    "    return pd.read_parquet(p)\n",
    "\n",
    "airnow_df = _read_any(AIRNOW_PATH)\n",
    "aqs_df    = _read_any(AQS_PATH)\n",
    "ghcnd_df  = _read_any(GHCND_PATH)\n",
    "uscrn_raw = _read_any(USCRN_PATH)\n",
    "hms_fire_df = _read_any(FIRE_PATH)\n",
    "# smoke is polygon parquet; read with geopandas so geometry is respected\n",
    "hms_smoke_gdf = gpd.read_parquet(SMOKE_PATH)\n",
    "\n",
    "# 4) normalize schemas quickly\n",
    "\n",
    "# AirNow PM2.5 daily or hourly\n",
    "airnow_df = _ensure_xy(airnow_df, \"AirNow\")\n",
    "airnow_df = _norm_date(airnow_df, \"AirNow\")\n",
    "if \"value\" not in airnow_df.columns:\n",
    "    # accept pm25 or concentration-like columns\n",
    "    cand = next((c for c in airnow_df.columns if c.lower() in (\"pm25\",\"pm_25\",\"concentration\",\"value\")), None)\n",
    "    if cand is None: raise ValueError(\"AirNow: need a value column\")\n",
    "    airnow_df = airnow_df.rename(columns={cand:\"value\"})\n",
    "airnow_df[\"pollutant\"] = \"PM25\"\n",
    "\n",
    "# AQS PM2.5 daily\n",
    "aqs_df = _ensure_xy(aqs_df, \"AQS\")\n",
    "aqs_df = _norm_date(aqs_df, \"AQS\")\n",
    "if \"value\" not in aqs_df.columns:\n",
    "    cand = next((c for c in aqs_df.columns if c.lower() in (\"pm25\",\"arith_mean\",\"sample_measurement\",\"value\")), None)\n",
    "    if cand is None: raise ValueError(\"AQS: need a value column\")\n",
    "    aqs_df = aqs_df.rename(columns={cand:\"value\"})\n",
    "aqs_df[\"pollutant\"] = \"PM25\"\n",
    "\n",
    "# GHCND daily\n",
    "ghcnd_df = _ensure_xy(ghcnd_df, \"GHCND\")\n",
    "ghcnd_df = _norm_date(ghcnd_df, \"GHCND\")\n",
    "# accept tmax/tmin/tavg/prcp variants\n",
    "rename_map = {\"tmax\":\"tmax_c\",\"tmin\":\"tmin_c\",\"tavg\":\"tavg_c\",\"prcp\":\"prcp_mm\",\n",
    "              \"t_max_c\":\"tmax_c\",\"t_min_c\":\"tmin_c\",\"t_avg_c\":\"tavg_c\",\"precip_mm\":\"prcp_mm\"}\n",
    "ghcnd_df = ghcnd_df.rename(columns={k:v for k,v in rename_map.items() if k in ghcnd_df.columns})\n",
    "\n",
    "# USCRN hourly → daily\n",
    "uscrn_raw = _ensure_xy(uscrn_raw, \"USCRN\")\n",
    "# try datetime first\n",
    "if not any(c in uscrn_raw.columns for c in (\"date\",\"datetime_utc\",\"datetime\",\"obs_date\",\"timestamp\",\"time\")):\n",
    "    raise ValueError(\"USCRN: need a datetime or date column\")\n",
    "uscrn_raw = _norm_date(uscrn_raw, \"USCRN\")\n",
    "# accept typical columns\n",
    "uscrn_raw = uscrn_raw.rename(columns={k:v for k,v in rename_map.items() if k in uscrn_raw.columns})\n",
    "# aggregate to daily per station\n",
    "grp = uscrn_raw.groupby([\"station_id\"] if \"station_id\" in uscrn_raw.columns else [\"lat\",\"lon\",\"date\"])\n",
    "uscrn_df = grp.agg({\n",
    "    \"tmax_c\": \"max\" if \"tmax_c\" in uscrn_raw.columns else \"mean\",\n",
    "    \"tmin_c\": \"min\" if \"tmin_c\" in uscrn_raw.columns else \"mean\",\n",
    "    \"tavg_c\": \"mean\" if \"tavg_c\" in uscrn_raw.columns else \"mean\",\n",
    "    \"prcp_mm\": \"sum\" if \"prcp_mm\" in uscrn_raw.columns else \"mean\",\n",
    "    \"lat\":\"last\", \"lon\":\"last\"\n",
    "}).reset_index()\n",
    "# ensure date present after groupby if group keys didn't include date properly\n",
    "if \"date\" not in uscrn_df.columns:\n",
    "    uscrn_df[\"date\"] = uscrn_raw[\"date\"]\n",
    "\n",
    "# HMS Fire daily points\n",
    "hms_fire_df = _ensure_xy(hms_fire_df, \"HMS Fire\")\n",
    "hms_fire_df = _norm_date(hms_fire_df, \"HMS Fire\")\n",
    "\n",
    "# HMS Smoke polygons\n",
    "if not isinstance(hms_smoke_gdf, gpd.GeoDataFrame):\n",
    "    raise ValueError(\"HMS Smoke must be a GeoDataFrame with geometry\")\n",
    "if hms_smoke_gdf.crs is None:\n",
    "    hms_smoke_gdf.set_crs(4326, inplace=True)\n",
    "hms_smoke_gdf = _norm_date(hms_smoke_gdf, \"HMS Smoke\")\n",
    "# map density → class if needed\n",
    "if \"smoke_class\" not in hms_smoke_gdf.columns:\n",
    "    if \"density\" in hms_smoke_gdf.columns:\n",
    "        hms_smoke_gdf[\"smoke_class\"] = hms_smoke_gdf[\"density\"].map({\"Light\":1,\"Medium\":2,\"Heavy\":3}).fillna(0).astype(int)\n",
    "    else:\n",
    "        hms_smoke_gdf[\"smoke_class\"] = 0\n",
    "\n",
    "# 5) date window from union\n",
    "dates = []\n",
    "for df in [airnow_df, aqs_df, ghcnd_df, uscrn_df, hms_fire_df, hms_smoke_gdf]:\n",
    "    if df is not None and \"date\" in df.columns and len(df): dates.append(pd.Index(df[\"date\"].unique()))\n",
    "date_from = str(pd.Index.union_many(dates).min())\n",
    "date_to   = str(pd.Index.union_many(dates).max())\n",
    "print(\"Window:\", date_from, \"→\", date_to)\n",
    "\n",
    "# 6) build final\n",
    "FINAL_PARQUET = os.path.join(CONFIG[\"out_dir\"], f\"final_daily_grid_{int(CONFIG.get('grid_resolution_m',3000))}m.parquet\")\n",
    "\n",
    "final_daily = build_final_daily(\n",
    "    grid_gdf=grid_3310,\n",
    "    airnow_df=airnow_df,\n",
    "    aqs_df=aqs_df,\n",
    "    ghcnd_df=ghcnd_df,\n",
    "    uscrn_df=uscrn_df,\n",
    "    hms_fire_df=hms_fire_df,\n",
    "    hms_smoke_gdf=hms_smoke_gdf,\n",
    "    out_parquet=FINAL_PARQUET,\n",
    "    date_from=date_from,\n",
    "    date_to=date_to\n",
    ")\n",
    "\n",
    "print(\"Saved:\", FINAL_PARQUET, \"| rows:\", len(final_daily), \"| days:\", final_daily['date'].nunique(), \"| cells:\", grid_3310.shape[0])\n",
    "display(final_daily.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3e8014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DEBUG: \n",
    "# === Final daily dataset builder: grid × day with AQ, met, fire, smoke ===\n",
    "# Requirements: pandas, numpy, geopandas, shapely, scikit-learn\n",
    "\n",
    "import numpy as np, pandas as pd, geopandas as gpd, os, warnings\n",
    "from datetime import datetime, timezone\n",
    "from shapely.geometry import Point\n",
    "from sklearn.neighbors import BallTree\n",
    "\n",
    "# -------------------------\n",
    "# Inputs expected in memory\n",
    "# -------------------------\n",
    "# grid_3310: GeoDataFrame in EPSG:3310 with columns:\n",
    "#   grid_id (str), geometry (Polygon), land_frac (float in 0..1)\n",
    "#\n",
    "# AirNow daily or hourly:\n",
    "#   airnow_df with columns:\n",
    "#     station_id, lat, lon, datetime_utc (if hourly) or date (YYYY-MM-DD), pollutant, value\n",
    "#     pollutants relevant: \"PM25\", \"O3\"\n",
    "#\n",
    "# AQS daily:\n",
    "#   aqs_df with columns: station_id, lat, lon, date, pollutant, value\n",
    "#\n",
    "# GHCND daily:\n",
    "#   ghcnd_df with columns: station_id, lat, lon, date, tmax_c, tmin_c, tavg_c, prcp_mm\n",
    "#\n",
    "# USCRN daily (optional supplement):\n",
    "#   uscrn_df with columns: station_id, lat, lon, date, tmax_c, tmin_c, tavg_c, prcp_mm\n",
    "#\n",
    "# HMS Fire points daily:\n",
    "#   hms_fire_df with columns: date, lat, lon\n",
    "#\n",
    "# HMS Smoke polygons daily:\n",
    "#   hms_smoke_gdf: GeoDataFrame with columns: date, smoke_class (int 1..3), geometry (Polygon/MultiPolygon)\n",
    "#   CRS can be 4326 or 3310; code will reproject.\n",
    "\n",
    "# -------------------------\n",
    "# Config\n",
    "# -------------------------\n",
    "OPS_EPSG = 3310  # all ops run in meters\n",
    "RES_M = int(CONFIG.get(\"grid_resolution_m\", 3000))\n",
    "OUT_DIR = CONFIG[\"out_dir\"]; os.makedirs(OUT_DIR, exist_ok=True)\n",
    "FINAL_PARQUET = os.path.join(OUT_DIR, f\"final_daily_grid_{RES_M}m.parquet\")\n",
    "\n",
    "# Radii and k per variable. Tiers widen to improve coverage but enforce integrity caps.\n",
    "CFG = {\n",
    "    \"PM25\": {\n",
    "        \"k_urban\": 3, \"k_nonurban\": 5,\n",
    "        \"tiers_km_urban\":   [15, 25, 40],\n",
    "        \"tiers_km_nonurb\":  [30, 50, 80],\n",
    "        \"hard_cap_km\": 80, \"integrity_cap_km\": 50\n",
    "    },\n",
    "    \"O3\": {\n",
    "        \"k_urban\": 3, \"k_nonurban\": 5,\n",
    "        \"tiers_km_urban\":   [30, 50, 75],\n",
    "        \"tiers_km_nonurb\":  [50, 80, 120],\n",
    "        \"hard_cap_km\": 120, \"integrity_cap_km\": 75\n",
    "    },\n",
    "    \"TEMP\": {  # for tmax/tmin/tavg\n",
    "        \"k_urban\": 4, \"k_nonurban\": 6,\n",
    "        \"tiers_km_urban\":   [20, 40, 70],\n",
    "        \"tiers_km_nonurb\":  [30, 60, 100],\n",
    "        \"hard_cap_km\": 100, \"integrity_cap_km\": 70\n",
    "    },\n",
    "    \"PRCP\": {\n",
    "        \"k_urban\": 4, \"k_nonurban\": 6,\n",
    "        \"tiers_km_urban\":   [15, 30, 50],\n",
    "        \"tiers_km_nonurb\":  [25, 50, 80],\n",
    "        \"hard_cap_km\": 80, \"integrity_cap_km\": 50\n",
    "    },\n",
    "}\n",
    "URBAN_CELL_AREA_KM2 = 5.0  # cells with area <5 km² considered urban for k,tier choice\n",
    "\n",
    "# -------------------------\n",
    "# Helpers\n",
    "# -------------------------\n",
    "def _ensure_points_3310(df, lon=\"lon\", lat=\"lat\"):\n",
    "    if isinstance(df, gpd.GeoDataFrame):\n",
    "        g = df.copy()\n",
    "        if g.crs is None: \n",
    "            warnings.warn(\"GeoDataFrame has no CRS; assuming EPSG:4326.\")\n",
    "            g.set_crs(4326, inplace=True)\n",
    "        if g.crs.to_epsg() != OPS_EPSG:\n",
    "            g = g.to_crs(OPS_EPSG)\n",
    "        return g\n",
    "    g = gpd.GeoDataFrame(df.copy(), geometry=gpd.points_from_xy(df[lon], df[lat]), crs=4326)\n",
    "    return g.to_crs(OPS_EPSG)\n",
    "\n",
    "def _daily_from_hourly(airnow_hourly):\n",
    "    # PM2.5 daily mean; O3 MDA8 (rolling 8-hr max) per station\n",
    "    df = airnow_hourly.copy()\n",
    "    df[\"datetime_utc\"] = pd.to_datetime(df[\"datetime_utc\"], utc=True)\n",
    "    df[\"date\"] = df[\"datetime_utc\"].dt.tz_convert(\"UTC\").dt.normalize().dt.date.astype(str)\n",
    "    out = []\n",
    "    for pol in [\"PM25\", \"O3\"]:\n",
    "        sub = df[df[\"pollutant\"].str.upper() == pol].copy()\n",
    "        if pol == \"PM25\":\n",
    "            agg = sub.groupby([\"station_id\",\"date\"], as_index=False)[\"value\"].mean().rename(columns={\"value\":\"value\"})\n",
    "        else:\n",
    "            # MDA8 per station: resample hourly, rolling 8h mean, then daily max\n",
    "            # Assumes one record per hour; if gaps exist, rolling will drop them implicitly.\n",
    "            def _mda8(g):\n",
    "                s = g.set_index(\"datetime_utc\")[\"value\"].sort_index()\n",
    "                mda8 = s.rolling(\"8H\").mean().resample(\"1D\").max()\n",
    "                return pd.DataFrame({\"date\": mda8.index.date.astype(str), \"value\": mda8.values})\n",
    "            agg = (sub.groupby(\"station_id\", group_keys=False).apply(_mda8).reset_index())\n",
    "        agg[\"pollutant\"] = pol\n",
    "        out.append(agg)\n",
    "    out = pd.concat(out, ignore_index=True)\n",
    "    # Bring back station lat/lon for geopointing\n",
    "    stns = df.groupby(\"station_id\")[[\"lat\",\"lon\"]].last().reset_index()\n",
    "    out = out.merge(stns, on=\"station_id\", how=\"left\")\n",
    "    return out\n",
    "\n",
    "def _balltree_xy(gpoints):\n",
    "    # gpoints: GeoSeries of Points in EPSG:3310\n",
    "    xy = np.column_stack([gpoints.x.values, gpoints.y.values])\n",
    "    return BallTree(xy, leaf_size=40, metric=\"euclidean\"), xy\n",
    "\n",
    "def _idw(values, dists_m, power=1.5):\n",
    "    d = np.maximum(dists_m/1000.0, 0.5)  # km with floor to avoid singularity\n",
    "    w = 1.0 / (d ** power)\n",
    "    return np.sum(w * values) / np.sum(w)\n",
    "\n",
    "def _urban_mask(grid_gdf):\n",
    "    # simple area heuristic from cell area in km2\n",
    "    # cell area = RES_M^2; project is meters\n",
    "    area_km2 = (RES_M/1000.0)**2\n",
    "    return np.full(len(grid_gdf), area_km2 < URBAN_CELL_AREA_KM2, dtype=bool)\n",
    "\n",
    "def _choose_tiers(var_key, urban):\n",
    "    cfg = CFG[var_key]\n",
    "    return (cfg[\"tiers_km_urban\"] if urban else cfg[\"tiers_km_nonurb\"],\n",
    "            cfg[\"k_urban\"] if urban else cfg[\"k_nonurban\"],\n",
    "            cfg[\"hard_cap_km\"], cfg[\"integrity_cap_km\"])\n",
    "\n",
    "# -------------------------\n",
    "# Interpolation kernels\n",
    "# -------------------------\n",
    "def _interp_points_one_var_day(grid_centroids, stations_gdf_day, values_col, var_key, aqs_bonus=1.0):\n",
    "    \"\"\"\n",
    "    grid_centroids: GeoSeries[Point] EPSG:3310\n",
    "    stations_gdf_day: GeoDataFrame with geometry (Point EPSG:3310) and scalar column values_col\n",
    "                      optional 'source' column for AQ blending ('AQS','AirNow')\n",
    "    var_key: \"PM25\", \"O3\", \"TEMP\", or \"PRCP\"\n",
    "    aqs_bonus: weight multiplier for AQS when blending with AirNow in wider tiers\n",
    "    Returns DataFrame with value, n_used, maxdist_km, method, radius_tier, integrity_warn\n",
    "    \"\"\"\n",
    "    n_cells = len(grid_centroids)\n",
    "    # Empty case → all NA\n",
    "    if stations_gdf_day is None or len(stations_gdf_day) == 0:\n",
    "        return pd.DataFrame({\n",
    "            \"value\": [np.nan]*n_cells, \"n_used\": 0, \"maxdist_km\": np.nan,\n",
    "            \"method\": None, \"radius_tier\": None, \"integrity_warn\": 0\n",
    "        })\n",
    "\n",
    "    # Build tree on all stations\n",
    "    tree, xy = _balltree_xy(stations_gdf_day.geometry)\n",
    "    vals = stations_gdf_day[values_col].to_numpy()\n",
    "    src = stations_gdf_day.get(\"source\", pd.Series([\"\"]*len(stations_gdf_day))).astype(str).to_numpy()\n",
    "\n",
    "    # Precompute centroids array\n",
    "    qxy = np.column_stack([grid_centroids.x.values, grid_centroids.y.values])\n",
    "\n",
    "    # Urban mask decides tiers and k\n",
    "    urban_mask = _urban_mask(grid_centroids.to_frame(name=\"geometry\"))\n",
    "\n",
    "    # Outputs\n",
    "    val_out = np.full(n_cells, np.nan, dtype=float)\n",
    "    n_out = np.zeros(n_cells, dtype=np.int16)\n",
    "    maxd_out = np.full(n_cells, np.nan, dtype=float)\n",
    "    meth_out = np.full(n_cells, None, dtype=object)\n",
    "    tier_out = np.full(n_cells, None, dtype=object)\n",
    "    warn_out = np.zeros(n_cells, dtype=np.int8)\n",
    "\n",
    "    # Per cell apply tiered search\n",
    "    for idx in range(n_cells):\n",
    "        tiers_km, k, hard_cap, integ_cap = _choose_tiers(var_key, urban_mask[idx])\n",
    "        found = False\n",
    "        for t_i, r_km in enumerate(tiers_km, start=1):\n",
    "            # Query neighbors within radius\n",
    "            ind = tree.query_radius(qxy[idx:idx+1, :], r=r_km*1000.0, return_distance=True)\n",
    "            ids = ind[0][0]; dists = ind[1][0]  # np arrays\n",
    "            if ids.size == 0:\n",
    "                continue\n",
    "            # take up to k nearest in this tier\n",
    "            order = np.argsort(dists)[:k]\n",
    "            ids, d = ids[order], dists[order]\n",
    "            v = vals[ids]\n",
    "\n",
    "            # AQ precedence logic: prefer AQS in tier 1; else blend with AQS bonus\n",
    "            if var_key in [\"PM25\",\"O3\"] and \"source\" in stations_gdf_day.columns:\n",
    "                sub_src = src[ids]\n",
    "                if t_i == 1 and np.any(sub_src == \"AQS\"):\n",
    "                    keep = (sub_src == \"AQS\")\n",
    "                    v = v[keep]; d = d[keep]\n",
    "                else:\n",
    "                    # weight AQS more\n",
    "                    w = 1.0 / np.maximum(d/1000.0, 0.5) ** 1.5\n",
    "                    w[sub_src == \"AQS\"] *= aqs_bonus\n",
    "                    val = np.sum(w * v) / np.sum(w)\n",
    "                    val_out[idx] = float(val)\n",
    "                    n_out[idx] = int(len(v))\n",
    "                    maxd_out[idx] = float(d.max()/1000.0)\n",
    "                    meth_out[idx] = \"IDW\"\n",
    "                    tier_out[idx] = t_i\n",
    "                    warn_out[idx] = int((d.max()/1000.0) > integ_cap)\n",
    "                    found = True\n",
    "                    break\n",
    "\n",
    "            # Non-AQ or AQ after AQS filter → IDW if ≥2 else NN\n",
    "            if len(v) == 1:\n",
    "                val_out[idx] = float(v[0]); n_out[idx] = 1\n",
    "                maxd_out[idx] = float(d[0]/1000.0); meth_out[idx] = \"NN\"; tier_out[idx] = t_i\n",
    "                warn_out[idx] = int((d[0]/1000.0) > integ_cap)\n",
    "            else:\n",
    "                val_out[idx] = float(_idw(v, d, power=1.5)); n_out[idx] = int(len(v))\n",
    "                maxd_out[idx] = float(d.max()/1000.0); meth_out[idx] = \"IDW\"; tier_out[idx] = t_i\n",
    "                warn_out[idx] = int((d.max()/1000.0) > integ_cap)\n",
    "            found = True\n",
    "            break\n",
    "\n",
    "        if not found:\n",
    "            # past tiers; do nothing (leave NA)\n",
    "            pass\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"value\": val_out, \"n_used\": n_out, \"maxdist_km\": maxd_out,\n",
    "        \"method\": meth_out, \"radius_tier\": tier_out, \"integrity_warn\": warn_out\n",
    "    })\n",
    "\n",
    "# -------------------------\n",
    "# Domain adapters (per day)\n",
    "# -------------------------\n",
    "def _aq_for_day(date_str, airnow_daily, aqs_daily, grid_gdf):\n",
    "    # Build daily station table with 'source' tag\n",
    "    frames = []\n",
    "    if airnow_daily is not None and len(airnow_daily):\n",
    "        a = airnow_daily[airnow_daily[\"date\"] == date_str].copy()\n",
    "        if len(a):\n",
    "            a[\"source\"] = \"AirNow\"; frames.append(a)\n",
    "    if aqs_daily is not None and len(aqs_daily):\n",
    "        q = aqs_daily[aqs_daily[\"date\"] == date_str].copy()\n",
    "        if len(q):\n",
    "            q[\"source\"] = \"AQS\"; frames.append(q)\n",
    "    if not frames:\n",
    "        return None, None\n",
    "    day = pd.concat(frames, ignore_index=True)\n",
    "    g = _ensure_points_3310(day)\n",
    "    out = {}\n",
    "    for pol, key in [(\"PM25\",\"PM25\"), (\"O3\",\"O3\")]:\n",
    "        sub = g[g[\"pollutant\"].str.upper()==pol][[\"source\",\"value\",\"geometry\"]].dropna(subset=[\"value\"])\n",
    "        if len(sub)==0:\n",
    "            out[pol] = None\n",
    "            continue\n",
    "        res = _interp_points_one_var_day(grid_gdf.geometry.centroid, sub, \"value\", key, aqs_bonus=1.5)\n",
    "        out[pol] = res.rename(columns={\n",
    "            \"value\": pol.lower() if pol==\"PM25\" else \"o3_mda8\",\n",
    "            \"n_used\": f\"{pol.lower()}_n\" if pol==\"PM25\" else \"o3_n\",\n",
    "            \"maxdist_km\": f\"{pol.lower()}_maxdist_km\" if pol==\"PM25\" else \"o3_maxdist_km\",\n",
    "            \"method\": f\"{pol.lower()}_method\" if pol==\"PM25\" else \"o3_method\",\n",
    "            \"radius_tier\": f\"{pol.lower()}_radius_tier\" if pol==\"PM25\" else \"o3_radius_tier\",\n",
    "            \"integrity_warn\": f\"{pol.lower()}_integrity_warn\" if pol==\"PM25\" else \"o3_integrity_warn\",\n",
    "        })\n",
    "    return out.get(\"PM25\"), out.get(\"O3\")\n",
    "\n",
    "def _met_for_day(date_str, ghcnd_daily, uscrn_daily, grid_gdf):\n",
    "    # Primary = GHCND; supplement = USCRN if few neighbors in first two tiers\n",
    "    frames = []\n",
    "    if ghcnd_daily is not None and len(ghcnd_daily):\n",
    "        g = ghcnd_daily[ghcnd_daily[\"date\"] == date_str].copy()\n",
    "        if len(g): g[\"source\"]=\"GHCND\"; frames.append(g)\n",
    "    if uscrn_daily is not None and len(uscrn_daily):\n",
    "        u = uscrn_daily[uscrn_daily[\"date\"] == date_str].copy()\n",
    "        if len(u): u[\"source\"]=\"USCRN\"; frames.append(u)\n",
    "    if not frames:\n",
    "        return None\n",
    "    day = pd.concat(frames, ignore_index=True)\n",
    "    gdf = _ensure_points_3310(day)\n",
    "\n",
    "    # Temp metrics\n",
    "    out = {}\n",
    "    for col, key in [(\"tmax_c\",\"TEMP\"), (\"tmin_c\",\"TEMP\"), (\"tavg_c\",\"TEMP\")]:\n",
    "        sub = gdf[[\"source\", col, \"geometry\"]].dropna(subset=[col])\n",
    "        if len(sub):\n",
    "            res = _interp_points_one_var_day(grid_gdf.geometry.centroid, sub.rename(columns={col:\"value\"}), \"value\", key)\n",
    "            out[col] = res.add_prefix(col+\"_\")\n",
    "        else:\n",
    "            out[col] = None\n",
    "\n",
    "    # Precip with sqrt-IDW trick\n",
    "    if \"prcp_mm\" in gdf.columns:\n",
    "        subp = gdf[[\"source\",\"prcp_mm\",\"geometry\"]].dropna(subset=[\"prcp_mm\"]).copy()\n",
    "        if len(subp):\n",
    "            subp[\"value\"] = np.sqrt(np.maximum(subp[\"prcp_mm\"].to_numpy(), 0.0))\n",
    "            res = _interp_points_one_var_day(grid_gdf.geometry.centroid, subp, \"value\", \"PRCP\")\n",
    "            res[\"value\"] = np.square(res[\"value\"])\n",
    "            out[\"prcp_mm\"] = res.rename(columns={\n",
    "                \"value\":\"prcp_mm\", \"n_used\":\"prcp_n\", \"maxdist_km\":\"prcp_maxdist_km\",\n",
    "                \"method\":\"prcp_method\", \"radius_tier\":\"prcp_radius_tier\", \"integrity_warn\":\"prcp_integrity_warn\"\n",
    "            })\n",
    "        else:\n",
    "            out[\"prcp_mm\"] = None\n",
    "    else:\n",
    "        out[\"prcp_mm\"] = None\n",
    "\n",
    "    # Merge pieces\n",
    "    pieces = []\n",
    "    for k,v in out.items():\n",
    "        if v is not None:\n",
    "            pieces.append(v)\n",
    "    if not pieces:\n",
    "        return None\n",
    "    met_df = pd.concat(pieces, axis=1)\n",
    "    return met_df\n",
    "\n",
    "def _fire_for_day(date_str, fire_df, grid_gdf):\n",
    "    if fire_df is None or len(fire_df)==0:\n",
    "        return pd.DataFrame({\"fire_count_in\":0, \"fire_buffer_count\":0, \"nearest_fire_km\":np.nan}, index=grid_gdf.index)\n",
    "    day = fire_df[fire_df[\"date\"]==date_str]\n",
    "    if len(day)==0:\n",
    "        return pd.DataFrame({\"fire_count_in\":0, \"fire_buffer_count\":0, \"nearest_fire_km\":np.nan}, index=grid_gdf.index)\n",
    "    g = _ensure_points_3310(day)\n",
    "    # counts in cell\n",
    "    join_in = gpd.sjoin(gpd.GeoDataFrame(geometry=grid_gdf.geometry, crs=OPS_EPSG),\n",
    "                        g[[\"geometry\"]], how=\"left\", predicate=\"contains\")\n",
    "    counts = join_in.groupby(join_in.index).size().reindex(grid_gdf.index, fill_value=0)\n",
    "    # buffer counts within 2 km of boundary (outside cell)\n",
    "    grid_buf = gpd.GeoDataFrame(geometry=grid_gdf.geometry.buffer(2000), crs=OPS_EPSG)\n",
    "    join_buf = gpd.sjoin(grid_buf, g[[\"geometry\"]], how=\"left\", predicate=\"contains\")\n",
    "    counts_buf = join_buf.groupby(join_buf.index).size().reindex(grid_gdf.index, fill_value=0) - counts\n",
    "    counts_buf = counts_buf.clip(lower=0)\n",
    "\n",
    "    # nearest distance ≤ 20 km\n",
    "    tree, xy = _balltree_xy(g.geometry)\n",
    "    qxy = np.column_stack([grid_gdf.geometry.centroid.x.values, grid_gdf.geometry.centroid.y.values])\n",
    "    d, _ = tree.query(qxy, k=1)\n",
    "    d_km = (d[:,0]/1000.0)\n",
    "    d_km[d_km>20] = np.nan\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"fire_count_in\": counts.values.astype(np.int16),\n",
    "        \"fire_buffer_count\": counts_buf.values.astype(np.int16),\n",
    "        \"nearest_fire_km\": d_km.astype(float)\n",
    "    }, index=grid_gdf.index)\n",
    "\n",
    "def _smoke_for_day(date_str, smoke_gdf, grid_gdf):\n",
    "    if smoke_gdf is None or len(smoke_gdf)==0:\n",
    "        return pd.DataFrame({\"smoke_class\":0, \"smoke_frac\":0.0, \"smoke_nearby\":0}, index=grid_gdf.index)\n",
    "    day = smoke_gdf[smoke_gdf[\"date\"]==date_str]\n",
    "    if len(day)==0:\n",
    "        return pd.DataFrame({\"smoke_class\":0, \"smoke_frac\":0.0, \"smoke_nearby\":0}, index=grid_gdf.index)\n",
    "    g = day.copy()\n",
    "    if not isinstance(g, gpd.GeoDataFrame):\n",
    "        g = gpd.GeoDataFrame(g, geometry=g[\"geometry\"])\n",
    "    if g.crs is None: g.set_crs(4326, inplace=True)\n",
    "    if g.crs.to_epsg()!=OPS_EPSG: g = g.to_crs(OPS_EPSG)\n",
    "\n",
    "    # Intersect with grid for area fraction\n",
    "    inter = gpd.overlay(gpd.GeoDataFrame({\"grid_id\":grid_gdf[\"grid_id\"]}, geometry=grid_gdf.geometry, crs=OPS_EPSG),\n",
    "                        g[[\"smoke_class\",\"geometry\"]], how=\"intersection\")\n",
    "    if len(inter)==0:\n",
    "        # maybe nearby\n",
    "        near = gpd.sjoin(\n",
    "            gpd.GeoDataFrame(geometry=grid_gdf.geometry.buffer(5000), crs=OPS_EPSG),\n",
    "            g[[\"geometry\"]], how=\"left\", predicate=\"intersects\"\n",
    "        ).groupby(level=0).size().reindex(range(len(grid_gdf)), fill_value=0).values\n",
    "        return pd.DataFrame({\"smoke_class\":0, \"smoke_frac\":0.0, \"smoke_nearby\": (near>0).astype(int)}, index=grid_gdf.index)\n",
    "\n",
    "    inter[\"a\"] = inter.geometry.area\n",
    "    cell_area = (RES_M*RES_M)\n",
    "    # Area-weighted class per cell; union-by-max approximation using weighted mean then round\n",
    "    tmp = inter.groupby(\"grid_id\").apply(lambda df: pd.Series({\n",
    "        \"smoke_frac\": float(df[\"a\"].sum()/cell_area),\n",
    "        \"smoke_class_aw\": float((df[\"smoke_class\"]*df[\"a\"]).sum()/df[\"a\"].sum())\n",
    "    })).reset_index()\n",
    "    tmp[\"smoke_class\"] = tmp[\"smoke_class_aw\"].round().clip(0,3).astype(int)\n",
    "    # Nearby flag\n",
    "    near = gpd.sjoin(\n",
    "        gpd.GeoDataFrame(geometry=grid_gdf.geometry.buffer(5000), crs=OPS_EPSG),\n",
    "        g[[\"geometry\"]], how=\"left\", predicate=\"intersects\"\n",
    "    ).groupby(level=0).size().reindex(range(len(grid_gdf)), fill_value=0).values\n",
    "\n",
    "    out = pd.DataFrame({\"smoke_class\":0, \"smoke_frac\":0.0, \"smoke_nearby\": (near>0).astype(int)}, index=grid_gdf.index)\n",
    "    out.loc[out.index[out.index.isin(grid_gdf.set_index(\"grid_id\").loc[tmp[\"grid_id\"]].index)], [\"smoke_class\",\"smoke_frac\"]] = \\\n",
    "        tmp.set_index(grid_gdf.set_index(\"grid_id\").loc[tmp[\"grid_id\"]].index)[[\"smoke_class\",\"smoke_frac\"]].values\n",
    "    return out\n",
    "\n",
    "# -------------------------\n",
    "# Main builder\n",
    "# -------------------------\n",
    "def build_final_daily(grid_gdf,\n",
    "                      airnow_df=None, aqs_df=None,\n",
    "                      ghcnd_df=None, uscrn_df=None,\n",
    "                      hms_fire_df=None, hms_smoke_gdf=None,\n",
    "                      out_parquet=FINAL_PARQUET):\n",
    "    # 1) Normalize date fields, compute AirNow daily if needed\n",
    "    def _norm_date_col(df, date_col):\n",
    "        d = df.copy()\n",
    "        d[date_col] = pd.to_datetime(d[date_col]).dt.date.astype(str)\n",
    "        return d\n",
    "\n",
    "    airnow_daily = None\n",
    "    if airnow_df is not None and len(airnow_df):\n",
    "        if \"date\" in airnow_df.columns:\n",
    "            airnow_daily = _norm_date_col(airnow_df, \"date\")\n",
    "        else:\n",
    "            airnow_daily = _daily_from_hourly(airnow_df)\n",
    "            airnow_daily = _norm_date_col(airnow_daily, \"date\")\n",
    "\n",
    "    aqs_daily = None\n",
    "    if aqs_df is not None and len(aqs_df):\n",
    "        aqs_daily = _norm_date_col(aqs_df, \"date\")\n",
    "\n",
    "    ghcnd_daily = None\n",
    "    if ghcnd_df is not None and len(ghcnd_df):\n",
    "        ghcnd_daily = _norm_date_col(ghcnd_df, \"date\")\n",
    "\n",
    "    uscrn_daily = None\n",
    "    if uscrn_df is not None and len(uscrn_df):\n",
    "        uscrn_daily = _norm_date_col(uscrn_df, \"date\")\n",
    "\n",
    "    fire_daily = None\n",
    "    if hms_fire_df is not None and len(hms_fire_df):\n",
    "        fire_daily = _ensure_points_3310(_norm_date_col(hms_fire_df, \"date\"))\n",
    "\n",
    "    smoke_daily = None\n",
    "    if hms_smoke_gdf is not None and len(hms_smoke_gdf):\n",
    "        g = hms_smoke_gdf.copy()\n",
    "        if not isinstance(g, gpd.GeoDataFrame):\n",
    "            g = gpd.GeoDataFrame(g, geometry=g[\"geometry\"])\n",
    "        if g.crs is None: g.set_crs(4326, inplace=True)\n",
    "        if g.crs.to_epsg()!=OPS_EPSG: g = g.to_crs(OPS_EPSG)\n",
    "        g[\"date\"] = pd.to_datetime(g[\"date\"]).dt.date.astype(str)\n",
    "        smoke_daily = g\n",
    "\n",
    "    # 2) Union of dates across sources\n",
    "    date_sets = []\n",
    "    for d, col in [(airnow_daily, \"date\"), (aqs_daily,\"date\"), (ghcnd_daily,\"date\"), (uscrn_daily,\"date\")]:\n",
    "        if d is not None and len(d): date_sets.append(pd.Index(d[\"date\"].unique()))\n",
    "    if fire_daily is not None and len(fire_daily): date_sets.append(pd.Index(fire_daily[\"date\"].unique()))\n",
    "    if smoke_daily is not None and len(smoke_daily): date_sets.append(pd.Index(smoke_daily[\"date\"].unique()))\n",
    "    if not date_sets:\n",
    "        raise ValueError(\"No dates found in inputs.\")\n",
    "    all_dates = sorted(pd.Index.union_many(date_sets).tolist())\n",
    "\n",
    "    # 3) Prepare constant frame with grid_id\n",
    "    base = pd.DataFrame({\"grid_id\": grid_gdf[\"grid_id\"].values})\n",
    "\n",
    "    # 4) Iterate per day to limit memory\n",
    "    out_parts = []\n",
    "    for dstr in all_dates:\n",
    "        # AQ\n",
    "        pm_df, o3_df = _aq_for_day(dstr, airnow_daily, aqs_daily, grid_gdf)\n",
    "        # Met\n",
    "        met_df = _met_for_day(dstr, ghcnd_daily, uscrn_daily, grid_gdf)\n",
    "        # Fire\n",
    "        fire_df_day = _fire_for_day(dstr, fire_daily, grid_gdf) if fire_daily is not None else None\n",
    "        # Smoke\n",
    "        smoke_df_day = _smoke_for_day(dstr, smoke_daily, grid_gdf) if smoke_daily is not None else None\n",
    "\n",
    "        # Merge wide for this day\n",
    "        day_df = base.copy()\n",
    "        if pm_df is not None:\n",
    "            day_df = pd.concat([day_df, pm_df.reset_index(drop=True)], axis=1)\n",
    "        else:\n",
    "            day_df[\"pm25\"] = np.nan; day_df[\"pm25_n\"]=0; day_df[\"pm25_maxdist_km\"]=np.nan\n",
    "            day_df[\"pm25_method\"]=None; day_df[\"pm25_radius_tier\"]=None; day_df[\"pm25_integrity_warn\"]=0\n",
    "\n",
    "        if o3_df is not None:\n",
    "            day_df = pd.concat([day_df, o3_df.reset_index(drop=True)], axis=1)\n",
    "        else:\n",
    "            day_df[\"o3_mda8\"] = np.nan; day_df[\"o3_n\"]=0; day_df[\"o3_maxdist_km\"]=np.nan\n",
    "            day_df[\"o3_method\"]=None; day_df[\"o3_radius_tier\"]=None; day_df[\"o3_integrity_warn\"]=0\n",
    "\n",
    "        if met_df is not None:\n",
    "            day_df = pd.concat([day_df, met_df.reset_index(drop=True)], axis=1)\n",
    "        else:\n",
    "            for c in [\"tmax_c\",\"tmin_c\",\"tavg_c\"]:\n",
    "                day_df[c+\"_value\"]=np.nan; day_df[c+\"_n\"]=0; day_df[c+\"_maxdist_km\"]=np.nan\n",
    "                day_df[c+\"_method\"]=None; day_df[c+\"_radius_tier\"]=None; day_df[c+\"_integrity_warn\"]=0\n",
    "            day_df[\"prcp_mm\"]=np.nan; day_df[\"prcp_n\"]=0; day_df[\"prcp_maxdist_km\"]=np.nan\n",
    "            day_df[\"prcp_method\"]=None; day_df[\"prcp_radius_tier\"]=None; day_df[\"prcp_integrity_warn\"]=0\n",
    "\n",
    "        if fire_df_day is not None:\n",
    "            day_df = pd.concat([day_df, fire_df_day.reset_index(drop=True)], axis=1)\n",
    "        else:\n",
    "            day_df[\"fire_count_in\"]=0; day_df[\"fire_buffer_count\"]=0; day_df[\"nearest_fire_km\"]=np.nan\n",
    "\n",
    "        if smoke_df_day is not None:\n",
    "            day_df = pd.concat([day_df, smoke_df_day.reset_index(drop=True)], axis=1)\n",
    "        else:\n",
    "            day_df[\"smoke_class\"]=0; day_df[\"smoke_frac\"]=0.0; day_df[\"smoke_nearby\"]=0\n",
    "\n",
    "        day_df.insert(1, \"date\", dstr)\n",
    "        out_parts.append(day_df)\n",
    "\n",
    "        if len(out_parts) % 20 == 0:\n",
    "            print(f\"[{datetime.now(timezone.utc).isoformat()}] Processed {len(out_parts)} days so far...\")\n",
    "\n",
    "    final_df = pd.concat(out_parts, ignore_index=True)\n",
    "\n",
    "    # Save to Parquet\n",
    "    final_df.to_parquet(out_parquet, index=False)\n",
    "\n",
    "    # Diagnostics\n",
    "    print(\"Saved:\", out_parquet)\n",
    "    print(\"Rows:\", len(final_df), \"| Unique days:\", final_df['date'].nunique(), \"| Grid cells:\", base.shape[0])\n",
    "    for col in [\"pm25\",\"o3_mda8\",\"tmax_c_value\",\"tmin_c_value\",\"tavg_c_value\",\"prcp_mm\",\"smoke_frac\",\"fire_count_in\"]:\n",
    "        if col in final_df.columns:\n",
    "            nz = final_df[col].notna().sum()\n",
    "            print(f\"Non-null {col}: {nz} ({nz/len(final_df):.1%})\")\n",
    "\n",
    "    # Coverage summary by distance bin for AQ and met\n",
    "    def _coverage(col):\n",
    "        if col in final_df.columns:\n",
    "            bins = pd.cut(final_df[col], bins=[0,10,25,50,75,100,150], include_lowest=True)\n",
    "            print(f\"{col} bins:\\n\", bins.value_counts().sort_index())\n",
    "\n",
    "    _coverage(\"pm25_maxdist_km\")\n",
    "    _coverage(\"o3_maxdist_km\")\n",
    "    _coverage(\"tavg_c_maxdist_km\")\n",
    "    _coverage(\"prcp_maxdist_km\")\n",
    "\n",
    "    return final_df\n",
    "\n",
    "# -------------------------\n",
    "# Run builder\n",
    "# -------------------------\n",
    "final_daily = build_final_daily(\n",
    "    grid_gdf=grid_3310,\n",
    "    airnow_df=globals().get(\"airnow_df\"),\n",
    "    aqs_df=globals().get(\"aqs_df\"),\n",
    "    ghcnd_df=globals().get(\"ghcnd_df\"),\n",
    "    uscrn_df=globals().get(\"uscrn_df\"),\n",
    "    hms_fire_df=globals().get(\"hms_fire_df\"),\n",
    "    hms_smoke_gdf=globals().get(\"hms_smoke_gdf\"),\n",
    "    out_parquet=FINAL_PARQUET\n",
    ")\n",
    "final_daily.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blueleaflabs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
